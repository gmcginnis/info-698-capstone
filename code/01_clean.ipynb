{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e36f3e2c",
   "metadata": {},
   "source": [
    "# Data Cleaning/Wrangling\n",
    "\n",
    "Author: Gillian A. McGinnis, final-semester M.S. Information Science - Machine Learning  \n",
    "The University of Arizona College of Information  \n",
    "INFO 698 - Capstone  \n",
    "Start date: 24 September 2025  \n",
    "Last updated: 20 November 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f4c138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nModule providing supporting code for preparing data for EDA and analysis.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Module providing supporting code for preparing data for EDA and analysis.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa7a4c",
   "metadata": {},
   "source": [
    "## Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e391302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For time series data management\n",
    "# import matplotlib.ticker as ticker\n",
    "import matplotlib.dates as mdates\n",
    "import datetime as dt\n",
    "# import pyarrow as pa\n",
    "\n",
    "# Custom function\n",
    "from helper_utils import get_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c5133f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details>\n",
       "<summary>Click to view session information</summary>\n",
       "<pre>\n",
       "-----\n",
       "helper_utils        NA\n",
       "matplotlib          3.10.7\n",
       "numpy               2.3.5\n",
       "pandas              2.3.3\n",
       "session_info        v1.0.1\n",
       "-----\n",
       "IPython             9.7.0\n",
       "jupyter_client      8.6.3\n",
       "jupyter_core        5.9.1\n",
       "jupyterlab          4.5.0\n",
       "notebook            7.5.0\n",
       "-----\n",
       "Python 3.13.7 (v3.13.7:bcee1c32211, Aug 14 2025, 19:10:51) [Clang 16.0.0 (clang-1600.0.26.6)]\n",
       "macOS-15.6.1-x86_64-i386-64bit-Mach-O\n",
       "-----\n",
       "Session information updated at 2025-11-29 17:32\n",
       "</pre>\n",
       "</details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## (Optional chunk)\n",
    "# Current session information\n",
    "import session_info\n",
    "session_info.show(dependencies=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de25f7",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Files of interest:\n",
    "- `weir_calibration.csv` includes calibration points for the weir\n",
    "- `bci_lutzweir_combined.csv` includes raw runoff measurement, corrected runoff measurement, data source (*Chart measurements can be removed)\n",
    "- `bci_cl_ra_elect2.CSV` has corrected rainfall (`ra`) in mm with measurements of `0` as `NA`s (`bci_cl_ra_elect.csv` has `0`s)\n",
    "- `bci_lutz_deep_gsm_man.csv`, `bci_lutz_shallow_gsm_man.csv` have soil moisture measurements (water by wet weight and water by dry weight; one can be chosen for analysis as they are linearly related)\n",
    "<!-- `bci_cl_ra_elect.csv` has corrected rainfall (`ra`) in mm, contains `0`s (large file) -->\n",
    "\n",
    "All values level values are in mm, and datetime is in UTC-5 (Panama time zone)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c671e55e",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083c8451",
   "metadata": {},
   "source": [
    "#### Weir calibrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f5d3909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 6465 entries, 1994-01-03 08:46:00 to 2025-09-02 08:50:00\n",
      "Data columns (total 1 columns):\n",
      " #   Column      Non-Null Count  Dtype\n",
      "---  ------      --------------  -----\n",
      " 0   weir_level  6465 non-null   Int8 \n",
      "dtypes: Int8(1)\n",
      "memory usage: 63.1 KB\n"
     ]
    }
   ],
   "source": [
    "## Calibrations dataset\n",
    "data_raw_calibration = pd.read_csv(\n",
    "    # Location of the dataset in the repo\n",
    "    # \"data/weir_calibration.csv\",\n",
    "    get_path(\"weir_calibration.csv\"),\n",
    "    # Specify columns to load\n",
    "    ## note- weir_hour is a repeat of the time in datetime and can be skipped\n",
    "    usecols = ['datetime', 'weir_level'],\n",
    "    # Convert datetime stamp strings to datetime objects\n",
    "    parse_dates = ['datetime'],\n",
    "    # Specify the types for specific columns\n",
    "    dtype = {\n",
    "        'weir_level': 'Int8'\n",
    "    },\n",
    "    # Specify the string formatting of the datetime stamps\n",
    "    date_format = \"%d/%m/%Y %H:%M:%S\",\n",
    "    # Use datetime stamp as index\n",
    "    index_col = 'datetime'\n",
    ")\n",
    "\n",
    "# Arrange chronologically\n",
    "data_raw_calibration = data_raw_calibration.sort_index()\n",
    "\n",
    "data_raw_calibration.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb01d3e7",
   "metadata": {},
   "source": [
    "#### Weir measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19382df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3951119 entries, 1972-01-01 01:00:00 to 2025-08-01 13:00:00\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Dtype   \n",
      "---  ------    -----   \n",
      " 0   level     float32 \n",
      " 1   raw       float32 \n",
      " 2   chk_note  category\n",
      " 3   chk_fail  category\n",
      " 4   comment   category\n",
      " 5   source    category\n",
      "dtypes: category(4), float32(2)\n",
      "memory usage: 75.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Combined data\n",
    "\n",
    "data_raw_weir = pd.read_csv(\n",
    "    # Location of the dataset in the repo\n",
    "    # \"data/bci_lutzweir_combined.csv\",\n",
    "    get_path(\"bci_lutzweir_combined.csv\"),\n",
    "    # Specify columns to load\n",
    "    usecols = ['datetime', 'level', 'raw', 'chk_note', 'chk_fail', 'comment', 'source'],\n",
    "    # Specify the types for specific columns\n",
    "    dtype = {\n",
    "        'level':'float32',\n",
    "        'raw':'float32',\n",
    "        'chk_note':'category',\n",
    "        'chk_fail':'category',\n",
    "        'comment':'category',\n",
    "        'source':'category'\n",
    "    },\n",
    "    # Convert datetime stamp strings to datetime objects\n",
    "    parse_dates = ['datetime'],\n",
    "    # Specify the string formatting of the datetime stamps\n",
    "    date_format = \"%d/%m/%Y %H:%M:%S\",\n",
    "    # Use datetime stamp as index\n",
    "    index_col = 'datetime'\n",
    ")\n",
    "\n",
    "## This variation checks first if the dataset is already loaded into the workspace\n",
    "# try:\n",
    "#     if data_weir.empty == False:\n",
    "#         print(\"Data loaded, random sample shown below\")\n",
    "#         print(data_weir.sample(n=5))\n",
    "# except NameError:\n",
    "#     print(\"Data has not yet been read in, loading now...\")\n",
    "#     data_weir = pd.read_csv(\n",
    "#         \"data/bci_lutzweir_combined.csv\",\n",
    "#         usecols = ['datetime', 'level', 'raw', 'chk_note', 'chk_fail', 'comment', 'source'],\n",
    "#         parse_dates=['datetime'],\n",
    "#         dtype = {'source':'category', 'chk_note':'category', 'chk_fail':'str', 'comment':'str'},\n",
    "#         date_format='%d/%m/%Y %H:%M:%S'\n",
    "#     )\n",
    "\n",
    "# Arrange chronologically\n",
    "data_raw_weir = data_raw_weir.sort_index()\n",
    "\n",
    "data_raw_weir.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b55aaf8",
   "metadata": {},
   "source": [
    "#### Rainfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49b35292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 179640 entries, 1929-01-02 08:00:00 to 2025-08-04 11:55:00\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count   Dtype   \n",
      "---  ------    --------------   -----   \n",
      " 0   ra        179640 non-null  float32 \n",
      " 1   raw       179640 non-null  float32 \n",
      " 2   chk_note  179640 non-null  category\n",
      " 3   chk_fail  29 non-null      category\n",
      "dtypes: category(2), float32(2)\n",
      "memory usage: 3.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Rainfall dataset\n",
    "\n",
    "# This data set skips the 0 readings (therefore much smaller):\n",
    "data_raw_rainfall = pd.read_csv(\n",
    "    # Location of the dataset in the repo\n",
    "    # \"data/bci_elect_cl_ra/bci_cl_ra_elect2.CSV\",\n",
    "    get_path(\"bci_elect_cl_ra/bci_cl_ra_elect2.CSV\"),\n",
    "    # Specify the types for specific columns\n",
    "    dtype = {\n",
    "        'ra':'float32',\n",
    "        'raw':'float32',\n",
    "        'chk_note':'category',\n",
    "        'chk_fail':'category'\n",
    "    },\n",
    "    # Convert datetime stamp strings to datetime objects\n",
    "    parse_dates = ['datetime'],\n",
    "    # Specify the string formatting of the datetime stamps\n",
    "    date_format = \"%d/%m/%Y %H:%M:%S\",\n",
    "    # Use datetime stamp as index\n",
    "    index_col = 'datetime'\n",
    ")\n",
    "\n",
    "# Arrange chronologically\n",
    "data_raw_rainfall = data_raw_rainfall.sort_index()\n",
    "\n",
    "# # This data set includes the 0 readings:\n",
    "# data_raw_rainfall_zeroes = pd.read_csv(\n",
    "#         \"data/bci_elect_cl_ra/bci_cl_ra_elect.csv\",\n",
    "#         usecols = ['datetime', 'ra', 'raw', 'chk_note', 'chk_fail'],\n",
    "#         # \"data/bci_elect_cl_ra/bci_cl_ra_elect2.CSV\",\n",
    "#         # usecols = ['datetime', 'level', 'raw', 'chk_note', 'chk_fail', 'comment', 'source'],\n",
    "#         parse_dates=['datetime'],\n",
    "#         dtype = {'chk_note':'category', 'chk_fail':'str'},\n",
    "#         # dtype = {'source':'category', 'chk_note':'category', 'chk_fail':'str', 'comment':'str'},\n",
    "#         date_format='%d/%m/%Y %H:%M:%S'\n",
    "#     )\n",
    "# # # Arrange chronologically\n",
    "# data_raw_rainfall_zeroes = data_raw_rainfall_zeroes.sort_index()\n",
    "\n",
    "data_raw_rainfall.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6121a42",
   "metadata": {},
   "source": [
    "#### Soil moisture\n",
    "\n",
    "*A note about the soil datasets:\n",
    "\n",
    "Both `h2o_by_wet` and `h2o_by_dry` are available in the datasets.\n",
    "Because they are linearly related to each other, only one of them is necessary for modelling.\n",
    "Arbitrarily, `h2o_by_wet` has been chosen for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4db0e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 18556 entries, 1972-03-03 to 2025-06-26\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype   \n",
      "---  ------      --------------  -----   \n",
      " 0   depth       18556 non-null  category\n",
      " 1   sample      18556 non-null  category\n",
      " 2   h2o_by_wet  18556 non-null  float32 \n",
      " 3   chk_note    18556 non-null  category\n",
      " 4   chk_fail    178 non-null    category\n",
      "dtypes: category(4), float32(1)\n",
      "memory usage: 291.5 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 15637 entries, 1972-03-03 to 2025-06-26\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype   \n",
      "---  ------      --------------  -----   \n",
      " 0   depth       15637 non-null  category\n",
      " 1   sample      15637 non-null  category\n",
      " 2   h2o_by_wet  15637 non-null  float32 \n",
      " 3   chk_note    15637 non-null  category\n",
      " 4   chk_fail    20 non-null     category\n",
      "dtypes: category(4), float32(1)\n",
      "memory usage: 245.4 KB\n"
     ]
    }
   ],
   "source": [
    "# Soil datasets\n",
    "\n",
    "# Shallow\n",
    "data_raw_soil_shallow = pd.read_csv(\n",
    "    # Location of the dataset in the repo\n",
    "    # \"data/bci_manual_soilh/bci_lutz_shallow_gsm_man.csv\",\n",
    "    get_path(\"bci_manual_soilh/bci_lutz_shallow_gsm_man.csv\"),\n",
    "    # Specify columns to load\n",
    "    usecols = ['date', 'depth', 'sample', 'h2o_by_wet', 'chk_note', 'chk_fail'],\n",
    "    # Specify the types for specific columns\n",
    "    dtype = {\n",
    "        'h2o_by_wet':'float32',\n",
    "        'depth':'category',\n",
    "        'sample':'category',\n",
    "        'chk_note':'category',\n",
    "        'chk_fail':'category'\n",
    "    },\n",
    "    # Convert date stamp strings to date objects\n",
    "    parse_dates = ['date'],\n",
    "    # Specify the string formatting of the date stamps\n",
    "    date_format = \"%d/%m/%Y\",\n",
    "    # Use date stamp as index\n",
    "    index_col = 'date'\n",
    ")\n",
    "\n",
    "# Deep\n",
    "data_raw_soil_deep = pd.read_csv(\n",
    "    # Location of the dataset in the repo\n",
    "    # \"data/bci_manual_soilh/bci_lutz_deep_gsm_man.csv\",\n",
    "    get_path(\"bci_manual_soilh/bci_lutz_deep_gsm_man.csv\"),\n",
    "    # Specify columns to load\n",
    "    usecols = ['date', 'depth', 'sample', 'h2o_by_wet', 'chk_note', 'chk_fail'],\n",
    "    # Specify the types for specific columns\n",
    "    dtype = {\n",
    "        'h2o_by_wet':'float32',\n",
    "        'depth':'category',\n",
    "        'sample':'category',\n",
    "        'chk_note':'category',\n",
    "        'chk_fail':'category'\n",
    "    },\n",
    "    # Convert date stamp strings to date objects\n",
    "    parse_dates = ['date'],\n",
    "    # Specify the string formatting of the date stamps\n",
    "    date_format = \"%d/%m/%Y\",\n",
    "    # Use date stamp as index\n",
    "    index_col = 'date'\n",
    ")\n",
    "\n",
    "# Arrange chronologically\n",
    "data_raw_soil_shallow = data_raw_soil_shallow.sort_index()\n",
    "data_raw_soil_deep = data_raw_soil_deep.sort_index()\n",
    "\n",
    "data_raw_soil_shallow.info()\n",
    "data_raw_soil_deep.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059eaebd",
   "metadata": {},
   "source": [
    "## Clean\n",
    "\n",
    "Data cleanup is necessary to ensure ease of uniting the sets, conducting a test/train split, and creation of & fitting of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114972b0",
   "metadata": {},
   "source": [
    "### Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a2e3b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest \t     Latest \t\t Source\n",
      "1972-01-01 01:00:00  2015-03-18 14:15:00 CHART\n",
      "1972-09-16 00:15:00  2025-08-01 13:00:00 nan\n",
      "1989-07-19 11:55:00  1996-10-01 23:55:00 CHART+AF\n",
      "1996-10-02 00:00:00  2013-01-13 05:50:00 ISCO\n",
      "2012-04-23 08:30:00  2012-04-24 08:35:00 ESTIMATED\n",
      "2014-08-22 10:30:00  2021-05-19 09:40:00 RADAR\n",
      "2018-08-31 10:05:00  2018-09-05 12:55:00 TROLL\n"
     ]
    }
   ],
   "source": [
    "# Explore: Get earliest and latest dates of sources\n",
    "\n",
    "cat_source = data_raw_weir['source'].unique().tolist()\n",
    "# Header for printed table\n",
    "print(\"Earliest\", \"\\t    \", \"Latest\", \"\\t\\t\", \"Source\")\n",
    "# Iterate across each source type\n",
    "for cat in cat_source:\n",
    "    # If the source is NaN\n",
    "    if pd.isna(cat) == True:\n",
    "        temp_subset = data_raw_weir[data_raw_weir['source'].isnull()]\n",
    "    else:\n",
    "        temp_subset = data_raw_weir[data_raw_weir['source'] == cat]\n",
    "    # Sort index\n",
    "    temp_subset = temp_subset\n",
    "    # Print\n",
    "    print(temp_subset.index[0], \"\", temp_subset.index[-1], cat)\n",
    "\n",
    "# Save space, remove no longer needed items\n",
    "del cat_source, cat, temp_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8d938c",
   "metadata": {},
   "source": [
    "#### CHART removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b191d88",
   "metadata": {},
   "source": [
    "Only values that are not solely reliant on CHART (i.e., after 1989) will be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a22e4d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non- CHART-only values: 1989-07-19 11:55:00 through 2025-08-01 13:00:00\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset to start once values stopped by being recorded by CHART\n",
    "date_weir_start = data_raw_weir[\n",
    "    # Remove CHART values\n",
    "    (data_raw_weir['source'] != 'CHART') &\n",
    "    # and\n",
    "    # Remove values without indicated source\n",
    "    (~data_raw_weir['source'].isnull())\n",
    "    # Pull earliest timestamp\n",
    "    ].index[0]\n",
    "# Get latest data point timestamp\n",
    "date_weir_end = data_raw_weir.index[-1]\n",
    "\n",
    "# Sanity check: it is expected that the start timestamp will be CHART+AF source\n",
    "if date_weir_start != data_raw_weir[data_raw_weir['source'] == 'CHART+AF'].index[0]:\n",
    "    print(\"-----!! Warning: Check start date !!-----\",\n",
    "          \"Calculated:\\t\", date_weir_start, \"\\n\"\n",
    "          \"Actual:\\t\\t\", data_raw_weir[data_raw_weir['source'] == 'CHART+AF'].index[0], \"\\n\")\n",
    "\n",
    "print(\"Non- CHART-only values:\", date_weir_start, \"through\", date_weir_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ffcf91",
   "metadata": {},
   "source": [
    "#### 2-Year Failure\n",
    "\n",
    "The ISCO sensor failed in early 2013, and there was no backup.\n",
    "Values were recording using the CHART resource, and gap filled accordingly.\n",
    "Electronic recording resumed with RADAR in late 2014.\n",
    "\n",
    "The model cannot be trained on this gap of data, as it is using `CHART` values, and all `raw` values report `-999.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b502b071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two year gap: 2013-01-13 05:54:01 through 2014-08-22 10:21:32\n"
     ]
    }
   ],
   "source": [
    "# data_gap = data_raw_weir['2013-01-01 00:00:00':'2014-08-22 23:59:59']\n",
    "# Isolate the rough start and stop dates of the gap\n",
    "data_gap = data_raw_weir['2013-01-13 05:00:00':'2014-08-22 23:59:59'].copy()\n",
    "\n",
    "# Get the earliest date of gap filling\n",
    "date_gap_start = data_gap[data_gap['source'] == 'CHART'].index[0]\n",
    "\n",
    "# Get the latest date of gap filling\n",
    "date_gap_end = data_gap[data_gap['source'] != 'RADAR'].index[-1]\n",
    "\n",
    "print(\"Two year gap:\", date_gap_start, \"through\", date_gap_end)\n",
    "## OLD EXPECTED -- 2013-01-02 18:54:38 through 2014-08-22 10:21:32\n",
    "## ADJ EXPECTED -- 2013-01-13 05:54:01 through 2014-08-22 10:21:32\n",
    "\n",
    "# Cleanup\n",
    "del data_gap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa52c2f1",
   "metadata": {},
   "source": [
    "#### COVID\n",
    "\n",
    "Ten months are missing from soil data in 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc639919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten month gap: 2020-02-27 00:00:00 through 2020-12-22 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Isolate the rough start and stop dates of the gap\n",
    "# Note: these dates are the same in the shallow set\n",
    "data_gap = data_raw_soil_deep['2020-02-15 00:00:01':'2020-12-31 23:23:59'].copy()\n",
    "\n",
    "# Get the latest date of gap filling\n",
    "date_gap_end_soil = data_gap.index[-1]\n",
    "\n",
    "data_gap = data_gap.drop(date_gap_end_soil)\n",
    "\n",
    "# Get the earliest date of missing data\n",
    "date_gap_start_soil = data_gap.index[-1]\n",
    "\n",
    "print(\"Ten month gap:\", date_gap_start_soil, \"through\", date_gap_end_soil)\n",
    "## EXPECTED -- 2020-02-27\t - 2020-12-22\t\n",
    "\n",
    "# Cleanup\n",
    "del data_gap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fdc059",
   "metadata": {},
   "source": [
    "#### Applying\n",
    "\n",
    "Removing larger gaps of irrelevant data help with memory and feature engineering later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4157eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to filter dates\n",
    "def filter_dates(input_dataset, input_date_start, input_date_end, drop_dates = False):\n",
    "# def filter_dates(input_dataset, input_date_start = date_weir_start, input_date_end = date_weir_end, drop_dates = False):\n",
    "    \"\"\"Filter data set to specified start and end dates.\n",
    "    \n",
    "    Args:\n",
    "        input_dataset (pd.DataFrame): Data indexed by datetime.\n",
    "        input_date_start (timestamp): The start date, defaults to the earliest from the combined data set.\n",
    "        input_date_end (timestamp): The end date, defaults to the earliest from the combined data set.\n",
    "        drop_dates (bool): Whether to remove the values between the specified dates.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Sorted and filtered dataset to or without the specified range.\n",
    "    \"\"\"\n",
    "    # Sort the dataframe\n",
    "    # data_filtered = input_dataset.copy().sort_index()\n",
    "    data_filtered = input_dataset.sort_index()\n",
    "    # Filter to select inputted dates\n",
    "    if drop_dates == False:\n",
    "        # data_subset = data_subset.loc[input_date_start:input_date_end]\n",
    "        data_filtered = data_filtered[input_date_start:input_date_end]\n",
    "    # Remove data between inputted dates\n",
    "    else:\n",
    "        data_filtered = data_filtered.drop(data_filtered.loc[input_date_start:input_date_end].index)\n",
    "    return data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "694211f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_window(input_dataset, input_timestamp_start, input_timestamp_end):\n",
    "#     \"\"\"Remove window in data set between specified start and end dates.\n",
    "    \n",
    "#     Args:\n",
    "#         input_dataset (pd.DataFrame): Data indexed and sorted by datetime.\n",
    "#         input_timestamp_start (Timestamp): The timestamp for which to start removal.\n",
    "#         input_timestamp_end (Timestamp): The final timestamp to removal.\n",
    "    \n",
    "#     Returns:\n",
    "#         pd.DataFrame sorted and filtered without the specified range.\n",
    "#     \"\"\"\n",
    "#     ## Sort the dataframe\n",
    "#     # data_subset = input_dataset.sort_index()\n",
    "#     # Remove the specified time window by dropping indices within the range\n",
    "#     data_filtered = input_dataset.drop(input_dataset.loc[input_timestamp_start:input_timestamp_end].index)\n",
    "#     return data_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3c676b",
   "metadata": {},
   "source": [
    "For large gaps, it may still be useful for the model to keep predictor variables in the time leading up to the useful weir data.\n",
    "\n",
    "Thus, the rainfall and soil moisture data up to one month prior to any hard cutoff will be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58c3284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify data removal\n",
    "def apply_filter_dates(input_dataset, input_adj = '0 D'):\n",
    "    \"\"\"Apply pre-defined date filters.\n",
    "    \n",
    "    Args:\n",
    "        input_dataset (pd.DataFrame): Data indexed by datetime.\n",
    "        input_adj (str): String argument to account for time leading up to weir data, defaults to none.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame filtered to the weir range and without the 2-year gap window.\n",
    "    \"\"\"\n",
    "    # data_subset = input_dataset[(date_weir_start - pd.Timedelta(input_adj)):date_weir_end]\n",
    "    # data_subset = remove_window(input_dataset = data_subset, input_timestamp_start = date_gap_start, input_timestamp_end = (date_gap_end - pd.Timedelta(input_adj)))\n",
    "    # Sensor data\n",
    "    data_adj = filter_dates(\n",
    "        input_dataset = input_dataset,\n",
    "        input_date_start = (date_weir_start - pd.Timedelta(input_adj)),\n",
    "        input_date_end = date_weir_end,\n",
    "        drop_dates = False\n",
    "    )\n",
    "    # 2-yr gap\n",
    "    data_adj = filter_dates(\n",
    "        input_dataset = data_adj,\n",
    "        input_date_start = date_gap_start,\n",
    "        input_date_end = (date_gap_end - pd.Timedelta(input_adj)),\n",
    "        drop_dates = True\n",
    "    )\n",
    "    # Missing soil samples\n",
    "    data_adj = filter_dates(\n",
    "        input_dataset = data_adj,\n",
    "        input_date_start = (date_gap_start_soil + pd.Timedelta('1 W')),\n",
    "        input_date_end = (date_gap_end_soil - pd.Timedelta('1 D')),\n",
    "        drop_dates = True\n",
    "    )\n",
    "    # data_subset = input_dataset[date_weir_start:date_weir_end]\n",
    "    # data_subset = remove_window(input_dataset = data_subset, input_timestamp_start = date_gap_start, input_timestamp_end = date_gap_end)\n",
    "    return data_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d3fb029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filter\n",
    "# gap_adj = '4 W'\n",
    "gap_adj = '2 W'\n",
    "\n",
    "data_weir = apply_filter_dates(data_raw_weir)\n",
    "data_calibration = apply_filter_dates(data_raw_calibration)\n",
    "\n",
    "# Include data leading up to the weir values and shortly before the end of the 2-yr gap\n",
    "data_rainfall = apply_filter_dates(data_raw_rainfall, gap_adj)\n",
    "data_soil_shallow = apply_filter_dates(data_raw_soil_shallow, gap_adj)\n",
    "data_soil_deep = apply_filter_dates(data_raw_soil_deep, gap_adj)\n",
    "\n",
    "del gap_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ea89ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove old stuff to save space\n",
    "del data_raw_calibration, data_raw_weir, data_raw_rainfall, data_raw_soil_shallow, data_raw_soil_deep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef07c2d",
   "metadata": {},
   "source": [
    "### Soil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029c00c5",
   "metadata": {},
   "source": [
    "#### Duplicates\n",
    "\n",
    "There are some duplicated records between the \"shallow\" and \"deep\" data set. Most are identical, but there were two dates with differing records.\n",
    "It was concluded that those values from the \"deep\" set with a depth of \"0â€“10\" may be eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2887432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>depth_shallow</th>\n",
       "      <th>depth_deep</th>\n",
       "      <th>sample</th>\n",
       "      <th>h2o_by_wet_shallow</th>\n",
       "      <th>h2o_by_wet_deep</th>\n",
       "      <th>chk_note_shallow</th>\n",
       "      <th>chk_note_deep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2005-06-16</td>\n",
       "      <td>1-10</td>\n",
       "      <td>0-10</td>\n",
       "      <td>1</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>40.799999</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2005-06-16</td>\n",
       "      <td>1-10</td>\n",
       "      <td>0-10</td>\n",
       "      <td>2</td>\n",
       "      <td>37.799999</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2005-06-16</td>\n",
       "      <td>1-10</td>\n",
       "      <td>0-10</td>\n",
       "      <td>3</td>\n",
       "      <td>36.099998</td>\n",
       "      <td>36.400002</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2005-06-16</td>\n",
       "      <td>1-10</td>\n",
       "      <td>0-10</td>\n",
       "      <td>4</td>\n",
       "      <td>38.400002</td>\n",
       "      <td>38.599998</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2005-06-16</td>\n",
       "      <td>1-10</td>\n",
       "      <td>0-10</td>\n",
       "      <td>5</td>\n",
       "      <td>37.700001</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2005-06-16</td>\n",
       "      <td>1-10</td>\n",
       "      <td>0-10</td>\n",
       "      <td>6</td>\n",
       "      <td>37.900002</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2005-06-16</td>\n",
       "      <td>1-10</td>\n",
       "      <td>0-10</td>\n",
       "      <td>7</td>\n",
       "      <td>32.700001</td>\n",
       "      <td>34.099998</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2005-06-16</td>\n",
       "      <td>1-10</td>\n",
       "      <td>0-10</td>\n",
       "      <td>8</td>\n",
       "      <td>28.100000</td>\n",
       "      <td>28.799999</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2005-06-16</td>\n",
       "      <td>1-10</td>\n",
       "      <td>0-10</td>\n",
       "      <td>9</td>\n",
       "      <td>34.200001</td>\n",
       "      <td>33.200001</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2005-06-16</td>\n",
       "      <td>1-10</td>\n",
       "      <td>0-10</td>\n",
       "      <td>10</td>\n",
       "      <td>32.099998</td>\n",
       "      <td>32.200001</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2006-03-24</td>\n",
       "      <td>1-10</td>\n",
       "      <td>0-10</td>\n",
       "      <td>6</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>38.500000</td>\n",
       "      <td>duplicate</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date depth_shallow depth_deep  sample  h2o_by_wet_shallow  \\\n",
       "129 2005-06-16          1-10       0-10       1           43.000000   \n",
       "128 2005-06-16          1-10       0-10       2           37.799999   \n",
       "127 2005-06-16          1-10       0-10       3           36.099998   \n",
       "126 2005-06-16          1-10       0-10       4           38.400002   \n",
       "125 2005-06-16          1-10       0-10       5           37.700001   \n",
       "124 2005-06-16          1-10       0-10       6           37.900002   \n",
       "123 2005-06-16          1-10       0-10       7           32.700001   \n",
       "122 2005-06-16          1-10       0-10       8           28.100000   \n",
       "121 2005-06-16          1-10       0-10       9           34.200001   \n",
       "120 2005-06-16          1-10       0-10      10           32.099998   \n",
       "134 2006-03-24          1-10       0-10       6           38.000000   \n",
       "\n",
       "     h2o_by_wet_deep chk_note_shallow chk_note_deep  \n",
       "129        40.799999             good          good  \n",
       "128        36.500000             good          good  \n",
       "127        36.400002             good          good  \n",
       "126        38.599998             good          good  \n",
       "125        37.500000             good          good  \n",
       "124        37.500000             good          good  \n",
       "123        34.099998             good          good  \n",
       "122        28.799999             good          good  \n",
       "121        33.200001             good          good  \n",
       "120        32.200001             good          good  \n",
       "134        38.500000        duplicate          good  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the values in the deep data set that have the shallower depth\n",
    "data_deep_subset = data_soil_deep[data_soil_deep[\"depth\"] != \"30-40\"]\n",
    "\n",
    "# Filter set to only be of dates where deep set has shallow values\n",
    "data_shallow_subset = data_soil_shallow[data_soil_shallow.index.isin(data_deep_subset.index)]\n",
    "\n",
    "# Inner merge based on date and sample number\n",
    "data_soil_mismatch = pd.merge(\n",
    "    # Set the timestamp to be a column, for merging\n",
    "    data_deep_subset.reset_index(),\n",
    "    data_shallow_subset.reset_index(),\n",
    "    # Unite based on timestamp and sample number\n",
    "    on = [\"date\", \"sample\"],\n",
    "    # Flags\n",
    "    suffixes = (\"_deep\", \"_shallow\"),\n",
    "    how = \"inner\"\n",
    "    )\n",
    "\n",
    "# Create a variable to indicate if the values match\n",
    "data_soil_mismatch[\"match_wet\"] = (data_soil_mismatch[\"h2o_by_wet_deep\"] == data_soil_mismatch[\"h2o_by_wet_shallow\"])\n",
    "\n",
    "## The dry var was not loaded in this analysis, but the exact same issue occurred in it (i.e., the same dates had mismatching values)\n",
    "# match_all[\"match_dry\"] = (match_all[\"h2o_by_dry_deep\"] == match_all[\"h2o_by_dry_shallow\"])\n",
    "\n",
    "# Set the sample var to be an integer, for sorting purposes\n",
    "data_soil_mismatch[\"sample\"] = data_soil_mismatch[\"sample\"].astype('int')\n",
    "\n",
    "# Sort by date and sample for readability\n",
    "data_soil_mismatch = data_soil_mismatch.sort_values(by = ['date', 'sample'])\n",
    "\n",
    "# Remove unneeded columns\n",
    "data_soil_mismatch = data_soil_mismatch.drop(['chk_fail_shallow', 'chk_fail_deep'], axis=1)\n",
    "\n",
    "# Filter where there is a mismatch\n",
    "data_soil_mismatch = data_soil_mismatch[(data_soil_mismatch[\"match_wet\"] == False)]\n",
    "\n",
    "# Reordering vars for readability\n",
    "data_soil_mismatch = data_soil_mismatch[['date', 'depth_shallow', 'depth_deep', 'sample', 'h2o_by_wet_shallow', 'h2o_by_wet_deep', 'chk_note_shallow', 'chk_note_deep']]\n",
    "\n",
    "# Print result\n",
    "data_soil_mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd774997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del data_shallow_subset, data_deep_subset, data_soil_mismatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c1e8f",
   "metadata": {},
   "source": [
    "It was confirmed that values at depth `0-10` in the \"deep\" data set (including those that do not match the equivalent in the \"shallow\" set) can be disregarded and excluded from analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a101b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the duplicated samples\n",
    "data_soil_deep = data_soil_deep[data_soil_deep[\"depth\"] != \"0-10\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfe92a0",
   "metadata": {},
   "source": [
    "#### Abnormal depths\n",
    "\n",
    "Additional values are reported for a few different, non-standard depths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4842cd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Shallow-----\n",
      "depth\n",
      "1-10    11489\n",
      "Name: h2o_by_wet, dtype: int64\n",
      "\n",
      "\n",
      "-----Deep-----\n",
      "depth\n",
      "30-40    11489\n",
      "Name: h2o_by_wet, dtype: int64\n",
      "\n",
      "\n",
      "Unique time stamps:\n",
      "-----Shallow-----\n",
      "DatetimeIndex([], dtype='datetime64[ns]', name='date', freq=None)\n",
      "\n",
      "\n",
      "-----Deep-----\n",
      "DatetimeIndex([], dtype='datetime64[ns]', name='date', freq=None)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"-----Shallow-----\",\n",
    "    data_soil_shallow.groupby('depth', dropna=False, observed=True)['h2o_by_wet'].count(),\n",
    "    \"\\n\",\n",
    "    \"-----Deep-----\",\n",
    "    data_soil_deep.groupby('depth', dropna=False, observed=True)['h2o_by_wet'].count(),\n",
    "    sep=\"\\n\"\n",
    ")\n",
    "\n",
    "date_abnorm_shallow = data_soil_shallow[data_soil_shallow['depth']!='1-10'].index.unique()\n",
    "date_abnorm_deep = data_soil_deep[data_soil_deep['depth']!='30-40'].index.unique()\n",
    "\n",
    "print(\n",
    "    \"\\n\\nUnique time stamps:\",\n",
    "    \"-----Shallow-----\",\n",
    "    date_abnorm_shallow,\n",
    "    \"\\n\",\n",
    "    \"-----Deep-----\",\n",
    "    date_abnorm_deep,\n",
    "    sep=\"\\n\"\n",
    ")\n",
    "\n",
    "# data_soil_shallow.loc['1989-06-23 00:00:00']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb156593",
   "metadata": {},
   "source": [
    "Both sets have extra values on the same date. Before removal, it is wise to check that the expected depth samples are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8021b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Shallow-----\n",
      "depth\n",
      "0-5      0\n",
      "1-10     0\n",
      "10-20    0\n",
      "Name: h2o_by_wet, dtype: int64\n",
      "check:\n",
      "False\n",
      "\n",
      "\n",
      "-----Deep-----\n",
      "depth\n",
      "0-10     0\n",
      "10-20    0\n",
      "20-30    0\n",
      "30-40    0\n",
      "40-50    0\n",
      "Name: h2o_by_wet, dtype: int64\n",
      "check:\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"-----Shallow-----\",\n",
    "    data_soil_shallow.loc[date_abnorm_shallow].groupby('depth', dropna=False, observed=False)['h2o_by_wet'].count(),\n",
    "    \"check:\",\n",
    "    '1-10' in data_soil_shallow.loc[date_abnorm_shallow]['depth'].unique(),\n",
    "    \"\\n\",\n",
    "    \"-----Deep-----\",\n",
    "    data_soil_deep.loc[date_abnorm_deep].groupby('depth', dropna=False, observed=False)['h2o_by_wet'].count(),\n",
    "    \"check:\",\n",
    "    '30-40' in data_soil_deep.loc[date_abnorm_deep]['depth'].unique(),\n",
    "    sep=\"\\n\"\n",
    ")\n",
    "\n",
    "# '1-10' in data_soil_shallow.loc[date_abnorm_shallow]['depth'].unique()\n",
    "\n",
    "# '30-40' in data_soil_deep.loc[date_abnorm_deep]['depth'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753c606",
   "metadata": {},
   "source": [
    "The extra values will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee47f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the extra values by selecting only the depths of interest\n",
    "\n",
    "data_soil_shallow = data_soil_shallow[data_soil_shallow['depth'] == '1-10']\n",
    "\n",
    "data_soil_deep = data_soil_deep[data_soil_deep['depth'] == '30-40']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f5e7591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth\n",
      "0-5          0\n",
      "1-10     11489\n",
      "10-20        0\n",
      "Name: h2o_by_wet, dtype: int64 depth\n",
      "0-10         0\n",
      "10-20        0\n",
      "20-30        0\n",
      "30-40    11489\n",
      "40-50        0\n",
      "Name: h2o_by_wet, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify that all abnormal depths have been removed\n",
    "print(\n",
    "    data_soil_shallow.groupby('depth', dropna=False, observed=False)['h2o_by_wet'].count(),\n",
    "    data_soil_deep.groupby('depth', dropna=False, observed=False)['h2o_by_wet'].count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0b3af1",
   "metadata": {},
   "source": [
    "#### Abnormal locations\n",
    "\n",
    "Another anomaly occurs with non-standard samples location numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b88384f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depth</th>\n",
       "      <th>sample</th>\n",
       "      <th>h2o_by_wet</th>\n",
       "      <th>chk_note</th>\n",
       "      <th>chk_fail</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-03-02</th>\n",
       "      <td>1-10</td>\n",
       "      <td>9</td>\n",
       "      <td>29.100000</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-03-02</th>\n",
       "      <td>1-10</td>\n",
       "      <td>68</td>\n",
       "      <td>39.500000</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-03-02</th>\n",
       "      <td>1-10</td>\n",
       "      <td>10</td>\n",
       "      <td>29.900000</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-03-02</th>\n",
       "      <td>1-10</td>\n",
       "      <td>8</td>\n",
       "      <td>36.400002</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-03-02</th>\n",
       "      <td>1-10</td>\n",
       "      <td>70</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-03-02</th>\n",
       "      <td>1-10</td>\n",
       "      <td>6</td>\n",
       "      <td>40.099998</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-03-02</th>\n",
       "      <td>1-10</td>\n",
       "      <td>5</td>\n",
       "      <td>41.299999</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-03-02</th>\n",
       "      <td>1-10</td>\n",
       "      <td>4</td>\n",
       "      <td>35.099998</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-03-02</th>\n",
       "      <td>1-10</td>\n",
       "      <td>3</td>\n",
       "      <td>38.700001</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-03-02</th>\n",
       "      <td>1-10</td>\n",
       "      <td>7</td>\n",
       "      <td>36.400002</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           depth sample  h2o_by_wet chk_note chk_fail\n",
       "date                                                 \n",
       "2005-03-02  1-10      9   29.100000     good      NaN\n",
       "2005-03-02  1-10     68   39.500000     good      NaN\n",
       "2005-03-02  1-10     10   29.900000     good      NaN\n",
       "2005-03-02  1-10      8   36.400002     good      NaN\n",
       "2005-03-02  1-10     70   43.500000     good      NaN\n",
       "2005-03-02  1-10      6   40.099998     good      NaN\n",
       "2005-03-02  1-10      5   41.299999     good      NaN\n",
       "2005-03-02  1-10      4   35.099998     good      NaN\n",
       "2005-03-02  1-10      3   38.700001     good      NaN\n",
       "2005-03-02  1-10      7   36.400002     good      NaN"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get rows where the sample is not the standard 1 through 10\n",
    "data_soil_sample_abnorm = data_soil_shallow[~data_soil_shallow['sample'].astype('int').isin(list(range(1, 10+1, 1)))].copy()\n",
    "\n",
    "# Locate other entries from the same date\n",
    "data_soil_shallow.loc[data_soil_sample_abnorm.index.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f9521a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del data_soil_sample_abnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6847cca1",
   "metadata": {},
   "source": [
    "It is possible that the data entries for samples `1` and `2` were miscoded, however at the moment is is not possible to verify in which way. For now, the values will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50332bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-standard sample locations\n",
    "data_soil_shallow = data_soil_shallow[data_soil_shallow['sample'].astype('int').isin(list(range(1, 10+1, 1)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5684a77",
   "metadata": {},
   "source": [
    "#### General cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "319dcbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove values flagged as bad or doubtful\n",
    "data_soil_shallow = data_soil_shallow[\n",
    "    (data_soil_shallow['chk_note'] != 'bad') &\n",
    "    (data_soil_shallow['chk_note'] != 'doubtful')\n",
    "]\n",
    "\n",
    "data_soil_deep = data_soil_deep[\n",
    "    (data_soil_deep['chk_note'] != 'bad') &\n",
    "    (data_soil_deep['chk_note'] != 'doubtful')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3214a64",
   "metadata": {},
   "source": [
    "Remove duplicated entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebad5990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data_soil_shallow[data_soil_shallow.duplicated(subset=['sample', 'h2o_by_wet'])]\n",
    "# dss_dup = data_soil_shallow.copy()\n",
    "# dss_dup = dss_dup.reset_index()\n",
    "# dss_dup['dup'] = dss_dup.duplicated(subset=['date', 'sample', 'h2o_by_wet'])\n",
    "# # dss_dup[dss_dup['dup']==True]\n",
    "# # data_soil_shallow.loc['2023-07-14 00:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b02e01b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quick copy\n",
    "# dss_shallow = data_soil_shallow.copy()\n",
    "# dss_deep = data_soil_deep.copy()\n",
    "\n",
    "# # Reset indeces to allow for easier filtering\n",
    "# dss_shallow = dss_shallow.reset_index()\n",
    "# dss_deep = dss_deep.reset_index()\n",
    "\n",
    "# dss_shallow = dss_shallow[dss_shallow.duplicated(subset=['date', 'sample', 'h2o_by_wet']) == False]\n",
    "# dss_deep = dss_deep[dss_deep.duplicated(subset=['date', 'sample', 'h2o_by_wet']) == False]\n",
    "\n",
    "# print(len(dss_shallow), len(dss_deep))\n",
    "\n",
    "# dss_un = pd.merge(\n",
    "#     data_soil_shallow.reset_index(),\n",
    "#     data_soil_deep.reset_index(),\n",
    "#     on = [\"date\", \"sample\"],\n",
    "#     suffixes = (\"_shallow\", \"_deep\"),\n",
    "#     how = \"outer\"\n",
    "#     )\n",
    "\n",
    "# dss_un = dss_un[dss_un.duplicated(subset=['date', 'sample', 'h2o_by_wet_shallow', 'h2o_by_wet_deep']) == False]\n",
    "# print(len(dss_un))\n",
    "# # Return the index\n",
    "# # united_soil = united_soil.set_index('date')\n",
    "# # dss_dup = data_soil_shallow.copy()\n",
    "# # dss_dup = dss_dup.reset_index()\n",
    "# # dss_dup['dup'] = dss_dup.duplicated(subset=['date', 'sample', 'h2o_by_wet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6eaf56a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dss = data_soil_shallow.copy()\n",
    "# dsd = data_soil_deep.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45a02c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data_water = data_water.reset_index().drop_duplicates(keep='first').set_index('datetime')\n",
    "# # data_soil_shallow.head()\n",
    "# ds_test = dss.copy().reset_index()\n",
    "# ds_test = ds_test.drop_duplicates(subset=['date', 'sample', 'h2o_by_wet'], keep='first')\n",
    "# print(\n",
    "#     len(ds_test),\n",
    "#     len(ds_test[ds_test.duplicated(subset=['date', 'sample']) == False]),\n",
    "#     len(ds_test.drop_duplicates(subset=['date', 'sample'], keep='first')),\n",
    "#     len(ds_test.drop_duplicates(subset=['date', 'sample'], keep=False)),\n",
    "#     sep=\"\\n\"\n",
    "#     )\n",
    "\n",
    "# # ds_test = ds_test.drop_duplicates(subset=['date', 'sample'], keep=False)\n",
    "# # ds_test[ds_test.duplicated(subset=['date', 'sample']) == True]\n",
    "# # ds_test[ds_test['date']=='2023-04-06 00:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6102a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reset indeces to allow for easier filtering\n",
    "# data_soil_shallow = data_soil_shallow.reset_index()\n",
    "# data_soil_deep = data_soil_deep.reset_index()\n",
    "# print(\"\\tshallow\", \"deep\", sep=\"\\t\")\n",
    "# print(\"Before:\\t\", len(data_soil_shallow), len(data_soil_deep))\n",
    "\n",
    "# # Remove duplicate rows\n",
    "# data_soil_shallow = data_soil_shallow[data_soil_shallow.duplicated(subset=['date', 'sample', 'h2o_by_wet']) == False]\n",
    "# data_soil_deep = data_soil_deep[data_soil_deep.duplicated(subset=['date', 'sample', 'h2o_by_wet']) == False]\n",
    "# # data_soil_shallow = data_soil_shallow.drop_duplicates(subset=['date', 'sample', 'h2o_by_wet'], keep='first')\n",
    "# # data_soil_deep = data_soil_deep.drop_duplicates(subset=['date', 'sample', 'h2o_by_wet'], keep='first')\n",
    "# print(\"Non-dup:\", len(data_soil_shallow), len(data_soil_deep))\n",
    "\n",
    "# # Remove rows with multiple entries by date and site\n",
    "# data_soil_shallow = data_soil_shallow[data_soil_shallow.duplicated(subset=['date', 'sample']) == False]\n",
    "# data_soil_deep = data_soil_deep[data_soil_deep.duplicated(subset=['date', 'sample']) == False]\n",
    "# # data_soil_shallow = data_soil_shallow.drop_duplicates(subset=['date', 'sample'], keep=False)\n",
    "# # data_soil_deep = data_soil_deep.drop_duplicates(subset=['date', 'sample'], keep= False)\n",
    "# print(\"Singles:\", len(data_soil_shallow), len(data_soil_deep))\n",
    "\n",
    "# # Return indeces\n",
    "# data_soil_shallow = data_soil_shallow.set_index('date')\n",
    "# data_soil_deep = data_soil_deep.set_index('date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27b634f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tshallow\tdeep\n",
      "Before:\t 11432 11478\n",
      "Non-dup: 11012 11026\n",
      "Singles: 10952 11026\n"
     ]
    }
   ],
   "source": [
    "# Reset indeces to allow for easier filtering\n",
    "data_soil_shallow = data_soil_shallow.reset_index()\n",
    "data_soil_deep = data_soil_deep.reset_index()\n",
    "print(\"\\tshallow\", \"deep\", sep=\"\\t\")\n",
    "print(\"Before:\\t\", len(data_soil_shallow), len(data_soil_deep))\n",
    "\n",
    "# # Remove duplicate rows\n",
    "# data_soil_shallow = data_soil_shallow[data_soil_shallow.duplicated(subset=['date', 'sample', 'h2o_by_wet']) == False]\n",
    "# data_soil_deep = data_soil_deep[data_soil_deep.duplicated(subset=['date', 'sample', 'h2o_by_wet']) == False]\n",
    "\n",
    "# Remove duplicate rows\n",
    "data_soil_shallow = data_soil_shallow.drop_duplicates(subset=['date', 'sample', 'h2o_by_wet'], keep='first')\n",
    "data_soil_deep = data_soil_deep.drop_duplicates(subset=['date', 'sample', 'h2o_by_wet'], keep='first')\n",
    "print(\"Non-dup:\", len(data_soil_shallow), len(data_soil_deep))\n",
    "\n",
    "# # Remove rows with multiple entries by date and site\n",
    "# data_soil_shallow = data_soil_shallow[data_soil_shallow.duplicated(subset=['date', 'sample']) == False]\n",
    "# data_soil_deep = data_soil_deep[data_soil_deep.duplicated(subset=['date', 'sample']) == False]\n",
    "\n",
    "# Remove rows with multiple entries by date and site\n",
    "data_soil_shallow = data_soil_shallow.drop_duplicates(subset=['date', 'sample'], keep=False)\n",
    "data_soil_deep = data_soil_deep.drop_duplicates(subset=['date', 'sample'], keep= False)\n",
    "print(\"Singles:\", len(data_soil_shallow), len(data_soil_deep))\n",
    "\n",
    "# Return indeces\n",
    "data_soil_shallow = data_soil_shallow.set_index('date')\n",
    "data_soil_deep = data_soil_deep.set_index('date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e316c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_soil_shallow = data_soil_shallow.drop(columns=['depth', 'chk_note', 'chk_fail'])\n",
    "data_soil_deep = data_soil_deep.drop(columns=['depth', 'chk_note', 'chk_fail'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13e5cd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dss_dup = data_soil_shallow.reset_index()\n",
    "# dss_dup['dup'] = dss_dup.duplicated(subset=['date', 'sample'])\n",
    "# dss_dup.set_index('date').loc['2023-04-21 00:00:00']\n",
    "\n",
    "# data_soil_shallow.reset_index()[data_soil_shallow.reset_index().duplicated(subset=['date', 'sample'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4aa6cc",
   "metadata": {},
   "source": [
    "### Additional CHART Removals\n",
    "\n",
    "Only non-`CHART` values will be used for making the model.\n",
    "Prior to removing them entirely, other missing values must also be dealt with, as they can relate to gaps within CHART-reliant ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6fc94bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup\n",
    "# data_weir_chart = data_weir.copy()\n",
    "# data_weir_nochart = data_raw_weir.copy()\n",
    "\n",
    "# Create a column which will forward fill the source--i.e., fill NAs with the most recent value reported in 'source'\n",
    "data_weir['source_ffill'] = data_weir['source'].ffill()\n",
    "\n",
    "# Create a column which will back fill the source--i.e., fill NAs with the next value reported in 'source'\n",
    "data_weir['source_bfill'] = data_weir['source'].bfill()\n",
    "\n",
    "# Filtering to remove CHART values and gap fills that rely on CHART values\n",
    "data_weir = data_weir[\n",
    "    # Remove CHART values\n",
    "    (data_weir['source'] != \"CHART\") &\n",
    "    # Remove NA values where the most recent source was CHART\n",
    "    (data_weir['source_ffill'] != \"CHART\") &\n",
    "    # Remove NA values where the next source is CHART\n",
    "    (data_weir['source_bfill'] != \"CHART\")\n",
    "]\n",
    "\n",
    "# Remove extra variables\n",
    "data_weir = data_weir.drop(['source_ffill', 'source_bfill'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9895b11",
   "metadata": {},
   "source": [
    "### Missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1252b75",
   "metadata": {},
   "source": [
    "Other gaps of missing values occur and should be addressed.\n",
    "These can be identified by the `chk_note` of 'missing' with a `raw` values of -999.0.\n",
    "In the manually-adjusted results, the levels of such points have been set to 0.0.\n",
    "Because there are so few instances of these in the data set, they will simply be removed for this analysis.\n",
    "\n",
    "*A `chk_note` of 'missing' differs from instances of where a `chk_fail` is a 'Gap Fill'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe41facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_weir['2025-07-24 14:00:00':'2025-07-24 17:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b2c1bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 rows removed.\n"
     ]
    }
   ],
   "source": [
    "# data_weir[(data_weir['raw'] == -999.0) & (data_weir['chk_note'] == \"missing\")]\n",
    "# data_weir_chart['2024-11-15 10:50:00':'2024-11-15 11:15:00']\n",
    "# data_weir_chart['2024-11-22 11:10:00':'2024-11-22 11:45:00']\n",
    "# data_weir_chart['2025-07-24 15:10:00':'2025-07-24 16:25:00']\n",
    "\n",
    "# data_weir[\n",
    "#     (data_weir['raw'] == -999.0) &\n",
    "#     (data_weir['chk_note'] == \"missing\")\n",
    "# ]\n",
    "exp_len = len(data_weir[\n",
    "    (data_weir['raw'] == -999.0) &\n",
    "    (data_weir['chk_note'] == \"missing\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "len_before = len(data_weir)\n",
    "\n",
    "data_weir = data_weir[\n",
    "    # Invert results\n",
    "    ~(\n",
    "        # Selecting rows where raw value is -999.0\n",
    "        (data_weir['raw'] == -999.0) &\n",
    "        # and\n",
    "        # chk_note is missing\n",
    "        (data_weir['chk_note'] == \"missing\")\n",
    "    )\n",
    "]\n",
    "\n",
    "if len(data_weir) != (len_before-exp_len):\n",
    "    print(\"Check values!\")\n",
    "else:\n",
    "    print(len_before-len(data_weir), \"rows removed.\")\n",
    "\n",
    "del len_before, exp_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aadc03",
   "metadata": {},
   "source": [
    "### Failure Modes\n",
    "\n",
    "Some data points have multiple failure modes, and some entries contain duplicate flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5982b88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fail_bool(input_data, input_string):\n",
    "    \"\"\"Return a bool list where the chk_fail column contains a strong.\n",
    "\n",
    "    Args:\n",
    "        input_data (DataFrame): Data with a 'chk_fail' column to analyze.\n",
    "        input_string (str): String to find; case ignored.\n",
    "\n",
    "    Returns:\n",
    "        array: List of True/False corresponding to instances of string matches.\n",
    "    \"\"\"\n",
    "    # Return a bool list for an inputted data frame where the chk_fail column contains a string\n",
    "    # Ignores case, and reports NA values as False\n",
    "    return np.where(input_data['chk_fail'].str.contains(input_string, case = False, na = False), True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1214897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fail_isolate(input_data):\n",
    "    \"\"\"Completes the flagging process for the five major failure modes on an inputted data set.\n",
    "\n",
    "    Args:\n",
    "        input_data (DataFrame): Data with a 'chk_fail' column to analyze.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Data with individual boolean columns of the failure modes.\n",
    "    \"\"\"\n",
    "    # \"Obs\" is over \"Obstruction\" used because there are a few instances the flag is truncated.\n",
    "    input_data['obstruction'] = fail_bool(input_data = input_data, input_string = \"Obs\")\n",
    "    input_data['gap_fill'] = fail_bool(input_data = input_data, input_string = \"Gap Fill\")\n",
    "    # \"Weir\" is over \"Weir Cleaning\" used because there are a few instances the flag is truncated.\n",
    "    input_data['weir_cleaning'] = fail_bool(input_data = input_data, input_string = \"Weir\")\n",
    "    input_data['spike'] = fail_bool(input_data = input_data, input_string = \"Spike\")\n",
    "    # \"Calib\" is over \"Calibration\" used because there are a few instances the flag is truncated.\n",
    "    input_data['calibration'] = fail_bool(input_data = input_data, input_string = \"Calib\")\n",
    "\n",
    "    # Find all new bool columns\n",
    "    bool_cols = input_data.select_dtypes(include=['bool']).columns\n",
    "\n",
    "    # Convert those specific columns to the nullable 'boolean' dtype\n",
    "    input_data[bool_cols] = input_data[bool_cols].astype('boolean')\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca468899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3473198 entries, 1989-07-19 11:55:00 to 2025-08-01 13:00:00\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Dtype   \n",
      "---  ------         -----   \n",
      " 0   level          float32 \n",
      " 1   raw            float32 \n",
      " 2   chk_note       category\n",
      " 3   chk_fail       category\n",
      " 4   comment        category\n",
      " 5   source         category\n",
      " 6   obstruction    boolean \n",
      " 7   gap_fill       boolean \n",
      " 8   weir_cleaning  boolean \n",
      " 9   spike          boolean \n",
      " 10  calibration    boolean \n",
      "dtypes: boolean(5), category(4), float32(2)\n",
      "memory usage: 99.4 MB\n"
     ]
    }
   ],
   "source": [
    "data_weir = fail_isolate(data_weir)\n",
    "\n",
    "data_weir.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff1dc52",
   "metadata": {},
   "source": [
    "## Uniting & Exporting\n",
    "\n",
    "The calibration, combined (runoff), and rainfall data can be united into a single data frame.\n",
    "Soil samples do not have the same granularity, so can be stored separately from these so as to avoid duplicated values.\n",
    "\n",
    "Save the resulting cleaned data frames to parquet files for ease of loading in later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b26b2a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 11042 entries, 1989-07-06 to 2025-06-26\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count  Dtype   \n",
      "---  ------              --------------  -----   \n",
      " 0   sample              11042 non-null  category\n",
      " 1   h2o_by_wet_shallow  10952 non-null  float32 \n",
      " 2   h2o_by_wet_deep     11026 non-null  float32 \n",
      "dtypes: category(1), float32(2)\n",
      "memory usage: 183.7 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>h2o_by_wet_shallow</th>\n",
       "      <th>h2o_by_wet_deep</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1989-07-06</th>\n",
       "      <td>1</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>35.799999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-07-06</th>\n",
       "      <td>2</td>\n",
       "      <td>44.099998</td>\n",
       "      <td>39.599998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-07-06</th>\n",
       "      <td>3</td>\n",
       "      <td>37.099998</td>\n",
       "      <td>31.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-07-06</th>\n",
       "      <td>4</td>\n",
       "      <td>37.700001</td>\n",
       "      <td>34.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-07-06</th>\n",
       "      <td>5</td>\n",
       "      <td>43.799999</td>\n",
       "      <td>39.799999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sample  h2o_by_wet_shallow  h2o_by_wet_deep\n",
       "date                                                  \n",
       "1989-07-06      1           36.500000        35.799999\n",
       "1989-07-06      2           44.099998        39.599998\n",
       "1989-07-06      3           37.099998        31.100000\n",
       "1989-07-06      4           37.700001        34.500000\n",
       "1989-07-06      5           43.799999        39.799999"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "united_soil = pd.merge(\n",
    "    data_soil_shallow.reset_index(),\n",
    "    data_soil_deep.reset_index(),\n",
    "    on = [\"date\", \"sample\"],\n",
    "    suffixes = (\"_shallow\", \"_deep\"),\n",
    "    how = \"outer\"\n",
    "    )\n",
    "\n",
    "# Return the index\n",
    "united_soil = united_soil.set_index('date')\n",
    "\n",
    "# Modifying sample to int for sorting\n",
    "united_soil[\"sample\"] = united_soil[\"sample\"].astype('int')\n",
    "\n",
    "# Sorting for readability\n",
    "united_soil = united_soil.sort_values(by=['date', 'sample'])\n",
    "\n",
    "# Reset to category\n",
    "united_soil[\"sample\"] = united_soil[\"sample\"].astype('category')\n",
    "\n",
    "# Moving sample to front of data frame\n",
    "soil_samples = united_soil.pop('sample')\n",
    "united_soil.insert(0, 'sample', soil_samples)\n",
    "del soil_samples\n",
    "#\n",
    "\n",
    "united_soil.info(memory_usage='deep')\n",
    "united_soil.head()\n",
    "# Missing values:\n",
    "# united_soil[united_soil['h2o_by_wet_shallow'].isnull() | united_soil['h2o_by_wet_deep'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1101fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export soil data\n",
    "united_soil.to_parquet(get_path('clean/soil.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35ff3d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checking column matching\n",
    "# # Checking to make sure sources match\n",
    "# def check_cols(input_df, input_col_left, input_col_right, find_mismatch=True):\n",
    "#     input_df[\"match\"] = (input_df[input_col_left] == input_df[input_col_right]) | (input_df[input_col_left].isnull() & input_df[input_col_right].isnull())\n",
    "#     if find_mismatch == True:\n",
    "#         input_df = input_df[(input_df[\"match\"]==False)]\n",
    "#     return input_df\n",
    "\n",
    "# check_cols(mini_united, \"source_ro\", \"source_rain\")\n",
    "# check_cols(mini_united, \"chk_note_rain\", \"chk_note_ro\")\n",
    "# check_cols(mini_united, \"comment_rain\", \"comment_ro\")\n",
    "\n",
    "# check_cols(mini_united, 'chk_note_rain', 'chk_note_ro')\n",
    "# mini_united.dropna(subset=\"chk_note_rain\")\n",
    "# check_cols(mini_united, 'chk_fail_rain', 'chk_fail_rain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5fb28766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine the rainfall and weir data\n",
    "# united_water_nocal = pd.merge(\n",
    "#     # Add identifiers to column names\n",
    "#     data_rainfall.add_suffix(\"_rain\"),\n",
    "#     data_weir.add_suffix(\"_ro\"),\n",
    "#     # Merge based on index (datetime)\n",
    "#     left_index=True,\n",
    "#     right_index=True,\n",
    "#     # Keep all data from both frames\n",
    "#     how='outer'\n",
    "# )\n",
    "\n",
    "# united_water_nocal.info(memory_usage='deep')\n",
    "\n",
    "# # Combine the calibration data with the above merged data\n",
    "# united_water = pd.merge(\n",
    "#     # Add identifiers to column names from calibration set\n",
    "#     data_calibration.add_suffix(\"_cal\"),\n",
    "#     united_water_nocal,#.copy(),\n",
    "#     # Merge based on index (datetime)\n",
    "#     left_index=True,\n",
    "#     right_index=True,\n",
    "#     # Keep all data from both frames\n",
    "#     how='outer'\n",
    "# )\n",
    "\n",
    "# united_water_nocal.info(memory_usage='deep')\n",
    "# united_water.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc3ec1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3497851 entries, 1989-07-05 14:00:00 to 2025-08-01 13:00:00\n",
      "Data columns (total 15 columns):\n",
      " #   Column            Dtype   \n",
      "---  ------            -----   \n",
      " 0   ra_rain           float32 \n",
      " 1   raw_rain          float32 \n",
      " 2   chk_note_rain     category\n",
      " 3   chk_fail_rain     category\n",
      " 4   level_ro          float32 \n",
      " 5   raw_ro            float32 \n",
      " 6   chk_note_ro       category\n",
      " 7   chk_fail_ro       category\n",
      " 8   comment_ro        category\n",
      " 9   source_ro         category\n",
      " 10  obstruction_ro    boolean \n",
      " 11  gap_fill_ro       boolean \n",
      " 12  weir_cleaning_ro  boolean \n",
      " 13  spike_ro          boolean \n",
      " 14  calibration_ro    boolean \n",
      "dtypes: boolean(5), category(6), float32(4)\n",
      "memory usage: 133.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Combine the rainfall and weir data\n",
    "united_water_nocal = pd.merge(\n",
    "    # Add identifiers to column names\n",
    "    data_rainfall.add_suffix(\"_rain\"),\n",
    "    data_weir.add_suffix(\"_ro\"),\n",
    "    # Merge based on index (datetime)\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    # Keep all data from both frames\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "united_water_nocal.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e9d71cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export water data (includes rain and weir info)\n",
    "united_water_nocal.to_parquet(get_path('clean/water_nocal.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "effa5b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export calibration data (isolated)\n",
    "data_calibration.to_parquet(get_path('clean/calibration.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ddcbcdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3502113 entries, 1989-07-05 14:00:00 to 2025-08-01 13:00:00\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Dtype   \n",
      "---  ------            -----   \n",
      " 0   weir_level_cal    Int8    \n",
      " 1   ra_rain           float32 \n",
      " 2   raw_rain          float32 \n",
      " 3   chk_note_rain     category\n",
      " 4   chk_fail_rain     category\n",
      " 5   level_ro          float32 \n",
      " 6   raw_ro            float32 \n",
      " 7   chk_note_ro       category\n",
      " 8   chk_fail_ro       category\n",
      " 9   comment_ro        category\n",
      " 10  source_ro         category\n",
      " 11  obstruction_ro    boolean \n",
      " 12  gap_fill_ro       boolean \n",
      " 13  weir_cleaning_ro  boolean \n",
      " 14  spike_ro          boolean \n",
      " 15  calibration_ro    boolean \n",
      "dtypes: Int8(1), boolean(5), category(6), float32(4)\n",
      "memory usage: 140.3 MB\n"
     ]
    }
   ],
   "source": [
    "united_water = pd.merge(\n",
    "    # Add identifiers to column names from calibration set\n",
    "    data_calibration.add_suffix(\"_cal\"),\n",
    "    united_water_nocal,#.copy(),\n",
    "    # Merge based on index (datetime)\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    # Keep all data from both frames\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "united_water.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5b5a8141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export water data (includes rain, weir, AND calibrations)\n",
    "united_water.to_parquet(get_path('clean/water.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "adf8945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.merge(\n",
    "#     # Add identifiers to column names\n",
    "#     united_water_nocal.memory_usage().to_frame().add_suffix(\"_nocal\"),\n",
    "#     united_water.memory_usage().to_frame().add_suffix(\"_cal\"),\n",
    "#     # Merge based on index (datetime)\n",
    "#     left_index=True,\n",
    "#     right_index=True,\n",
    "#     # Keep all data from both frames\n",
    "#     how='outer'\n",
    "# )\n",
    "\n",
    "# # united_water_nocal.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af01200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# united_soil['2020-01-01 00:00:00':'2021-12-31 23:59:59']\n",
    "# united_water['1996-11-01 00:00:00':'1997-01-31 23:59:59']\n",
    "# 1996-11-05 12:10:00 -- 1997-01-29 08:34:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "54fdcb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Water data (not including calibration points)\n",
    "# united_waternocal = pd.merge(\n",
    "#     data_rainfall.add_suffix(\"_rain\"), data_weir.add_suffix(\"_ro\"), left_index=True, right_index=True, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487aca99",
   "metadata": {},
   "source": [
    "<!-- ## Export\n",
    "\n",
    "Save the resulting cleaned data frames to parquet files for ease of loading in later analysis. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "778ddbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(get_path('clean/water.parquet'))\n",
    "# united_soil.to_parquet(get_path('clean/TEST.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dfcfac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # united_water.to_parquet('data/clean/water.parquet')\n",
    "# # united_soil.to_parquet('data/clean/soil.parquet')\n",
    "\n",
    "# # Water data (includes rain and weir info)\n",
    "# united_water_nocal.to_parquet(get_path('clean/water_nocal.parquet'))\n",
    "\n",
    "# # Water data (includes rain, weir, and calibrations)\n",
    "# united_water.to_parquet(get_path('clean/water.parquet'))\n",
    "\n",
    "# # Soil data\n",
    "# united_soil.to_parquet(get_path('clean/soil.parquet'))\n",
    "\n",
    "# # Calibration data (isolated)\n",
    "# data_calibration.to_parquet(get_path('clean/calibration.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7669c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data_calibration.to_parquet('data/clean/calibration.parquet')\n",
    "# # Calibration data (isolated)\n",
    "# data_calibration.to_parquet(get_path('clean/calibration.parquet'))\n",
    "\n",
    "# united_water_nocal.to_parquet(get_path('clean/water_nocal.parquet'))\n",
    "# # united_waternocal.to_parquet('data/clean/water_nocal.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_jupyter (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
