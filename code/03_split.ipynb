{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68a3c4a0",
   "metadata": {},
   "source": [
    "# Data Splitting\n",
    "\n",
    "Author: Gillian A. McGinnis, final-semester M.S. Information Science - Machine Learning  \n",
    "The University of Arizona College of Information  \n",
    "INFO 698 - Capstone  \n",
    "Start date: 21 October 2025  \n",
    "Last updated: 20 November 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d80b978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nModule providing code for test/train split and sliding window creation. Relies on 01_clean.ipynb completion.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Module providing code for test/train split and sliding window creation. Relies on 01_clean.ipynb completion.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de84a462",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33ab0f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_of_interest = \"obstruction_ro\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7208265e",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb557fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import randint, uniform\n",
    "# # import matplotlib.ticker as ticker\n",
    "# import matplotlib.dates as mdates\n",
    "# import datetime as dt\n",
    "# from datetime import date\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split, RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, ConfusionMatrixDisplay, f1_score, precision_score, recall_score, accuracy_score, precision_recall_curve, make_scorer\n",
    "\n",
    "from skforecast.recursive import ForecasterRecursive\n",
    "from skforecast.model_selection import backtesting_forecaster, TimeSeriesFold\n",
    "\n",
    "# For saving models\n",
    "import joblib\n",
    "\n",
    "# For data importing\n",
    "import os\n",
    "from helper_utils import get_path, model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb5d8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details>\n",
       "<summary>Click to view session information</summary>\n",
       "<pre>\n",
       "-----\n",
       "helper_utils        NA\n",
       "joblib              1.5.2\n",
       "matplotlib          3.10.7\n",
       "numpy               2.3.5\n",
       "pandas              2.3.3\n",
       "scipy               1.16.3\n",
       "session_info        v1.0.1\n",
       "skforecast          0.18.0\n",
       "sklearn             1.7.2\n",
       "xgboost             3.1.2\n",
       "-----\n",
       "IPython             9.7.0\n",
       "jupyter_client      8.6.3\n",
       "jupyter_core        5.9.1\n",
       "jupyterlab          4.5.0\n",
       "notebook            7.5.0\n",
       "-----\n",
       "Python 3.13.7 (v3.13.7:bcee1c32211, Aug 14 2025, 19:10:51) [Clang 16.0.0 (clang-1600.0.26.6)]\n",
       "macOS-15.6.1-x86_64-i386-64bit-Mach-O\n",
       "-----\n",
       "Session information updated at 2025-11-21 12:14\n",
       "</pre>\n",
       "</details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## (Optional chunk)\n",
    "# Current session information\n",
    "import session_info\n",
    "session_info.show(dependencies=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c40baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make it easier to tell when processes have completed\n",
    "from playsound3 import playsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f1ce6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3+3\n",
    "\n",
    "# _ = playsound(get_path('completed.mp3', 'code'), block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef4832",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd2e4fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# united_water = pd.read_parquet('data/clean/water_nocal.parquet')\n",
    "# data_cal = pd.read_parquet('data/clean/calibration.parquet')\n",
    "# data_cal = data_cal.rename(columns={'weir_level':'weir_level_cal'})\n",
    "\n",
    "# united_soil = pd.read_parquet('data/clean/soil.parquet')\n",
    "\n",
    "united_water = pd.read_parquet(get_path('clean/water_nocal.parquet'))\n",
    "united_soil = pd.read_parquet(get_path('clean/soil.parquet'))\n",
    "\n",
    "# united_water = pd.read_parquet('data/clean/water_nocal.parquet')\n",
    "data_cal = pd.read_parquet(get_path('clean/calibration.parquet'))\n",
    "data_cal = data_cal.rename(columns={'weir_level':'weir_level_cal'})\n",
    "\n",
    "# united_soil = pd.read_parquet('data/clean/soil.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f94e4",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Small amount of data wrangling for memory improvements (some as a consequence of importing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf48710",
   "metadata": {},
   "source": [
    "#### Memory improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73854a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Reduce size\n",
    "# def mod_float(input_df):\n",
    "#     output_df = input_df#.copy()\n",
    "#     for col in output_df.select_dtypes(include = [np.float64, 'Float64']).columns:\n",
    "#         # print(col)\n",
    "#         output_df[col] = output_df[col].astype(np.float32)\n",
    "#     return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8ccb562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Convert T/F\n",
    "# def mod_obj(input_df):\n",
    "#     output_df = input_df#.copy()\n",
    "#     for col in output_df.select_dtypes(include=['object']).columns:\n",
    "#         u_vals = output_df[col].unique()\n",
    "#         u_vals_nonna = [u for u in u_vals if pd.notna(u)]\n",
    "#         isboolean = all(isinstance(u_n, bool) for u_n in u_vals_nonna)\n",
    "#         if isboolean and len(u_vals_nonna) <= 2:\n",
    "#             output_df[col] = output_df[col].astype('boolean')\n",
    "#         # print(col, u_vals_nonna)\n",
    "#     return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "689c32ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns of interest\n",
    "data_water = united_water.drop(columns=['raw_rain', 'chk_note_rain', 'chk_fail_rain', 'chk_note_ro', 'chk_fail_ro', 'comment_ro', 'source_ro'])\n",
    "\n",
    "# Cleanup\n",
    "del united_water\n",
    "\n",
    "# Remove duplicate entries\n",
    "data_water = data_water.reset_index().drop_duplicates(keep='first').set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "357e91e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3581782 entries, 1989-06-21 13:00:00 to 2025-08-01 13:00:00\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Dtype  \n",
      "---  ------          -----  \n",
      " 0   ra_rain         float32\n",
      " 1   raw_ro          float32\n",
      " 2   obstruction_ro  boolean\n",
      "dtypes: boolean(1), float32(2)\n",
      "memory usage: 61.5 MB\n"
     ]
    }
   ],
   "source": [
    "water_drops = ['level_ro', 'obstruction_ro', 'gap_fill_ro', 'weir_cleaning_ro', 'spike_ro', 'calibration_ro']\n",
    "water_drops.remove(var_of_interest)\n",
    "\n",
    "data_water = data_water.drop(water_drops, axis=1)\n",
    "\n",
    "del water_drops\n",
    "\n",
    "data_water.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1706a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "united_soil['sample'] = united_soil['sample'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "288330f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_water = mod_float(data_water)\n",
    "# united_soil = mod_float(united_soil)\n",
    "\n",
    "# data_water = mod_obj(data_water)\n",
    "\n",
    "# data_cal['weir_level_cal'] = data_cal['weir_level_cal'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168e27bd",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "712ada84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_water['1999-12-31 23:00:00':'2000-01-13 00:00:00']\n",
    "# Missing, inclusive:\n",
    "# 1999-12-31 23:25:00\n",
    "# 2000-01-11 14:05:00\n",
    "# united_water[united_water['level_ro'].isna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa5776e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Note ###\n",
    "# REMOVE this later -- just a smaller subset for feature engineering testing!!!\n",
    "# temp_subset_start = '2000-01-01 00:00:00'\n",
    "temp_subset_start = '2001-02-01 00:00:00'\n",
    "temp_subset_end = '2011-12-31 23:59:59'\n",
    "# data_water = data_water['2015-01-01 00:00:00':'2016-12-31 23:59:59']\n",
    "data_water = data_water[temp_subset_start:temp_subset_end]\n",
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30d5a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del united_water\n",
    "# del mod_float, mod_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb9dc61",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa843b0",
   "metadata": {},
   "source": [
    "### Distance from Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8544fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def since_feat(input_df, input_col):\n",
    "#     output_df = input_df#.copy()\n",
    "#     # Create index of instances where there is a data point\n",
    "#     instances = output_df[input_col].notna()\n",
    "#     # Create groupings based on most recent instance\n",
    "#     group_id = instances.cumsum()\n",
    "#     # Exclude the first grouping\n",
    "#     # otherwise it assumes there was an event just prior to the first entry\n",
    "#     group_id = group_id.replace(0, np.nan)\n",
    "#     # Create new column to count number of records since the point\n",
    "#     # which resets to 0 at each new point\n",
    "#     output_df[f\"since_{input_col}\"] = output_df.groupby(group_id).cumcount()\n",
    "#     return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ddd3302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def minsince_feat(input_df, input_col):\n",
    "#     output_df = input_df#.copy()#[input_col].to_frame()\n",
    "#     instances = output_df[input_col].notna()\n",
    "#     # Create groupings based on most recent instance\n",
    "#     group_id = instances.cumsum()\n",
    "#     # Exclude the first grouping\n",
    "#     # otherwise it assumes there was an event just prior to the first entry\n",
    "#     group_id = group_id.replace(0, np.nan)\n",
    "#     # Create new column to count the distance in minutes since the point\n",
    "#     # which resets to 0 at each new point\n",
    "#     output_df['timestamp'] = pd.to_datetime(output_df.index)\n",
    "#     # Get start timestamp of the group\n",
    "#     output_df['ts_start'] = output_df.groupby(group_id)['timestamp'].transform('min')\n",
    "#     # Calculate the distance\n",
    "#     output_df[f\"minsince_{input_col}\"] = (output_df['timestamp'] - output_df['ts_start']).dt.total_seconds()/60\n",
    "#     # Remove extra cols\n",
    "#     output_df = output_df.drop(columns=['timestamp', 'ts_start'])\n",
    "#     return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ab0b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def daysince_feat(input_df, input_col):\n",
    "#     output_df = input_df#.copy()#[input_col].to_frame()\n",
    "#     instances = output_df[input_col].notna()\n",
    "#     # Create groupings based on most recent instance\n",
    "#     group_id = instances.cumsum()\n",
    "#     # Exclude the first grouping\n",
    "#     # otherwise it assumes there was an event just prior to the first entry\n",
    "#     group_id = group_id.replace(0, np.nan)\n",
    "#     # Create new column to count the distance in days since the point\n",
    "#     # which resets to 0 at each new point\n",
    "#     output_df['timestamp'] = pd.to_datetime(output_df.index)\n",
    "#     # Get start timestamp of the group\n",
    "#     output_df['ts_start'] = output_df.groupby(group_id)['timestamp'].transform('min')\n",
    "#     # Calculate the distance\n",
    "#     # output_df[f\"daysince_{input_col}\"] = (output_df['timestamp'] - output_df['ts_start']).dt.total_seconds()/(24 * 60 * 60)\n",
    "#     output_df[f\"daysince_{input_col}\"] = (output_df['timestamp'] - output_df['ts_start']).dt.days\n",
    "#     # Remove extra cols\n",
    "#     output_df = output_df.drop(columns=['timestamp', 'ts_start'])\n",
    "#     return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24851c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timesince_feat(input_df, input_col, input_unit):\n",
    "    # output_df = input_df.copy()#[input_col].to_frame()\n",
    "    output_df = input_df\n",
    "    instances = output_df[input_col].notna()\n",
    "    # Create groupings based on most recent instance\n",
    "    group_id = instances.cumsum()\n",
    "    # Exclude the first grouping\n",
    "    # otherwise it assumes there was an event just prior to the first entry\n",
    "    group_id = group_id.replace(0, np.nan)\n",
    "    # Create new column to count the distance in days since the point\n",
    "    # which resets to 0 at each new point\n",
    "    output_df['timestamp'] = pd.to_datetime(output_df.index)\n",
    "    # Get start timestamp of the group\n",
    "    output_df['ts_start'] = output_df.groupby(group_id)['timestamp'].transform('min')\n",
    "    # Calculate the distance\n",
    "    if input_unit == \"minutes\":\n",
    "        output_df[f\"minsince_{input_col}\"] = (output_df['timestamp'] - output_df['ts_start']).dt.total_seconds().div(60).astype('Int32')\n",
    "        # output_df[f\"minsince_{input_col}\"] = output_df[f\"minsince_{input_col}\"].astype(np.float32)\n",
    "    elif input_unit == \"days\":\n",
    "        output_df[f\"daysince_{input_col}\"] = (output_df['timestamp'] - output_df['ts_start']).dt.days.astype('Int32')\n",
    "        # output_df[f\"minsince_{input_col}\"] = output_df[f\"minsince_{input_col}\"].astype(np.float32)\n",
    "        # output_df[f\"daysince_{input_col}\"] = output_df[f\"daysince_{input_col}\"].astype('Int32')\n",
    "    # Remove extra cols\n",
    "    output_df = output_df.drop(columns=['timestamp', 'ts_start'])\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "726384de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# daysince_feat(data_mini, '1_shallow')[['1_shallow', 'level_ro', 'daysince_1_shallow']]\n",
    "# timesince_feat(data_mini, '1_shallow', \"days\")[['1_shallow', 'level_ro', 'daysince_1_shallow']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3630890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeuntil_feat(input_df, input_col, input_unit):\n",
    "    output_df = input_df#.copy()\n",
    "    output_df['timestamp'] = output_df.index\n",
    "\n",
    "    # find where the point occurred (not null)\n",
    "    instances = output_df[input_col].notna()\n",
    "\n",
    "    # Create groupings based on the next instance\n",
    "    # bfill on cumsum to associate ea row with the group ending at the next event\n",
    "    group_id = instances[::-1].cumsum()[::-1].replace(0, np.nan)\n",
    "\n",
    "    # Get the end timestamp of the group\n",
    "    output_df['ts_end'] = output_df.groupby(group_id)['timestamp'].transform('max')\n",
    "\n",
    "    # Calculate the distance\n",
    "    if input_unit == \"minutes\":\n",
    "        output_df[f\"minuntil_{input_col}\"] = (output_df['ts_end'] - output_df['timestamp']).dt.total_seconds().div(60).astype('Int32')\n",
    "        # output_df[f\"minuntil_{input_col}\"] = output_df[f\"minuntil_{input_col}\"].astype('Int32')\n",
    "    elif input_unit == \"days\":\n",
    "        output_df[f\"dayuntil_{input_col}\"] = (output_df['ts_end'] - output_df['timestamp']).dt.days.astype('Int32')\n",
    "        # output_df[f\"dayuntil_{input_col}\"] = output_df[f\"dayuntil_{input_col}\"].astype('Int32')\n",
    "\n",
    "    # Remove extra cols\n",
    "    output_df = output_df.drop(columns=['timestamp', 'ts_end'])\n",
    "\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "399dac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_water_mini = data_water.copy()['2000-01-01 00:00:00':'2000-06-01 00:00:00'][['ra_rain', 'level_ro']]\n",
    "\n",
    "# data_water_mini.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c83c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeuntil_feat(data_water_mini, 'ra_rain', 'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0a73b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # since_feat(data_water[['ra_rain', 'raw_ro']], 'ra_rain')\n",
    "# # data_water[['ra_rain', 'raw_ro']]\n",
    "# data_w_test = data_water.copy()[['ra_rain', 'raw_ro']]\n",
    "# data_w_test['ra_rain'] = data_w_test['ra_rain'].replace(0, np.nan)\n",
    "\n",
    "# data_w_test = since_feat(data_w_test, 'ra_rain')\n",
    "# data_w_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af4d6dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create index of instances where there is a calibration point\n",
    "# cal_instances = data_water['weir_level_cal'].notna()\n",
    "# # Create groupings based on most recent instance\n",
    "# cal_group_id = cal_instances.cumsum()\n",
    "# # Create new column to count number of records since the calibration point\n",
    "# # which resets to 0 at each new calibration\n",
    "# data_water['records_since_cal'] = data_water.groupby(cal_group_id).cumcount()\n",
    "\n",
    "# # Clean up environment\n",
    "# del cal_instances, cal_group_id\n",
    "\n",
    "# # data_water\n",
    "\n",
    "# data_water = since_feat(data_water, 'weir_level_cal')\n",
    "\n",
    "## HERE\n",
    "# data_water = minsince_feat(data_water, 'weir_level_cal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830a5cbc",
   "metadata": {},
   "source": [
    "#### Rain\n",
    "Create feature which tracks how recent a rain event occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0a3b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_water = timesince_feat(data_water, 'ra_rain', \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b21cc478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create index of instances where there is a calibration point\n",
    "# rain_instances = data_water['ra_rain'].notna()\n",
    "# # Create groupings based on most recent instance\n",
    "# rain_group_id = rain_instances.cumsum()\n",
    "# # Create new column to count number of records since the calibration point\n",
    "# # which resets to 0 at each new calibration\n",
    "# data_water['records_since_rain'] = data_water.groupby(rain_group_id).cumcount()\n",
    "\n",
    "# # Clean up environment\n",
    "# del rain_instances, rain_group_id\n",
    "\n",
    "# # Replace NAs with 0\n",
    "# data_water['ra_rain'] = data_water['ra_rain'].fillna(0)\n",
    "\n",
    "# data_water.sample(10)\n",
    "# # data_water.dropna(subset='raw_ro')\n",
    "\n",
    "# data_water = since_feat(data_water, 'ra_rain')\n",
    "# data_water = minsince_feat(data_water, 'ra_rain')\n",
    "# data_water.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02690142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_water['ra_rain'] = data_water['ra_rain'].fillna(0)\n",
    "# data_water.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e786a7e9",
   "metadata": {},
   "source": [
    "### Rain event\n",
    "\n",
    "Keep track of cumulative rainfall during a specific event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59b7c231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index of instances where there is a data point\n",
    "# rain_event = data_water['ra_rain'].isnull()\n",
    "# rain_event = (data_water['ra_rain'].isnull() & ((data_water['minsince_ra_rain'] >= 5.0) & (data_water['minsince_ra_rain'] != 0)))\n",
    "rain_event = (data_water['ra_rain'].isnull() & ((data_water['minsince_ra_rain'] >= 5.0) & (data_water['minsince_ra_rain'] != 0)))\n",
    "# Create groupings based on most recent instance\n",
    "rain_event_id = rain_event.cumsum()\n",
    "# Create new column to count number of records since the point\n",
    "# which resets to 0 at each new point\n",
    "# del group_id, instances\n",
    "# water_mini\n",
    "# group_id = group_id.replace(0, np.nan)\n",
    "# water_mini['since_ra_rain2'] = water_mini.groupby(group_id).cumcount()\n",
    "# water_mini\n",
    "# water_mini.info()\n",
    "data_water['eventsum_ra_rain'] = data_water.groupby(rain_event_id)['ra_rain'].cumsum()\n",
    "\n",
    "del rain_event, rain_event_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3353b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_water[data_water['minsince_ra_rain'] > 0]\n",
    "# data_water\n",
    "# data_water[(data_water['minsince_weir_level_cal'] < 5) & (data_water['minsince_ra_rain'] != 0)]\n",
    "# data_water[(data_water['minsince_weir_level_cal'] < 5) & (data_water['minsince_ra_rain'] == 2.0)]\n",
    "# data_water[(data_water['minsince_weir_level_cal'] < 6) & (data_water['eventsum_ra_rain'].notnull())][['ra_rain', 'eventsum_ra_rain','minsince_ra_rain', 'minsince_weir_level_cal']]\n",
    "# data_water['2008-10-15 00:00:00':'2008-11-04 10:30:00']\n",
    "# data_water['2006-06-21 09:15:00':'2006-06-21 09:30:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0da2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_water[['ra_rain', 'since_ra_rain', 'rain_event_cumsum']]\n",
    "# data_water\n",
    "# 475"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "122cc839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# water_mini = data_water.copy()[['weir_level_cal', 'ra_rain', 'raw_ro', 'since_weir_level_cal', 'since_ra_rain']]\n",
    "# water_mini.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c9dd8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # water_mini = data_water.copy()[['ra_rain', 'level_ro']]\n",
    "# water_mini = data_water.copy()['ra_rain'].to_frame()\n",
    "# instances = water_mini['ra_rain'].notna()\n",
    "# # Create groupings based on most recent instance\n",
    "# group_id = instances.cumsum()\n",
    "# # Exclude the first grouping\n",
    "# # otherwise it assumes there was an event just prior to the first entry\n",
    "# group_id = group_id.replace(0, np.nan)\n",
    "# # Create new column to count number of records since the point\n",
    "# # which resets to 0 at each new point\n",
    "# # output_df[f\"since_{input_col}\"] = output_df.groupby(group_id).cumcount()\n",
    "# # group_id\n",
    "\n",
    "# water_mini['timestamp'] = pd.to_datetime(water_mini.index)\n",
    "# water_mini['ts_start'] = water_mini.groupby(group_id)['timestamp'].transform('min')\n",
    "# water_mini['ts_dist'] = (water_mini['timestamp'] - water_mini['ts_start']).dt.total_seconds()/60\n",
    "# water_mini\n",
    "\n",
    "# # water_mini = water_mini.reset_index()\n",
    "# # water_mini['ts_start'] = water_mini.groupby(group_id)['datetime'].transform('min')\n",
    "# # water_mini['ts_dist'] = (water_mini['datetime'] - water_mini['ts_start']).dt.total_seconds() / 60\n",
    "# # water_mini.set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be3ee645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create index of instances where there is a data point\n",
    "# instances = water_mini['ra_rain'].notna()\n",
    "# # Create groupings based on most recent instance\n",
    "# group_id = instances.cumsum()\n",
    "# # Create new column to count number of records since the point\n",
    "# # which resets to 0 at each new point\n",
    "# # del group_id, instances\n",
    "# # water_mini\n",
    "# # group_id = group_id.replace(0, np.nan)\n",
    "# # water_mini['since_ra_rain2'] = water_mini.groupby(group_id).cumcount()\n",
    "# # water_mini\n",
    "# # water_mini.info()\n",
    "# water_mini['rain_event'] = water_mini.groupby(group_id)['ra_rain'].cumsum()\n",
    "\n",
    "# # rain_null_mask = water_mini['ra_rain'].isnull()\n",
    "# # rain_group_id = rain_null_mask.cumsum()\n",
    "# # water_mini.groupby(rain_group_id)['ra_rain'].cumsum()\n",
    "# # # rain_null_mask\n",
    "# # # water_mini['rain_event_cumsum'] = water_mini.groupby(rain_group_id)['ra_rain'].cumsum()\n",
    "\n",
    "# # # g_id_event = null_mask.cumsum()\n",
    "# # # water_m['r_event_sum'] = water_m.groupby(g_id_event)['ra_rain'].cumsum()\n",
    "\n",
    "# # # del rain_null_mask, rain_group_id\n",
    "# # # data_water['rain_event_sum'] = data_water.groupby(g_id_event)['ra_rain'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9bde8f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # israin = water_mini['ra_rain'].notna()\n",
    "# # israin_group_id = israin.cumsum()\n",
    "# # # g_id\n",
    "# # water_mini['since_rain_2'] = water_mini.groupby(israin_group_id).cumcount()\n",
    "# water_mini['dec'] = np.exp(-0.1*water_mini['since_ra_rain'])\n",
    "# water_mini['rain_fill'] = water_mini['rain_event_cumsum'].ffill()\n",
    "# # data_u['1_shallow_f'] = data_u['1_shallow'].ffill()\n",
    "# water_mini['rain_dec'] = (water_mini['rain_fill']*water_mini['dec'])\n",
    "# # del israin, israin_group_id\n",
    "# water_mini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a81ae4",
   "metadata": {},
   "source": [
    "### Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46f01638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_feat(input_df, input_col, input_dec_rate = -0.1):\n",
    "    output_df = input_df#.copy()\n",
    "    # output_df = since_feat(input_df = output_df, input_col = input_col)\n",
    "    if f\"minsince_{input_col}\" not in output_df.columns:\n",
    "        # output_df = minsince_feat(input_df = output_df, input_col = input_col)\n",
    "        output_df = timesince_feat(input_df = output_df, input_col = input_col, input_unit = \"minutes\")\n",
    "    \n",
    "    output_df[f\"decayrate{input_dec_rate}_{input_col}\"] = np.exp(input_dec_rate * output_df[f\"minsince_{input_col}\"]).astype(np.float32)\n",
    "    output_df[f\"ffill_{input_col}\"] = output_df[input_col].ffill()\n",
    "    output_df[f\"decay{input_dec_rate}_{input_col}\"] = (output_df[f\"ffill_{input_col}\"] * output_df[f\"decayrate{input_dec_rate}_{input_col}\"])\n",
    "\n",
    "    return output_df\n",
    "\n",
    "# water_m = united_water[['raw_ro', 'level_ro', 'ra_rain', 'obstruction_ro']]\n",
    "\n",
    "# null_mask = water_m['ra_rain'].isnull()\n",
    "# g_id_event = null_mask.cumsum()\n",
    "# water_m['r_event_sum'] = water_m.groupby(g_id_event)['ra_rain'].cumsum()\n",
    "\n",
    "# is_rain = water_m['ra_rain'].notna()\n",
    "# g_id = is_rain.cumsum()\n",
    "# # g_id\n",
    "# water_m['since_rain'] = water_m.groupby(g_id).cumcount()\n",
    "# water_m['dec'] = np.exp(-0.1*water_m['since_rain'])\n",
    "# water_m['rain_fill'] = water_m['r_event_sum'].ffill()\n",
    "# # data_u['1_shallow_f'] = data_u['1_shallow'].ffill()\n",
    "# water_m['rain_dec'] = (water_m['rain_fill']*water_m['dec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96198948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# water_mini = data_water.copy()\n",
    "# water_mini['ra_rain'] = water_mini['ra_rain'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43d22bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# water_mini = data_water.copy()\n",
    "# water_mini['ra_rain'] = water_mini['ra_rain'].fillna(0)\n",
    "# water_mini = decay_feat(water_mini, 'eventsum_ra_rain')\n",
    "# selected_columns = water_mini.columns[water_mini.columns.str.contains('ra_rain')]\n",
    "# water_mini[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "947bae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NAs in rain with 0\n",
    "data_water['ra_rain'] = data_water['ra_rain'].fillna(0)\n",
    "\n",
    "# Apply decay function\n",
    "data_water = decay_feat(data_water, 'eventsum_ra_rain')\n",
    "\n",
    "# Drop extra column\n",
    "# minutes since rain event will be the same as minutes since most recent rain\n",
    "data_water = data_water.drop('minsince_eventsum_ra_rain', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6154e3b7",
   "metadata": {},
   "source": [
    "### Lag features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5d3c7b",
   "metadata": {},
   "source": [
    "#### Consistent cols\n",
    "\n",
    "Modify the rows to prevent inappropriate data shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ad565a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data_mini_min = data_water.copy()['1996-11-01 00:00:00':'1997-01-31 23:59:59']\n",
    "# # data_mini_min = data_water.copy()['1993-01-01 00:00:00':'1997-12-25 00:00:00']\n",
    "# # data_water.head()\n",
    "# data_mini_min = data_water.copy()\n",
    "\n",
    "# print(len(data_mini_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c7eef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_mini_min.sort_index()['2000-04-14 02:35:00':'2000-04-14 03:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6dfd043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # full_range = pd.date_range(start = '1996-11-01 00:00:00', end = '1997-02-01 00:00:00', freq = '5min')\n",
    "# # full_range.duplicated().any()\n",
    "# # data_mini_min.reindex(full_range)\n",
    "# # data_mini_min.drop_duplicates(keep='first').asfreq(freq='5min')['2000-04-14 02:35:00':'2000-04-14 03:00:00']\n",
    "\n",
    "\n",
    "# # Assuming your df has a unique DatetimeIndex already\n",
    "# # Ensure the index is sorted first (good practice for time series ops)\n",
    "# # df = df.sort_index()\n",
    "\n",
    "# # 1. Define the complete range (e.g., every minute)\n",
    "# new_index = pd.date_range(start=data_mini_min.index.min(), \n",
    "#                           end=data_mini_min.index.max(), \n",
    "#                           freq='5min')\n",
    "\n",
    "# # 2. Reindex to this full range\n",
    "# # data_mini_min.drop_duplicates(keep='first').reindex(new_index)['2000-04-14 02:35:00':'2000-04-14 03:00:00']\n",
    "# data_mini_min.reset_index().drop_duplicates(keep='first').set_index('datetime').reindex(new_index)['2000-04-14 02:35:00':'2000-04-14 03:00:00']\n",
    "\n",
    "\n",
    "# # data_mini_min = data_mini_min.reset_index()\n",
    "\n",
    "\n",
    "# # 3. Use ffill/bfill to fill the new NaNs\n",
    "# # df_complete = df_complete.ffill() \n",
    "\n",
    "\n",
    "# # print(f\"Index type: {type(data_mini_min.index)}\")\n",
    "# # print(f\"Index dtype: {data_mini_min.index.dtype}\")\n",
    "# # print(f\"Index has duplicates: {data_mini_min.index.duplicated().any()}\")\n",
    "# # print(f\"Number of rows: {len(data_mini_min)}\")\n",
    "# # print(f\"Number of unique index values: {len(data_mini_min.index.unique())}\")\n",
    "\n",
    "# # # data_mini_min.index.duplicated().any()\n",
    "\n",
    "# # duplicated_index_mask = data_mini_min.index.duplicated(keep=False)\n",
    "\n",
    "# # # Filter the DataFrame using the boolean mask\n",
    "# # data_mini_min[duplicated_index_mask]\n",
    "\n",
    "# # # type(data_mini_min.index)\n",
    "# # # full_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a90a89ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_water['ra_rain'] = data_water['ra_rain'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a0e75eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_indices = data_water.index.copy()\n",
    "\n",
    "new_index = pd.date_range(start = data_water.index.min(), \n",
    "                          end = data_water.index.max(), \n",
    "                          freq = '5min')\n",
    "\n",
    "# Reindex\n",
    "data_water = data_water.reindex(new_index)\n",
    "\n",
    "# Cleanup\n",
    "del new_index\n",
    "\n",
    "# # Return\n",
    "# data_water = data_water.loc[original_indices]\n",
    "# del original_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "87c9f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_index = pd.date_range(start = data_water.index.min(), \n",
    "#                           end = data_water.index.max(), \n",
    "#                           freq = '5min')\n",
    "\n",
    "# # 2. Reindex to this full range\n",
    "# # data_mini_min.drop_duplicates(keep='first').reindex(new_index)['2000-04-14 02:35:00':'2000-04-14 03:00:00']\n",
    "# data_water = data_water.reindex(new_index)\n",
    "\n",
    "# del new_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8945f",
   "metadata": {},
   "source": [
    "Get values from other recent time stamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "09c4b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_feats(input_df, input_cols, input_lags):\n",
    "    output_df = input_df#.copy()\n",
    "    for col in input_cols:\n",
    "        for lag in input_lags:\n",
    "            output_df[f\"{col}_lag{lag}\"] = output_df[col].shift(lag)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "797920ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag_feats(data_water, ['raw_ro'], [1, 2, 3, 24]).dropna(subset='raw_ro')[['raw_ro', 'raw_ro_lag1', 'raw_ro_lag2']]\n",
    "# lag_feats(data_water, ['raw_ro'], [1, 2, 3, 24]).dropna(subset='raw_ro')[['raw_ro', 'raw_ro_lag1', 'raw_ro_lag24']]\n",
    "\n",
    "# Columns to get temporal stats on\n",
    "cols_to_shift = ['raw_ro', 'ra_rain']\n",
    "# # data at 5-min increments -- lag to record values at 5m, 10m, 15m, 30m, 1h, and 2h prior\n",
    "# lags_of_interest = [1, 2, 3, 6, 12, 24]\n",
    "# data at 5-min increments -- lag to record values at 5m, 10m, 15m, 20m, 25m, 30m, 1h, 2h, 3h prior\n",
    "lags_of_interest = [1, 2, 3, 4, 5, 6, 12, 24, 36]\n",
    "\n",
    "data_water = lag_feats(data_water, cols_to_shift, lags_of_interest)\n",
    "\n",
    "# data_water.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc9519",
   "metadata": {},
   "source": [
    "#### Risky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe140165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## risky?\n",
    "# var_of_interest = 'obstruction_ro'\n",
    "# data_water = lag_feats(data_water, [var_of_interest], [1])\n",
    "# # data_water = lag_feats(data_water, [var_of_interest], lags_of_interest)\n",
    "# ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddce5f3",
   "metadata": {},
   "source": [
    "### Rolling stats\n",
    "\n",
    "Get stat values from range of recent time stamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2cbbea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rolling_feats(input_df, input_cols, input_windows):\n",
    "#     output_df = input_df.copy()\n",
    "#     for col in input_cols:\n",
    "#         for window in input_windows:\n",
    "#             output_df[f\"{col}_rollmean_{window}\"] = output_df[col].rolling(window).mean()\n",
    "#             output_df[f\"{col}_rollstd_{window}\"] = output_df[col].rolling(window).std()\n",
    "#             output_df[f\"{col}_rollslope_{window}\"] = (output_df[col].rolling(window).apply(lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=True))\n",
    "#     return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8931d09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data_water_mini = data_water['1990-01-01 00:00:00':'1990-01-30 23:59:59']\n",
    "# # rolling_feats(data_water_mini, cols_to_lag, [6, 12, 36])\n",
    "# # 10m, 30m, 1h, 6h\n",
    "# windows_of_interest = [2, 6, 12, 72]\n",
    "\n",
    "\n",
    "# # windows_of_interest = [2, 6, 12]\n",
    "# # cols_to_shift = ['raw_ro', 'ra_rain']\n",
    "# # data_water_slow = rolling_feats(data_water['2000-01-01 00:00:00':'2000-12-31 23:59:59'], cols_to_shift, windows_of_interest)\n",
    "\n",
    "# # data_water_slow.sample(10)\n",
    "# data_water = rolling_feats(data_water, cols_to_shift, windows_of_interest)\n",
    "\n",
    "# # data_water.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb010731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_feats(input_df, input_cols, input_windows, input_mtype = \"mean\"):\n",
    "    output_df = input_df#.copy()\n",
    "    \n",
    "    # Create a dummy series of index values (0, 1, 2, ... N) once\n",
    "    # 'x' represents the position within the dataframe for the regression calculation\n",
    "    x_series = pd.Series(np.arange(len(output_df)), index=output_df.index)\n",
    "    \n",
    "    for col in input_cols:\n",
    "        for window in input_windows:\n",
    "            # 1. Calculate Mean and Std Dev (already fast)\n",
    "            if input_mtype == \"mean\":\n",
    "                output_df[f\"{col}_rollmean_{window}\"] = output_df[col].rolling(window).mean().astype(np.float32)\n",
    "            elif input_mtype == \"sum\":\n",
    "                output_df[f\"{col}_rollsum_{window}\"] = output_df[col].rolling(window).sum().astype(np.float32)\n",
    "            elif input_mtype == \"both\":\n",
    "                output_df[f\"{col}_rollmean_{window}\"] = output_df[col].rolling(window).mean().astype(np.float32)\n",
    "                output_df[f\"{col}_rollsum_{window}\"] = output_df[col].rolling(window).sum().astype(np.float32)\n",
    "            output_df[f\"{col}_rollstd_{window}\"] = output_df[col].rolling(window).std().astype(np.float32)\n",
    "\n",
    "            # 2. Calculate Slope using vectorized operations (Fast)\n",
    "            # Slope = Cov(Y, X) / Var(X)\n",
    "            \n",
    "            # Calculate Covariance of Y (your data) vs X (the index position)\n",
    "            rolling_cov = output_df[col].rolling(window).cov(x_series)\n",
    "            \n",
    "            # Calculate Variance of X (index position)\n",
    "            rolling_var_x = x_series.rolling(window).var()\n",
    "            \n",
    "            # The slope is Cov(X, Y) / Var(X)\n",
    "            output_df[f\"{col}_rollslope_{window}\"] = (rolling_cov / rolling_var_x).astype(np.float32)\n",
    "            \n",
    "            # Note on edge cases: \n",
    "            # The initial 'window-1' values for rolling_var_x will be NaN/incorrect. \n",
    "            # Pandas automatically handles alignment, so the division result will also be NaN where appropriate.\n",
    "            # This method works very well for standard time series analysis.\n",
    "    # output_df = mod_float(output_df)\n",
    "    return output_df\n",
    "\n",
    "# cols_to_shift = ['raw_ro', 'ra_rain']\n",
    "# # # 10m, 30m, 1h, 6h\n",
    "# windows_of_interest = [2, 6, 12]\n",
    "# data_owater = optimized_rolling_feats_vectorized(data_water['2000-01-01 00:00:00':'2000-12-31 23:59:59'], cols_to_shift, windows_of_interest)\n",
    "# data_owater\n",
    "\n",
    "# optimized_rolling_feats_vectorized(data_water['2000-01-01 00:00:00':'2000-12-31 23:59:59'], cols_to_shift, windows_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cf7eaaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_to_shift = ['raw_ro', 'ra_rain']\n",
    "# Inclusive of current point--\n",
    "# 10m, 15m, 20m, 25m, 30m, 1h, 3h, 6h, 12h, 24h\n",
    "windows_of_interest = [2, 3, 4, 5, 6, 12, 36, 72, 144, 288]\n",
    "# data_water = rolling_feats(data_water, cols_to_shift, windows_of_interest)\n",
    "\n",
    "# data_water = rolling_feats(data_water, ['raw_ro'], windows_of_interest, \"mean\")\n",
    "data_water = rolling_feats(data_water, ['raw_ro'], windows_of_interest, \"both\")\n",
    "data_water = rolling_feats(data_water, ['ra_rain'], windows_of_interest, \"sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596fa05d",
   "metadata": {},
   "source": [
    "Change since last value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ccb6f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_water['raw_ro_change'] = data_water['raw_ro'].diff()\n",
    "\n",
    "# cal_na_mask = data_water['weir_level_cal'].notna() & data_water['raw_ro'].notna()\n",
    "# # cal_na_mask\n",
    "# (data_water['weir_level_cal'] - data_water['raw_ro']).dropna()\n",
    "# del cal_na_mask\n",
    "# data_water['diff_ro_cal'] = (data_water['weir_level_cal'] - data_water['raw_ro'])\n",
    "# data_water['rain_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b0d8524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return\n",
    "data_water = data_water.loc[original_indices]\n",
    "\n",
    "del original_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794b582",
   "metadata": {},
   "source": [
    "## Soil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8e3f87",
   "metadata": {},
   "source": [
    "Pivot the soil data such that each sample has its own columns, and separated by depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd40d185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant column\n",
    "data_soil_shallow = united_soil.copy().drop('h2o_by_wet_deep', axis=1)\n",
    "data_soil_shallow['sample'] = data_soil_shallow['sample'].astype('float32')\n",
    "# Pivot wider\n",
    "data_soil_shallow = data_soil_shallow.pivot(columns='sample', values='h2o_by_wet_shallow')\n",
    "\n",
    "# Drop irrelevant column\n",
    "data_soil_deep = united_soil.copy().drop('h2o_by_wet_shallow', axis=1)\n",
    "\n",
    "data_soil_deep['sample'] = data_soil_deep['sample'].astype('float32')\n",
    "# Pivot wider\n",
    "data_soil_deep = data_soil_deep.pivot(columns='sample', values='h2o_by_wet_deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "21b91e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_soil = pd.merge(\n",
    "    data_soil_shallow,\n",
    "    data_soil_deep,\n",
    "    left_index = True,\n",
    "    right_index = True,\n",
    "    suffixes = (\"_shallow\", \"_deep\"),\n",
    "    how = \"outer\"\n",
    ")\n",
    "\n",
    "del data_soil_shallow, data_soil_deep\n",
    "del united_soil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "17263955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soil_mini_shallow = united_soil.copy().drop('h2o_by_wet_deep', axis=1)\n",
    "# soil_mini_shallow = soil_mini_shallow.pivot(columns='sample', values='h2o_by_wet_shallow')\n",
    "\n",
    "# soil_mini_deep = united_soil.copy().drop('h2o_by_wet_shallow', axis=1)\n",
    "# soil_mini_deep = soil_mini_deep.pivot(columns='sample', values='h2o_by_wet_deep')\n",
    "\n",
    "# soil_mini = pd.merge(\n",
    "#     soil_mini_shallow,\n",
    "#     soil_mini_deep,\n",
    "#     left_index=True,\n",
    "#     right_index=True,\n",
    "#     # soil_mini_shallow.reset_index(),\n",
    "#     # soil_mini_deep.reset_index(),\n",
    "#     # on = [\"date\", \"sample\"],\n",
    "#     suffixes = (\"_shallow\", \"_deep\"),\n",
    "#     how = \"outer\"\n",
    "#     )\n",
    "\n",
    "# soil_mini.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47024efe",
   "metadata": {},
   "source": [
    "## Unite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "56358374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_u_test = pd.merge(\n",
    "#     data_water,\n",
    "#     data_soil['2015-01-01 00:00:00':'2016-12-31 23:59:59'],\n",
    "#     left_index = True,\n",
    "#     right_index = True,\n",
    "#     how = 'outer'\n",
    "# )\n",
    "\n",
    "# data_u_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d8de6109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def since_feat(input_df, input_col):\n",
    "# #     output_df = input_df.copy()\n",
    "# #     # Create index of instances where there is a data point\n",
    "# #     instances = output_df[input_col].notna()\n",
    "# #     # Create groupings based on most recent instance\n",
    "# #     group_id = instances.cumsum()\n",
    "# #     # Create new column to count number of records since the point\n",
    "# #     # which resets to 0 at each new point\n",
    "# #     output_df[f\"since_{input_col}\"] = output_df.groupby(group_id).cumcount()\n",
    "# #     return output_df\n",
    "\n",
    "# cols_soil = [col for col in data_u_test.columns if (col.endswith('shallow') | col.endswith('deep'))]\n",
    "# soil_instances = data_u_test[cols_soil].notna()\n",
    "# soil_group_id = soil_instances.cumsum().max(axis=1)\n",
    "# data_u_test[\"since_soil\"] = data_u_test.groupby(soil_group_id).cumcount()\n",
    "# # data_u_test.groupby(soil_group_id).cumcount()\n",
    "# # data_u_test[\"since_soil\"] = data_u_test.groupby(soil_group_id).cumcount()\n",
    "# # data_u_test[cols_soil].notna().cumsum().max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a80c556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_united = pd.merge(\n",
    "#     data_water,\n",
    "#     # REMOVE LATER\n",
    "#     # data_soil['2015-01-01 00:00:00':'2016-12-31 23:59:59'],\n",
    "#     data_soil['2000-01-01 00:00:00':'2015-12-31 23:59:59'],\n",
    "#     # data_soil,\n",
    "#     #\n",
    "#     left_index = True,\n",
    "#     right_index = True,\n",
    "#     how = 'outer'\n",
    "# )\n",
    "\n",
    "data_united = pd.merge(\n",
    "    data_water,\n",
    "    # REMOVE LATER\n",
    "    # data_soil['2015-01-01 00:00:00':'2016-12-31 23:59:59'],\n",
    "    # data_soil['2000-01-01 00:00:00':'2015-12-31 23:59:59'],\n",
    "    # data_cal,\n",
    "    # data_cal['2000-01-01 00:00:00':'2015-12-31 23:59:59'],\n",
    "    data_cal[temp_subset_start:temp_subset_end],\n",
    "    #\n",
    "    left_index = True,\n",
    "    right_index = True,\n",
    "    how = 'outer'\n",
    ")\n",
    "\n",
    "data_united = pd.merge(\n",
    "    data_united,\n",
    "    # REMOVE LATER\n",
    "    # data_soil['2015-01-01 00:00:00':'2016-12-31 23:59:59'],\n",
    "    # data_soil['2000-01-01 00:00:00':'2015-12-31 23:59:59'],\n",
    "    # data_soil,\n",
    "    # data_soil['2000-01-01 00:00:00':'2015-12-31 23:59:59'],\n",
    "    data_soil[temp_subset_start:temp_subset_end],\n",
    "    #\n",
    "    left_index = True,\n",
    "    right_index = True,\n",
    "    how = 'outer'\n",
    ")\n",
    "\n",
    "# data_united['diff_ro_cal'] = (data_united['weir_level_cal'] - data_united['raw_ro'])\n",
    "# data_united = minsince_feat(data_united, 'weir_level_cal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f814e925",
   "metadata": {},
   "source": [
    "### United features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "90f96840",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_united['diff_ro_cal'] = (data_united['weir_level_cal'] - data_united['raw_ro'])\n",
    "data_united['diff_ro_cal'] = data_united['diff_ro_cal'].astype(np.float32)\n",
    "data_united = timesince_feat(data_united, 'weir_level_cal', \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c93d9bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeuntil_feat(data_united_m, 'weir_level_cal', \"minutes\")[['ra_rain', 'weir_level_cal', 'minuntil_weir_level_cal']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1621fcb7",
   "metadata": {},
   "source": [
    "### Temporal features\n",
    "Modify temporal features to be based on sine and cosine transformations, which allows for the model to be based on the cyclical patterns of time rather than abrupt distances\n",
    "\n",
    "(e.g., the raw values Day 365 of the year is 'far' from Day 001, but in reality they are very near)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5624d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_u_mini = data_united['2001-01-01 00:00:00':'2002-01-01 00:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dfbcf31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_feat(input_df, input_unit):\n",
    "    output_df = input_df\n",
    "    if input_unit=='day':\n",
    "        cycle_length = 365.25\n",
    "        value = output_df.index.dayofyear\n",
    "    elif input_unit=='month':\n",
    "        cycle_length = 12\n",
    "        value = output_df.index.month\n",
    "    elif input_unit=='hour':\n",
    "        cycle_length = 24\n",
    "        value = output_df.index.hour\n",
    "    elif input_unit=='minute':\n",
    "        cycle_length = 60\n",
    "        value = output_df.index.minute\n",
    "    \n",
    "    output_df[f'{input_unit}_sin'] = np.sin(2 * np.pi * value / cycle_length).astype(np.float32)\n",
    "    output_df[f'{input_unit}_cos'] = np.cos(2 * np.pi * value / cycle_length).astype(np.float32)\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "29877802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporal_feat(data_united['2001-01-01 00:00:00':'2002-01-01 00:00:00'], 'day').info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "72593493",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_united = temporal_feat(data_united, 'minute')\n",
    "data_united = temporal_feat(data_united, 'hour')\n",
    "data_united = temporal_feat(data_united, 'day')\n",
    "data_united = temporal_feat(data_united, 'month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "489a17ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in data_united.columns:\n",
    "#     if (col.endswith('shallow') | col.endswith('deep')):\n",
    "#         data_united = minsince_feat(data_united, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9b4b5605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create feature to track soil value staleness\n",
    "# cols_soil = [col for col in data_united.columns if (col.endswith('shallow') | col.endswith('deep'))]\n",
    "# soil_instances = data_united[cols_soil].notna()\n",
    "# soil_group_id = soil_instances.cumsum().max(axis=1)\n",
    "# data_united[\"since_soil\"] = data_united.groupby(soil_group_id).cumcount()\n",
    "\n",
    "# del soil_instances, soil_group_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0a4e0dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features to track soil value staleness\n",
    "cols_soil = [col for col in data_united.columns if (col.endswith('shallow') | col.endswith('deep'))]\n",
    "\n",
    "for col in cols_soil:\n",
    "# for col in data_united.columns:\n",
    "    # if (col.endswith('shallow') | col.endswith('deep')):\n",
    "    # data_united = minsince_feat(data_united, col)\n",
    "    data_united = timesince_feat(data_united, col, \"days\")\n",
    "\n",
    "# Extend soil vals\n",
    "data_united[cols_soil] = data_united[cols_soil].ffill()\n",
    "\n",
    "# Cutoff\n",
    "# cols_soil_days = [col for col in data_united.columns if (col.startswith('daysince_') & (col.endswith('_shallow') | col.endswith('_deep')))]\n",
    "# data_united['daysince_soil'] = data_united[cols_soil_days].min(axis=1)\n",
    "\n",
    "\n",
    "del col, cols_soil\n",
    "# data_united.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4e45432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_united_m = data_united.copy()['2001-01-01 00:00:00':'2002-01-01 00:00:00']\n",
    "\n",
    "# cols_soil = [col for col in data_united.columns if (col.endswith('shallow') | col.endswith('deep'))]\n",
    "# shallow_cols = [col for col in data_united.columns if (col.endswith('_shallow') & col.startswith('daysince_'))]\n",
    "# [col for col in data_united.columns if ((col.endswith('_shallow') | col.endswith('_deep')) & col.startswith('daysince_'))]\n",
    "# print(shallow_cols)\n",
    "# data_united['2002-01-01 00:00:00':'2003-01-01 00:00:00'][shallow_cols].min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9e7ab317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_soil_days = [col for col in data_united_m.columns if (col.startswith('daysince_') & (col.endswith('_shallow') | col.endswith('_deep')))]\n",
    "# data_united_m['daysince_soil'] = data_united_m[cols_soil_days].min(axis=1)\n",
    "# data_united_m.drop(cols_soil_days, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9c4b46c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extend soil vals\n",
    "# # cols_to_fill = [col for col in data_united.columns if (col.endswith('shallow') | col.endswith('deep'))]\n",
    "# # data_united[cols_to_fill] = data_united[cols_to_fill].ffill()\n",
    "# data_united[cols_soil] = data_united[cols_soil].ffill()\n",
    "\n",
    "# del cols_soil\n",
    "# data_united.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d348e",
   "metadata": {},
   "source": [
    "last param?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ccbf1304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_u_mini = data_united[['raw_ro', 'weir_level_cal']]['2004-01-01 00:00:00':'2005-01-01 00:00:00'].copy()\n",
    "# data_u_mini = timeuntil_feat(data_u_mini, 'weir_level_cal', 'minutes')\n",
    "# data_u_mini = data_u_mini.dropna(subset='minuntil_weir_level_cal')\n",
    "\n",
    "# split_date = '2004-06-15 12:30:30' # Define your cutoff date\n",
    "\n",
    "# # Create independent copies using .copy()\n",
    "# train_df = data_u_mini.loc[:split_date].copy()\n",
    "# test_df = data_u_mini.loc[split_date:].copy()\n",
    "\n",
    "# # The test_df index minimum confirms the exact moment of the split\n",
    "# test_start_time = test_df.index.min()\n",
    "\n",
    "# del data_u_mini, train_df, test_df, test_start_time, split_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "54f39d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df[test_df['minuntil_weir_level_cal'] == 0].index.min()\n",
    "\n",
    "# if train_df.iloc[-1]['minuntil_weir_level_cal'] != 0:\n",
    "\n",
    "    # print(train_df.iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd20a0d9",
   "metadata": {},
   "source": [
    "## Train/Test split\n",
    "\n",
    "80/20 initial split, with expanding sliding window for training/validation for hyperparameters, model stability, and feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1741c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Var to predict\n",
    "# var_of_interest = 'obstruction_ro'\n",
    "\n",
    "# # For if doing forward-interpretations\n",
    "# # i_to_drop = data_united.loc[data_united[var_of_interest].isnull()].index.tolist()\n",
    "\n",
    "# # Columns to be removed for training\n",
    "# # Corrected RO value assumed not available, since raw RO is what is the largest indicator\n",
    "# y_drops = ['level_ro', 'obstruction_ro', 'gap_fill_ro', 'weir_cleaning_ro', 'spike_ro', 'calibration_ro']\n",
    "\n",
    "# # Remove if doing forward-features\n",
    "# X_all = data_united.dropna(subset=[var_of_interest]).drop(y_drops, axis=1).copy()\n",
    "# y_all = data_united.dropna(subset=[var_of_interest])[var_of_interest].copy()\n",
    "\n",
    "# # For if doing forward-interpretations\n",
    "# i_to_drop = data_united.loc[data_united[var_of_interest].isnull()].index.tolist()\n",
    "# # X_all = data_united.drop(y_drops, axis=1).copy()\n",
    "# # y_all = data_united[var_of_interest].copy()\n",
    "\n",
    "# REMOVE NAs\n",
    "data_united = data_united.dropna(subset=[var_of_interest])\n",
    "\n",
    "X_all = data_united.drop(var_of_interest, axis=1).copy()\n",
    "y_all = data_united[var_of_interest].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "df5ab1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1134443\n",
      "1134443\n",
      "Train:\t80p of 1134443 is 907554\n",
      "Test:\t20p of 1134443 is 226889\n"
     ]
    }
   ],
   "source": [
    "y_len = len(y_all)\n",
    "\n",
    "print(\n",
    "    y_len, \"\\n\",\n",
    "    (round(.2*y_len) + round(.8*y_len)),\n",
    "    \"\\nTrain:\\t80p of \", y_len, \" is \", round(.8*y_len),\n",
    "    \"\\nTest:\\t20p of \", y_len, \" is \", round(.2*y_len),\n",
    "    sep=\"\"\n",
    ")\n",
    "\n",
    "del y_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daf36fd",
   "metadata": {},
   "source": [
    "Unlike the typical approach for train/test splits, temporal data in this context must _not_ be randomly split as it would lead to severe leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7e023cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\t 907554 \t 2001-02-01 00:00:00 thru 2009-10-26 18:05:00 \n",
      "Test:\t 226889 \t 2009-10-26 18:10:00 thru 2011-12-31 23:55:00\n"
     ]
    }
   ],
   "source": [
    "# Conduct the split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.2, shuffle=False)\n",
    "\n",
    "# Cleanup\n",
    "del X_all, y_all\n",
    "\n",
    "print(\n",
    "    \"Train:\\t\", len(X_train), \"\\t\", X_train.index[0], \"thru\", X_train.index[-1],\n",
    "    \"\\nTest:\\t\", len(X_test), \"\\t\", X_test.index[0], \"thru\", X_test.index[-1]\n",
    "    # len(x_train), len(x_test), \"\\n\",\n",
    "    # x_train.index[-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11921d04",
   "metadata": {},
   "source": [
    "### Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "661eeac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train: index=[     0      1      2 ... 226887 226888 226889]\n",
      "  Test:  index=[226890 226891 226892 ... 453775 453776 453777]\n",
      "------------------------------------------------------------\n",
      "Fold 1:\n",
      "  Train: index=[     0      1      2 ... 453775 453776 453777]\n",
      "  Test:  index=[453778 453779 453780 ... 680663 680664 680665]\n",
      "------------------------------------------------------------\n",
      "Fold 2:\n",
      "  Train: index=[     0      1      2 ... 680663 680664 680665]\n",
      "  Test:  index=[680666 680667 680668 ... 907551 907552 907553]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize the split function\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "# print(tscv)\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(tscv.split(X_train)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={val_index}\")\n",
    "    # print(\"  Train: index=\", mini_x.index[train_index])\n",
    "    # print(f\"  Test:  index={val_index}\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "\n",
    "del i, train_index, val_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c343fa3",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "61270f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'n_estimators': randint(50, 500), # early stopping control the actual number\n",
    "    'learning_rate': uniform(0.01, 0.29),\n",
    "    'max_depth': randint(3, 7),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    early_stopping_rounds=50,\n",
    "    objective='binary:logistic',\n",
    "    tree_method='hist',\n",
    "    n_jobs=-1,\n",
    "    eval_metric='logloss',\n",
    "    scale_pos_weight = (np.sum(y_train == 0) / np.sum(y_train == 1))\n",
    "    # scale_pos_weight = (y_sub_train.value_counts()[False] / y_sub_train.value_counts()[True]).item()\n",
    ")\n",
    "\n",
    "# Randomized search with efficient settings\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    # n_iter=20,                    # Start with a small number of iterations (e.g., 20-40)\n",
    "    n_iter=2,\n",
    "    scoring='roc_auc',\n",
    "    cv=tscv,                      # Use TimeSeriesSplit\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# When you run the fit, ensure you pass early stopping parameters:\n",
    "# X_train, y_train, X_val, y_val = ... define your data ...\n",
    "# eval_set = [(X_train, y_train), (X_val, y_val)]\n",
    "#\n",
    "# random_search.fit(\n",
    "#     X_train, y_train,\n",
    "#     early_stopping_rounds=50, # Stop if validation metric doesn't improve for 50 rounds\n",
    "#     eval_set=eval_set,\n",
    "#     verbose=False # Set to True if you want to watch the early stopping logs\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "14a97d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you run the fit, ensure you pass early stopping parameters:\n",
    "X_train_two, X_val, y_train_two, y_val = train_test_split(X_train, y_train, test_size = 0.2, shuffle=False)\n",
    "eval_set = [(X_train_two, y_train_two), (X_val, y_val)]\n",
    "\n",
    "# print(\"Starting hyperparameter tuning...\")\n",
    "# random_search.fit(X_sub_train, y_sub_train)\n",
    "# X_train_two = X_train_two.drop(i_to_drop, errors='ignore')\n",
    "# y_train_two = y_train_two.drop(i_to_drop, errors='ignore')\n",
    "# X_val = X_val.drop(i_to_drop, errors='ignore')\n",
    "# y_val = y_val.drop(i_to_drop, errors = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a80eaf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check main training labels\n",
    "if y_train.isnull().any() or np.isinf(y_train).any():\n",
    "    print(\"ERROR: y_train contains NaN or Inf values!\")\n",
    "\n",
    "# Check validation labels (if you are using an eval_set)\n",
    "# Assuming y_val is the label portion of your validation set\n",
    "if y_val.isnull().any() or np.isinf(y_val).any():\n",
    "    print(\"ERROR: y_val contains NaN or Inf values!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "75109175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     if data_weir.empty == False:\n",
    "#         print(\"Data loaded, random sample shown below\")\n",
    "#         print(data_weir.sample(n=5))\n",
    "# except NameError:\n",
    "#     print(\"Data has not yet been read in, loading now...\")\n",
    "#     data_weir = pd.read_csv(\n",
    "#         \"data/bci_lutzweir_combined.csv\",\n",
    "#         usecols = ['datetime', 'level', 'raw', 'chk_note', 'chk_fail', 'comment', 'source'],\n",
    "#         parse_dates=['datetime'],\n",
    "#         dtype = {'source':'category', 'chk_note':'category', 'chk_fail':'str', 'comment':'str'},\n",
    "#         date_format='%d/%m/%Y %H:%M:%S'\n",
    "#     )\n",
    "# import os\n",
    "# if os.path.exists(get_path('models/random_search_mini_spw.joblib', 'outputs')):\n",
    "#     continue\n",
    "# else:\n",
    "#     print(\"nope\")\n",
    "# os.path.exists(get_path('models/random_search_mini_spw2.joblib', 'outputs')) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9952c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_path(input_name):\n",
    "#     mod_loc = \"models/\"+input_name+\".joblib\"\n",
    "#     return get_path(mod_loc, 'outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "147a80c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.exists(model_path(\"random_search_mini_spw\")) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d6aaeaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing model from saved files...\n"
     ]
    }
   ],
   "source": [
    "model_name = \"random_search_mini_spw\"\n",
    "\n",
    "if os.path.exists(model_path(model_name)) == False:\n",
    "    print(\"Starting hyperparameter tuning...\")\n",
    "    random_search.fit(\n",
    "        X_train_two, y_train_two,\n",
    "        # early_stopping_rounds=50, # Stop if validation metric doesn't improve for 50 rounds\n",
    "        eval_set=eval_set,\n",
    "        verbose=False # Set to True if you want to watch the early stopping logs\n",
    "    )\n",
    "\n",
    "    # Save\n",
    "    joblib.dump(random_search, model_path(model_name))\n",
    "else:\n",
    "    print(\"Importing model from saved files...\")\n",
    "    random_search = joblib.load(model_path(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5a0c2af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving result\n",
    "# import joblib\n",
    "\n",
    "# filename = 'random_search_results.joblib'\n",
    "# joblib.dump(random_search, filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f1de8f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "# joblib.dump(random_search, get_path('models/random_search_mini_spw.joblib', 'outputs'))\n",
    "\n",
    "# Make a chime to indicate completion\n",
    "_ = playsound(get_path('completed.mp3', 'code'), block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a3ed4348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found:\n",
      "{'colsample_bytree': np.float64(0.749816047538945), 'gamma': 0, 'learning_rate': np.float64(0.0631960890611875), 'max_depth': 6, 'n_estimators': 238, 'subsample': np.float64(0.8387400631785948)}\n",
      "Best F1 Score (averaged across CV folds): 0.6509\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(random_search.best_params_)\n",
    "print(f\"Best F1 Score (averaged across CV folds): {random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d614dcc",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3ae4e7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2.0_deep</td>\n",
       "      <td>0.066555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>4.0_shallow</td>\n",
       "      <td>0.037670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.0_shallow</td>\n",
       "      <td>0.037144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>10.0_shallow</td>\n",
       "      <td>0.036348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>raw_ro_rollstd_288</td>\n",
       "      <td>0.035889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>daysince_4.0_deep</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>daysince_6.0_deep</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>daysince_5.0_deep</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>daysince_7.0_deep</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>daysince_10.0_deep</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   feat  importance\n",
       "108            2.0_deep    0.066555\n",
       "100         4.0_shallow    0.037670\n",
       "97          1.0_shallow    0.037144\n",
       "106        10.0_shallow    0.036348\n",
       "63   raw_ro_rollstd_288    0.035889\n",
       "..                  ...         ...\n",
       "140   daysince_4.0_deep    0.000000\n",
       "142   daysince_6.0_deep    0.000000\n",
       "141   daysince_5.0_deep    0.000000\n",
       "143   daysince_7.0_deep    0.000000\n",
       "146  daysince_10.0_deep    0.000000\n",
       "\n",
       "[147 rows x 2 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = random_search.best_estimator_\n",
    "feature_importances = best_model.feature_importances_\n",
    "# map scores to feature names\n",
    "# feature_importances\n",
    "feature_names = X_train.columns.tolist() \n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feat': feature_names,\n",
    "    'importance': feature_importances\n",
    "})\n",
    "\n",
    "# sort importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# print(feature_importance_df)\n",
    "feature_importance_df\n",
    "\n",
    "# most important features\n",
    "# print(feature_importance_df.head(25)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fd348692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold_importance = 0.95\n",
    "# calculate most important 90 percent of the importance\n",
    "feature_importance_df['cumulative_imp'] = feature_importance_df['importance'].cumsum()\n",
    "features_percent = feature_importance_df[feature_importance_df['cumulative_imp'] <= threshold_importance].shape[0] + 1\n",
    "features_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4517ba9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 % (most important features): 83\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "      <th>importance</th>\n",
       "      <th>cumulative_imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2.0_deep</td>\n",
       "      <td>0.066555</td>\n",
       "      <td>0.066555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>4.0_shallow</td>\n",
       "      <td>0.037670</td>\n",
       "      <td>0.104225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.0_shallow</td>\n",
       "      <td>0.037144</td>\n",
       "      <td>0.141369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>10.0_shallow</td>\n",
       "      <td>0.036348</td>\n",
       "      <td>0.177717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>raw_ro_rollstd_288</td>\n",
       "      <td>0.035889</td>\n",
       "      <td>0.213607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>raw_ro_rollstd_4</td>\n",
       "      <td>0.003889</td>\n",
       "      <td>0.937437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>raw_ro_rollsum_288</td>\n",
       "      <td>0.003647</td>\n",
       "      <td>0.941085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>daysince_2.0_shallow</td>\n",
       "      <td>0.003620</td>\n",
       "      <td>0.944705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>daysince_3.0_shallow</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>0.948287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>raw_ro_rollmean_6</td>\n",
       "      <td>0.003476</td>\n",
       "      <td>0.951764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     feat  importance  cumulative_imp\n",
       "108              2.0_deep    0.066555        0.066555\n",
       "100           4.0_shallow    0.037670        0.104225\n",
       "97            1.0_shallow    0.037144        0.141369\n",
       "106          10.0_shallow    0.036348        0.177717\n",
       "63     raw_ro_rollstd_288    0.035889        0.213607\n",
       "..                    ...         ...             ...\n",
       "35       raw_ro_rollstd_4    0.003889        0.937437\n",
       "62     raw_ro_rollsum_288    0.003647        0.941085\n",
       "128  daysince_2.0_shallow    0.003620        0.944705\n",
       "129  daysince_3.0_shallow    0.003583        0.948287\n",
       "41      raw_ro_rollmean_6    0.003476        0.951764\n",
       "\n",
       "[83 rows x 3 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most important features:\n",
    "print(round(threshold_importance*100), \"% (most important features):\", features_percent)\n",
    "feature_importance_df.head(features_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f820c002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining 5 % (least important features): 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "      <th>importance</th>\n",
       "      <th>cumulative_imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>raw_ro</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>0.955070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>raw_ro_rollsum_3</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>0.958338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>raw_ro_rollsum_5</td>\n",
       "      <td>0.003143</td>\n",
       "      <td>0.961482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>raw_ro_rollslope_5</td>\n",
       "      <td>0.003026</td>\n",
       "      <td>0.964507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>daysince_8.0_shallow</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>0.967506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>daysince_4.0_deep</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>daysince_6.0_deep</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>daysince_5.0_deep</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>daysince_7.0_deep</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>daysince_10.0_deep</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     feat  importance  cumulative_imp\n",
       "1                  raw_ro    0.003307        0.955070\n",
       "30       raw_ro_rollsum_3    0.003268        0.958338\n",
       "38       raw_ro_rollsum_5    0.003143        0.961482\n",
       "40     raw_ro_rollslope_5    0.003026        0.964507\n",
       "134  daysince_8.0_shallow    0.002998        0.967506\n",
       "..                    ...         ...             ...\n",
       "140     daysince_4.0_deep    0.000000        1.000000\n",
       "142     daysince_6.0_deep    0.000000        1.000000\n",
       "141     daysince_5.0_deep    0.000000        1.000000\n",
       "143     daysince_7.0_deep    0.000000        1.000000\n",
       "146    daysince_10.0_deep    0.000000        1.000000\n",
       "\n",
       "[64 rows x 3 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature_importance_df.tail(1)\n",
    "# Least important features:\n",
    "print(\"Remaining\", round((1-threshold_importance)*100), \"% (least important features):\", len(feature_names)-features_percent)\n",
    "feature_importance_df.tail(len(feature_names)-features_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eb8627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with 0 importance: 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "      <th>importance</th>\n",
       "      <th>cumulative_imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ra_rain_lag2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ra_rain_lag1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>ra_rain_rollstd_4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>ra_rain_rollsum_4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>ra_rain_rollsum_3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>ra_rain_rollstd_3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>ra_rain_rollsum_2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ra_rain_lag4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ra_rain_lag12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ra_rain_lag6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ra_rain_lag5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>ra_rain_rollsum_5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>ra_rain_rollslope_12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>ra_rain_rollslope_5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>ra_rain_rollsum_6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>ra_rain_rollstd_6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>ra_rain_rollstd_5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>weir_level_cal</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>diff_ro_cal</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>daysince_10.0_shallow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>daysince_2.0_deep</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>daysince_1.0_deep</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>daysince_3.0_deep</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>daysince_4.0_deep</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>daysince_6.0_deep</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>daysince_5.0_deep</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>daysince_7.0_deep</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>daysince_10.0_deep</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      feat  importance  cumulative_imp\n",
       "17            ra_rain_lag2         0.0             1.0\n",
       "16            ra_rain_lag1         0.0             1.0\n",
       "72       ra_rain_rollstd_4         0.0             1.0\n",
       "71       ra_rain_rollsum_4         0.0             1.0\n",
       "68       ra_rain_rollsum_3         0.0             1.0\n",
       "69       ra_rain_rollstd_3         0.0             1.0\n",
       "65       ra_rain_rollsum_2         0.0             1.0\n",
       "19            ra_rain_lag4         0.0             1.0\n",
       "22           ra_rain_lag12         0.0             1.0\n",
       "21            ra_rain_lag6         0.0             1.0\n",
       "20            ra_rain_lag5         0.0             1.0\n",
       "74       ra_rain_rollsum_5         0.0             1.0\n",
       "82    ra_rain_rollslope_12         0.0             1.0\n",
       "76     ra_rain_rollslope_5         0.0             1.0\n",
       "77       ra_rain_rollsum_6         0.0             1.0\n",
       "78       ra_rain_rollstd_6         0.0             1.0\n",
       "75       ra_rain_rollstd_5         0.0             1.0\n",
       "96          weir_level_cal         0.0             1.0\n",
       "117            diff_ro_cal         0.0             1.0\n",
       "136  daysince_10.0_shallow         0.0             1.0\n",
       "138      daysince_2.0_deep         0.0             1.0\n",
       "137      daysince_1.0_deep         0.0             1.0\n",
       "139      daysince_3.0_deep         0.0             1.0\n",
       "140      daysince_4.0_deep         0.0             1.0\n",
       "142      daysince_6.0_deep         0.0             1.0\n",
       "141      daysince_5.0_deep         0.0             1.0\n",
       "143      daysince_7.0_deep         0.0             1.0\n",
       "146     daysince_10.0_deep         0.0             1.0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Features with 0 importance:\n",
    "print(\"Features with 0 importance:\", len(feature_importance_df[feature_importance_df['importance']==0]))\n",
    "\n",
    "feature_importance_df[feature_importance_df['importance']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0dc876",
   "metadata": {},
   "source": [
    "Feature importance by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d908cbbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "      <th>importance</th>\n",
       "      <th>cumulative_imp</th>\n",
       "      <th>soil</th>\n",
       "      <th>runoff</th>\n",
       "      <th>rain</th>\n",
       "      <th>calibration</th>\n",
       "      <th>most</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2.0_deep</td>\n",
       "      <td>0.066555</td>\n",
       "      <td>0.066555</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>4.0_shallow</td>\n",
       "      <td>0.037670</td>\n",
       "      <td>0.104225</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.0_shallow</td>\n",
       "      <td>0.037144</td>\n",
       "      <td>0.141369</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>10.0_shallow</td>\n",
       "      <td>0.036348</td>\n",
       "      <td>0.177717</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>raw_ro_rollstd_288</td>\n",
       "      <td>0.035889</td>\n",
       "      <td>0.213607</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>daysince_4.0_deep</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>daysince_6.0_deep</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>daysince_5.0_deep</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>daysince_7.0_deep</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>daysince_10.0_deep</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   feat  importance  cumulative_imp   soil  runoff   rain  \\\n",
       "108            2.0_deep    0.066555        0.066555   True   False  False   \n",
       "100         4.0_shallow    0.037670        0.104225   True   False  False   \n",
       "97          1.0_shallow    0.037144        0.141369   True   False  False   \n",
       "106        10.0_shallow    0.036348        0.177717   True   False  False   \n",
       "63   raw_ro_rollstd_288    0.035889        0.213607  False    True  False   \n",
       "..                  ...         ...             ...    ...     ...    ...   \n",
       "140   daysince_4.0_deep    0.000000        1.000000   True   False  False   \n",
       "142   daysince_6.0_deep    0.000000        1.000000   True   False  False   \n",
       "141   daysince_5.0_deep    0.000000        1.000000   True   False  False   \n",
       "143   daysince_7.0_deep    0.000000        1.000000   True   False  False   \n",
       "146  daysince_10.0_deep    0.000000        1.000000   True   False  False   \n",
       "\n",
       "     calibration   most   zero  \n",
       "108        False   True  False  \n",
       "100        False   True  False  \n",
       "97         False   True  False  \n",
       "106        False   True  False  \n",
       "63         False   True  False  \n",
       "..           ...    ...    ...  \n",
       "140        False  False   True  \n",
       "142        False  False   True  \n",
       "141        False  False   True  \n",
       "143        False  False   True  \n",
       "146        False  False   True  \n",
       "\n",
       "[147 rows x 9 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_dict = {\n",
    "    'soil': '_deep|_shallow',\n",
    "    'runoff':'ro',\n",
    "    'rain':'rain',\n",
    "    'calibration':'_cal'\n",
    "}\n",
    "\n",
    "for col_name, pattern in mapping_dict.items():\n",
    "    feature_importance_df[col_name] = feature_importance_df['feat'].str.contains(pattern, case=False, regex=True)\n",
    "\n",
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "31c6b4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total features</th>\n",
       "      <th>Above threshold</th>\n",
       "      <th>Below threshold</th>\n",
       "      <th>Zero importance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>soil</th>\n",
       "      <td>40</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>runoff</th>\n",
       "      <td>82</td>\n",
       "      <td>45</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rain</th>\n",
       "      <td>45</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calibration</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Total features  Above threshold  Below threshold  Zero importance\n",
       "Category                                                                      \n",
       "soil                     40               28               12                9\n",
       "runoff                   82               45               37               12\n",
       "rain                     45               15               30               17\n",
       "calibration               3                1                2                2"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_df['most'] = (feature_importance_df['cumulative_imp'] <= threshold_importance)\n",
    "feature_importance_df['zero'] = (feature_importance_df['importance'] == 0)\n",
    "\n",
    "cat_cols = list(mapping_dict.keys())\n",
    "\n",
    "table_feature_cat_importance = pd.DataFrame({\n",
    "    'Total features': feature_importance_df[cat_cols].sum(),\n",
    "    'Above threshold': feature_importance_df[feature_importance_df['most']][cat_cols].sum(),\n",
    "    'Below threshold': feature_importance_df[~feature_importance_df['most']][cat_cols].sum(),\n",
    "    'Zero importance': feature_importance_df[feature_importance_df['zero']][cat_cols].sum()\n",
    "}).fillna(0).astype(int)\n",
    "\n",
    "del cat_cols, mapping_dict\n",
    "\n",
    "table_feature_cat_importance.index.name = 'Category'\n",
    "\n",
    "table_feature_cat_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc23c8fa",
   "metadata": {},
   "source": [
    "# draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee0ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = random_search.best_params_\n",
    "\n",
    "# final model w optimized params\n",
    "final_optimized_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    # use_label_encoder=False,\n",
    "    tree_method='hist',\n",
    "    random_state=42,\n",
    "    **best_params # unpack best params\n",
    ")\n",
    "\n",
    "final_optimized_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497b35b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a chime to indicate completion\n",
    "_ = playsound(get_path('completed.mp3', 'code'), block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3d368c",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d575a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicting...\")\n",
    "y_pred = final_optimized_model.predict(X_test)\n",
    "y_pred_proba = final_optimized_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# current fold info\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "# # Find the optimal threshold\n",
    "# Calculate F1-score for every possible threshold\n",
    "fscores = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "# Handle potential division by zero warnings\n",
    "# if no positive predictions were made\n",
    "fscores[np.isnan(fscores)] = 0 \n",
    "\n",
    "# index of highest F1\n",
    "optimal_idx = np.argmax(fscores)\n",
    "best_threshold = thresholds[optimal_idx] # Note: thresholds array is one element shorter than P/R arrays\n",
    "\n",
    "print(f\"Best threshold: {best_threshold:.4f}\")\n",
    "\n",
    "# print(f\"Fold {fold} Optimal Threshold (Max F1): {best_threshold:.4f}\")\n",
    "\n",
    "\n",
    "print(\"Getting metrics...\")\n",
    "print(\"F1\\tAcc\\tPre\\tRec\")\n",
    "print(\n",
    "    f1_score(y_test, y_pred),\n",
    "    accuracy_score(y_test, y_pred),\n",
    "    precision_score(y_test, y_pred),\n",
    "    recall_score(y_test, y_pred),\n",
    "    sep =\"\\t\"\n",
    ")\n",
    "\n",
    "# all_f1.append(fold_f1)\n",
    "# all_accuracy.append(fold_accuracy)\n",
    "# all_precision.append(fold_precision)\n",
    "# all_recall.append(fold_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06f4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_f1 = f1_score(y_val, y_test_pred)\n",
    "# final_accuracy = accuracy_score(y_val, y_test_pred)\n",
    "# final_precision = precision_score(y_val, y_test_pred)\n",
    "# final_recall = recall_score(y_val, y_test_pred)\n",
    "\n",
    "# print(f\"{fold}\\tF1: {final_f1:.4f}\\tAcc{final_accuracy:.4f}\\tPrec{final_precision:.4f}\\tRec: {final_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e08a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"i\\tF1\\tAcc\\tPre\\tRec\")\n",
    "# for i in range(len(all_f1)):\n",
    "#     print(i, round(all_f1[i], 4), round(all_accuracy[i], 4), round(all_precision[i], 4), round(all_recall[i],4), sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_jupyter (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
