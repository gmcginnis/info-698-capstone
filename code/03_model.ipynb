{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "68a3c4a0",
      "metadata": {
        "id": "68a3c4a0"
      },
      "source": [
        "# Data Splitting and Modelling\n",
        "\n",
        "Author: Gillian A. McGinnis, final-semester M.S. Information Science - Machine Learning  \n",
        "The University of Arizona College of Information  \n",
        "INFO 698 - Capstone  \n",
        "Start date: 21 October 2025  \n",
        "Last updated: 25 November 2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "6d80b978",
      "metadata": {
        "id": "6d80b978",
        "outputId": "182a1dfe-7b0b-44f3-a911-2d885d34c4ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nModule providing code for test/train split and sliding window creation. Relies on 01_clean.ipynb completion.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "\"\"\"\n",
        "Module providing code for test/train split and sliding window creation. Relies on 01_clean.ipynb completion.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de84a462",
      "metadata": {
        "id": "de84a462"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7208265e",
      "metadata": {
        "id": "7208265e"
      },
      "source": [
        "### Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU Setup\n",
        "%load_ext cudf.pandas\n",
        "import pandas as pd\n",
        "import cudf\n",
        "import cupy as cp\n",
        "\n",
        "import cuml.accel\n",
        "cuml.accel.install()\n",
        "\n",
        "# General packages\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from scipy.stats import randint, uniform\n",
        "# For getting medians for windowing/smoothing\n",
        "from scipy.signal import medfilt\n",
        "\n",
        "# For saving models\n",
        "import joblib\n",
        "import pickle\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit, train_test_split, RandomizedSearchCV, TunedThresholdClassifierCV\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, precision_recall_curve, make_scorer, roc_auc_score, auc\n",
        "# For help with model tuning\n",
        "from sklearn.base import clone"
      ],
      "metadata": {
        "id": "-2FCjcFbVrfq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "894d9317-fe57-4fe9-cb3c-bbdfd1418ec3"
      },
      "id": "-2FCjcFbVrfq",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cudf.pandas extension is already loaded. To reload it, use:\n",
            "  %reload_ext cudf.pandas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "if os.getcwd() == '/content':\n",
        "  gh_repo = userdata.get('gh_repo')\n",
        "  if not os.path.exists(f'{gh_repo}'):\n",
        "    print(\"Cloning repo...\")\n",
        "    gh_pat = userdata.get('gh_pat')\n",
        "    gh_user = userdata.get('gh_user')\n",
        "    repo_url = f'https://{gh_pat}@github.com/{gh_user}/{gh_repo}'\n",
        "    !git clone {repo_url}\n",
        "    del gh_pat, gh_user, repo_url\n",
        "  print(\"Changing wd...\")\n",
        "  os.chdir(f'{gh_repo}/code')\n",
        "  del gh_repo\n",
        "\n",
        "# # Verify the current working directory\n",
        "print(f\"Current working directory is: {os.getcwd()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxJXyRMZHzAc",
        "outputId": "051b870e-aa29-48d5-82e0-adbd12c494ba"
      },
      "id": "dxJXyRMZHzAc",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory is: /content/info-698-capstone/code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "0bb557fb",
      "metadata": {
        "id": "0bb557fb"
      },
      "outputs": [],
      "source": [
        "# For data importing and exporting\n",
        "from helper_utils import get_path, model_path, apply_model, report_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "9c40baf7",
      "metadata": {
        "id": "9c40baf7"
      },
      "outputs": [],
      "source": [
        "# # To make it easier to tell when processes have completed -- can delete later\n",
        "# Google colab compatible:\n",
        "# https://stackoverflow.com/a/68582785/23486987\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "def play_chime():\n",
        "  return Audio(get_path('completed.mp3', 'code'), autoplay=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "dcb5d8f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "dcb5d8f6",
        "outputId": "719e55d3-4bf0-42ae-cd09-eb857d35c350"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<details>\n",
              "<summary>Click to view session information</summary>\n",
              "<pre>\n",
              "-----\n",
              "cudf                25.10.00\n",
              "cuml                25.10.00\n",
              "cupy                13.6.0\n",
              "google              NA\n",
              "helper_utils        NA\n",
              "joblib              1.5.2\n",
              "numpy               2.0.2\n",
              "pandas              2.2.2\n",
              "scipy               1.16.3\n",
              "session_info        v1.0.1\n",
              "sklearn             1.6.1\n",
              "torch               2.9.0+cu126\n",
              "xgboost             3.1.2\n",
              "-----\n",
              "IPython             7.34.0\n",
              "jupyter_client      7.4.9\n",
              "jupyter_core        5.9.1\n",
              "notebook            6.5.7\n",
              "-----\n",
              "Python 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
              "Linux-6.6.105+-x86_64-with-glibc2.35\n",
              "-----\n",
              "Session information updated at 2025-11-27 21:21\n",
              "</pre>\n",
              "</details>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "## (Optional chunk)\n",
        "# Current session information\n",
        "\n",
        "# From StackOverflow,\n",
        "# https://stackoverflow.com/a/62128239/23486987\n",
        "try:\n",
        "    import session_info\n",
        "except:\n",
        "    !pip install session_info\n",
        "    import session_info\n",
        "# !pip install session_info\n",
        "# import session_info\n",
        "session_info.show(dependencies=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure GPU active\n",
        "# !nvidia-smi\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"Warning: GPU not found!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t3ZBgBzJqaU",
        "outputId": "2c4ccb7e-0fc3-49e0-915a-825acb486fb2"
      },
      "id": "1t3ZBgBzJqaU",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inputs"
      ],
      "metadata": {
        "id": "sYzRprhfshvL"
      },
      "id": "sYzRprhfshvL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Constants"
      ],
      "metadata": {
        "id": "51y2Qa6k3dRK"
      },
      "id": "51y2Qa6k3dRK"
    },
    {
      "cell_type": "code",
      "source": [
        "## Variable of interest\n",
        "# for the model to predict\n",
        "VAR_OF_INTEREST = \"obstruction_ro\"\n",
        "\n",
        "## Date subsets\n",
        "# (for testing smaller runs)\n",
        "# Set to None to use all data\n",
        "# DATE_START = None\n",
        "# DATE_END = None\n",
        "DATE_START = '2001-02-01 00:00:00'\n",
        "DATE_END = '2011-12-31 23:59:59'\n",
        "\n",
        "## Cutoff for correlation for feature removal\n",
        "# (i.e., columns that are at least this correlated will be removed)\n",
        "CORR_CUTOFF = 0.97\n",
        "\n",
        "## Number of splits\n",
        "# for hyperparameter tuning via expanding window\n",
        "N_SPLITS = 5\n",
        "\n",
        "## Number of rounds after which to stop the build\n",
        "# once the performance stops improving\n",
        "EARLY_STOPPING_ROUNDS = 50\n",
        "\n",
        "# Define hyperparameter distributions for random search\n",
        "N_ESTIMATORS = 1000\n",
        "\n",
        "## Number of times to iterate the tuning loop\n",
        "# (set to a small number for testing)\n",
        "N_ITERATIONS = 5\n",
        "# N_ITERATIONS = 50\n",
        "\n",
        "\n",
        "## Smoothing window sizes -- must be odd\n",
        "# Smallest window size\n",
        "WINDOW_MIN = 1\n",
        "# Largest window size\n",
        "WINDOW_MAX = 35\n",
        "\n",
        "## Model names\n",
        "MODEL_NAME = \"xgb_testing\"\n",
        "FITTED_MODEL_NAME = \"xgb_testing_fitted\"\n",
        "\n",
        "## Seed for reproducability\n",
        "SEED = 42"
      ],
      "metadata": {
        "id": "wS5xDyZm3lsJ"
      },
      "id": "wS5xDyZm3lsJ",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "1e7c4847",
      "metadata": {
        "id": "1e7c4847"
      },
      "outputs": [],
      "source": [
        "# Set seed\n",
        "np.random.seed(SEED)\n",
        "cp.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### Note ###\n",
        "# # Set to None later -- just a smaller subset for feature engineering testing\n",
        "# # DATE_START = '2000-01-01 00:00:00'\n",
        "# DATE_START = '2001-02-01 00:00:00'\n",
        "# DATE_END = '2011-12-31 23:59:59'\n",
        "# DATE_START= None\n",
        "# DATE_END = None\n",
        "# ######"
      ],
      "metadata": {
        "id": "hj1W4Bi8s3NB"
      },
      "id": "hj1W4Bi8s3NB",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "30ef4832",
      "metadata": {
        "id": "30ef4832"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "cd2e4fca",
      "metadata": {
        "id": "cd2e4fca"
      },
      "outputs": [],
      "source": [
        "united_water = pd.read_parquet(get_path('clean/water_nocal.parquet'))\n",
        "united_soil = pd.read_parquet(get_path('clean/soil.parquet'))\n",
        "\n",
        "data_cal = pd.read_parquet(get_path('clean/calibration.parquet'))\n",
        "data_cal = data_cal.rename(columns={'weir_level':'weir_level_cal'})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_cal.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QEIHj-wNiBd",
        "outputId": "949e3ed6-4f34-4ef6-b6d2-01ce065bc1fd"
      },
      "id": "9QEIHj-wNiBd",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'cudf.core.dataframe.DataFrame'>\n",
            "DatetimeIndex: 6136 entries, 1994-01-03 08:46:00 to 2025-08-01 09:10:00\n",
            "Data columns (total 1 columns):\n",
            " #   Column          Non-Null Count  Dtype\n",
            "---  ------          --------------  -----\n",
            " 0   weir_level_cal  6136 non-null   int8\n",
            "dtypes: int8(1)\n",
            "memory usage: 53.9 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d98f94e4",
      "metadata": {
        "id": "d98f94e4"
      },
      "source": [
        "#### Memory improvements\n",
        "\n",
        "Small amount of data wrangling for memory improvements (some as a consequence of importing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "689c32ff",
      "metadata": {
        "id": "689c32ff"
      },
      "outputs": [],
      "source": [
        "# Select columns of interest\n",
        "data_water = united_water.drop(columns=['raw_rain', 'chk_note_rain', 'chk_fail_rain', 'chk_note_ro', 'chk_fail_ro', 'comment_ro', 'source_ro'])\n",
        "\n",
        "# Cleanup\n",
        "del united_water\n",
        "\n",
        "# Remove duplicate entries\n",
        "data_water = data_water.reset_index().drop_duplicates(keep='first').set_index('datetime')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "357e91e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "357e91e4",
        "outputId": "eae5f646-14d1-4780-f25f-7fc523ca95f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'cudf.core.dataframe.DataFrame'>\n",
            "DatetimeIndex: 3581782 entries, 1989-06-21 13:00:00 to 2025-08-01 13:00:00\n",
            "Data columns (total 3 columns):\n",
            " #   Column          Dtype\n",
            "---  ------          -----\n",
            " 0   ra_rain         float32\n",
            " 1   raw_ro          float32\n",
            " 2   obstruction_ro  bool\n",
            "dtypes: bool(1), float32(2)\n",
            "memory usage: 59.4 MB\n"
          ]
        }
      ],
      "source": [
        "water_drops = ['level_ro', 'obstruction_ro', 'gap_fill_ro', 'weir_cleaning_ro', 'spike_ro', 'calibration_ro']\n",
        "water_drops.remove(VAR_OF_INTEREST)\n",
        "\n",
        "data_water = data_water.drop(water_drops, axis=1)\n",
        "\n",
        "del water_drops\n",
        "\n",
        "data_water.info(memory_usage=\"deep\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "1706a0d9",
      "metadata": {
        "id": "1706a0d9"
      },
      "outputs": [],
      "source": [
        "united_soil['sample'] = united_soil['sample'].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "aa5776e5",
      "metadata": {
        "id": "aa5776e5"
      },
      "outputs": [],
      "source": [
        "# Subset data (if applicable)\n",
        "data_water = data_water[DATE_START:DATE_END]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecb9dc61",
      "metadata": {
        "id": "ecb9dc61"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afa843b0",
      "metadata": {
        "id": "afa843b0"
      },
      "source": [
        "### Distance from Event"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24851c9b",
      "metadata": {
        "id": "24851c9b"
      },
      "outputs": [],
      "source": [
        "def timesince_feat(input_df, input_col, input_unit):\n",
        "    output_df = input_df\n",
        "    instances = output_df[input_col].notna()\n",
        "    # Create groupings based on most recent instance\n",
        "    group_id = instances.cumsum()\n",
        "    # Exclude the first grouping\n",
        "    # otherwise it assumes there was an event just prior to the first entry\n",
        "    group_id = group_id.replace(0, np.nan)\n",
        "    # Create new column to count the distance in days since the point\n",
        "    # which resets to 0 at each new point\n",
        "    output_df['timestamp'] = pd.to_datetime(output_df.index)\n",
        "    # Get start timestamp of the group\n",
        "    output_df['ts_start'] = output_df.groupby(group_id)['timestamp'].transform('min')\n",
        "    # Calculate the distance\n",
        "    if input_unit == \"minutes\":\n",
        "        output_df[f\"minsince_{input_col}\"] = (output_df['timestamp'] - output_df['ts_start']).dt.total_seconds().div(60).astype(np.float32)\n",
        "    elif input_unit == \"days\":\n",
        "        output_df[f\"daysince_{input_col}\"] = (output_df['timestamp'] - output_df['ts_start']).dt.days.astype(np.float32)\n",
        "    # Remove extra cols\n",
        "    output_df = output_df.drop(columns=['timestamp', 'ts_start'])\n",
        "    return output_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "830a5cbc",
      "metadata": {
        "id": "830a5cbc"
      },
      "source": [
        "#### Rain\n",
        "Create feature which tracks how recent a rain event occurred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0a3b363",
      "metadata": {
        "id": "e0a3b363"
      },
      "outputs": [],
      "source": [
        "data_water = timesince_feat(data_water, 'ra_rain', \"minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e786a7e9",
      "metadata": {
        "id": "e786a7e9"
      },
      "source": [
        "### Rain event\n",
        "\n",
        "Keep track of cumulative rainfall during a specific event."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59b7c231",
      "metadata": {
        "id": "59b7c231"
      },
      "outputs": [],
      "source": [
        "# Create index of instances where there is a data point\n",
        "rain_event = (data_water['ra_rain'].isnull() & ((data_water['minsince_ra_rain'] >= 5.0) & (data_water['minsince_ra_rain'] != 0)))\n",
        "# Create groupings based on most recent instance\n",
        "rain_event_id = rain_event.cumsum()\n",
        "# Create new column to count number of records since the point\n",
        "# which resets to 0 at each new point\n",
        "data_water['eventsum_ra_rain'] = data_water.groupby(rain_event_id)['ra_rain'].cumsum()\n",
        "\n",
        "del rain_event, rain_event_id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96a81ae4",
      "metadata": {
        "id": "96a81ae4"
      },
      "source": [
        "### Decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46f01638",
      "metadata": {
        "id": "46f01638"
      },
      "outputs": [],
      "source": [
        "def decay_feat(input_df, input_col, input_dec_rate = -0.1):\n",
        "    output_df = input_df\n",
        "    if f\"minsince_{input_col}\" not in output_df.columns:\n",
        "        output_df = timesince_feat(input_df = output_df, input_col = input_col, input_unit = \"minutes\")\n",
        "    # Update for GPU for overflow fix\n",
        "    output_df[f\"minsince_{input_col}\"] = output_df[f\"minsince_{input_col}\"].astype(np.float64)\n",
        "\n",
        "    output_df[f\"decayrate{input_dec_rate}_{input_col}\"] = np.exp(input_dec_rate * output_df[f\"minsince_{input_col}\"]).astype(np.float32)\n",
        "    output_df[f\"ffill_{input_col}\"] = output_df[input_col].ffill()\n",
        "    output_df[f\"decay{input_dec_rate}_{input_col}\"] = (output_df[f\"ffill_{input_col}\"] * output_df[f\"decayrate{input_dec_rate}_{input_col}\"])\n",
        "\n",
        "    return output_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "947bae8b",
      "metadata": {
        "id": "947bae8b"
      },
      "outputs": [],
      "source": [
        "# Replace NAs in rain with 0\n",
        "data_water['ra_rain'] = data_water['ra_rain'].fillna(0)\n",
        "\n",
        "# Apply decay function\n",
        "data_water = decay_feat(data_water, 'eventsum_ra_rain')\n",
        "\n",
        "# Drop extra column\n",
        "# minutes since rain event will be the same as minutes since most recent rain\n",
        "data_water = data_water.drop('minsince_eventsum_ra_rain', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6154e3b7",
      "metadata": {
        "id": "6154e3b7"
      },
      "source": [
        "### Lag features\n",
        "\n",
        "Because XGBoost predicts on one data point at a time, time series data must include \"lag\" features (i.e., data points prior to that event) in order to better repsent recent history."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c5d3c7b",
      "metadata": {
        "id": "8c5d3c7b"
      },
      "source": [
        "#### Consistent cols\n",
        "\n",
        "Modify the rows to prevent inappropriate data shifts.\n",
        "This expands the index to include _all_ possible 5 minute time stamps, so that lagging by index difference guarantees the temporal consistency of lagged features.\n",
        "This is important because there are some gaps of data, and it would be incorrect for a features that measures \"rainfall 5 minutes ago\" was actually representing that of the immediately previous point a few hours (or, in some cases, _years_) back due to large data gaps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0e75eb6",
      "metadata": {
        "id": "a0e75eb6"
      },
      "outputs": [],
      "source": [
        "original_indices = data_water.index.copy()\n",
        "\n",
        "new_index = pd.date_range(start = data_water.index.min(),\n",
        "                          end = data_water.index.max(),\n",
        "                          freq = '5min')\n",
        "\n",
        "# Reindex\n",
        "data_water = data_water.reindex(new_index)\n",
        "\n",
        "# Cleanup\n",
        "del new_index\n",
        "\n",
        "# # Return\n",
        "# data_water = data_water.loc[original_indices]\n",
        "# del original_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62d8945f",
      "metadata": {
        "id": "62d8945f"
      },
      "source": [
        "Get values from other recent time stamps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09c4b606",
      "metadata": {
        "id": "09c4b606"
      },
      "outputs": [],
      "source": [
        "def lag_feats(input_df, input_cols, input_lags):\n",
        "    output_df = input_df#.copy()\n",
        "    for col in input_cols:\n",
        "        for lag in input_lags:\n",
        "            output_df[f\"{col}_lag{lag}\"] = output_df[col].shift(lag)\n",
        "    return output_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "797920ef",
      "metadata": {
        "id": "797920ef"
      },
      "outputs": [],
      "source": [
        "# Columns to get temporal stats on\n",
        "cols_to_shift = ['raw_ro', 'ra_rain']\n",
        "\n",
        "# data at 5-min increments -- lag to record values at 5m, 10m, 15m, 20m, 25m, 30m, 1h, 2h, 3h prior\n",
        "lags_of_interest = [1, 2, 3, 4, 5, 6, 12, 24, 36]\n",
        "\n",
        "data_water = lag_feats(data_water, cols_to_shift, lags_of_interest)\n",
        "\n",
        "# data_water.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ddce5f3",
      "metadata": {
        "id": "0ddce5f3"
      },
      "source": [
        "### Rolling stats\n",
        "\n",
        "Similarly to lag features, rolling statistics can help XGBoost determine how typical recent behavior is, or any patterns that may emerge with \"abnormal\" or extreme behavior (such as a sharp slope)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb010731",
      "metadata": {
        "id": "eb010731"
      },
      "outputs": [],
      "source": [
        "def rolling_feats(input_df, input_cols, input_windows, input_mtype = \"mean\"):\n",
        "    output_df = input_df\n",
        "\n",
        "    # Create dummy series of index values (0, 1, 2, ... N)\n",
        "    # x represents the position within the df for the regression calculation\n",
        "    x_series = pd.Series(np.arange(len(output_df)), index=output_df.index)\n",
        "\n",
        "    for col in input_cols:\n",
        "        for window in input_windows:\n",
        "            # Calculate general stats\n",
        "            if input_mtype == \"mean\":\n",
        "                output_df[f\"{col}_rollmean_{window}\"] = output_df[col].rolling(window).mean().astype(np.float32)\n",
        "            elif input_mtype == \"sum\":\n",
        "                output_df[f\"{col}_rollsum_{window}\"] = output_df[col].rolling(window).sum().astype(np.float32)\n",
        "            elif input_mtype == \"both\":\n",
        "                output_df[f\"{col}_rollmean_{window}\"] = output_df[col].rolling(window).mean().astype(np.float32)\n",
        "                output_df[f\"{col}_rollsum_{window}\"] = output_df[col].rolling(window).sum().astype(np.float32)\n",
        "            output_df[f\"{col}_rollstd_{window}\"] = output_df[col].rolling(window).std().astype(np.float32)\n",
        "\n",
        "            # Calculating slope w vecotrized options\n",
        "            # Calculate covariance of y (data) vs X (index)\n",
        "            rolling_cov = output_df[col].rolling(window).cov(x_series)\n",
        "            # Calculate variance of X (index)\n",
        "            rolling_var_x = x_series.rolling(window).var()\n",
        "            # Slope = Cov(Y, X) / Var(X)\n",
        "            output_df[f\"{col}_rollslope_{window}\"] = (rolling_cov / rolling_var_x).astype(np.float32)\n",
        "    return output_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf7eaaaf",
      "metadata": {
        "id": "cf7eaaaf"
      },
      "outputs": [],
      "source": [
        "# Inclusive of current point--\n",
        "# 10m, 15m, 20m, 25m, 30m, 1h, 3h, 6h, 12h, 24h\n",
        "windows_of_interest = [2, 3, 4, 5, 6, 12, 36, 72, 144, 288]\n",
        "\n",
        "data_water = rolling_feats(data_water, ['raw_ro'], windows_of_interest, \"both\")\n",
        "data_water = rolling_feats(data_water, ['ra_rain'], windows_of_interest, \"sum\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "596fa05d",
      "metadata": {
        "id": "596fa05d"
      },
      "source": [
        "Change since last value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccb6f5c9",
      "metadata": {
        "id": "ccb6f5c9"
      },
      "outputs": [],
      "source": [
        "data_water['raw_ro_change'] = data_water['raw_ro'].diff()\n",
        "data_water['ra_rain_change'] = data_water['ra_rain'].diff()\n",
        "\n",
        "data_water['raw_ro_rollmean_2_change'] = data_water['raw_ro_rollmean_2'].diff()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Revert index\n",
        "# (adjusted for GPU)\n",
        "\n",
        "data_water_reset = data_water.reset_index()\n",
        "index_col_name = data_water_reset.columns[0]\n",
        "indices_df = original_indices.to_frame(name=index_col_name)\n",
        "\n",
        "filtered_data_water = cudf.merge(\n",
        "    data_water_reset,\n",
        "    indices_df,\n",
        "    on=index_col_name,\n",
        "    how='inner'\n",
        ")\n",
        "data_water = filtered_data_water.set_index(index_col_name)\n",
        "\n",
        "del original_indices, filtered_data_water, index_col_name, data_water_reset, indices_df"
      ],
      "metadata": {
        "id": "MH0ZMuxQGFwO"
      },
      "id": "MH0ZMuxQGFwO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0d8524e",
      "metadata": {
        "id": "b0d8524e"
      },
      "outputs": [],
      "source": [
        "# # Return\n",
        "# data_water = data_water.loc[original_indices]\n",
        "\n",
        "# del original_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4794b582",
      "metadata": {
        "id": "4794b582"
      },
      "source": [
        "## Soil"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f8e3f87",
      "metadata": {
        "id": "5f8e3f87"
      },
      "source": [
        "Pivot the soil data such that each sample has its own columns, and separated by depth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd40d185",
      "metadata": {
        "id": "fd40d185"
      },
      "outputs": [],
      "source": [
        "# Drop irrelevant column\n",
        "data_soil_shallow = united_soil.copy().drop('h2o_by_wet_deep', axis=1)\n",
        "data_soil_shallow['sample'] = data_soil_shallow['sample'].astype('float32')\n",
        "# Pivot wider\n",
        "data_soil_shallow = data_soil_shallow.pivot(columns='sample', values='h2o_by_wet_shallow')\n",
        "\n",
        "# Drop irrelevant column\n",
        "data_soil_deep = united_soil.copy().drop('h2o_by_wet_shallow', axis=1)\n",
        "\n",
        "data_soil_deep['sample'] = data_soil_deep['sample'].astype('float32')\n",
        "# Pivot wider\n",
        "data_soil_deep = data_soil_deep.pivot(columns='sample', values='h2o_by_wet_deep')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21b91e64",
      "metadata": {
        "id": "21b91e64"
      },
      "outputs": [],
      "source": [
        "data_soil = pd.merge(\n",
        "    data_soil_shallow,\n",
        "    data_soil_deep,\n",
        "    left_index = True,\n",
        "    right_index = True,\n",
        "    suffixes = (\"_shallow\", \"_deep\"),\n",
        "    how = \"outer\"\n",
        ")\n",
        "\n",
        "del data_soil_shallow, data_soil_deep\n",
        "del united_soil"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47024efe",
      "metadata": {
        "id": "47024efe"
      },
      "source": [
        "## Unite\n",
        "\n",
        "Join the data frames to prep for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a80c556f",
      "metadata": {
        "id": "a80c556f"
      },
      "outputs": [],
      "source": [
        "data_united = pd.merge(\n",
        "    data_water,\n",
        "    #\n",
        "    data_cal[DATE_START:DATE_END],\n",
        "    #\n",
        "    left_index = True,\n",
        "    right_index = True,\n",
        "    how = 'outer'\n",
        ")\n",
        "\n",
        "data_united = pd.merge(\n",
        "    data_united,\n",
        "    #\n",
        "    data_soil[DATE_START:DATE_END],\n",
        "    #\n",
        "    left_index = True,\n",
        "    right_index = True,\n",
        "    how = 'outer'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f814e925",
      "metadata": {
        "id": "f814e925"
      },
      "source": [
        "### United features\n",
        "\n",
        "Add a few final features on the united data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90f96840",
      "metadata": {
        "id": "90f96840"
      },
      "outputs": [],
      "source": [
        "# Difference compared to calibration point (infrequent)\n",
        "data_united['diff_ro_cal'] = (data_united['weir_level_cal'] - data_united['raw_ro'])\n",
        "\n",
        "# Convert to float\n",
        "data_united['diff_ro_cal'] = data_united['diff_ro_cal'].astype(np.float32)\n",
        "\n",
        "# Time since last calibration point\n",
        "data_united = timesince_feat(data_united, 'weir_level_cal', \"minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a4e0dc4",
      "metadata": {
        "id": "0a4e0dc4"
      },
      "outputs": [],
      "source": [
        "# create features to track soil value staleness\n",
        "cols_soil = [col for col in data_united.columns if (col.endswith('shallow') | col.endswith('deep'))]\n",
        "\n",
        "for col in cols_soil:\n",
        "# for col in data_united.columns:\n",
        "    # if (col.endswith('shallow') | col.endswith('deep')):\n",
        "    # data_united = minsince_feat(data_united, col)\n",
        "    data_united = timesince_feat(data_united, col, \"days\")\n",
        "\n",
        "# Extend soil vals\n",
        "data_united[cols_soil] = data_united[cols_soil].ffill()\n",
        "\n",
        "del col, cols_soil"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1621fcb7",
      "metadata": {
        "id": "1621fcb7"
      },
      "source": [
        "### Temporal features\n",
        "Modify temporal features to be based on sine and cosine transformations, which allows for the model to be based on the cyclical patterns of time rather than abrupt distances\n",
        "\n",
        "(e.g., the raw values Day 365 of the year is 'far' from Day 001, but in reality they are very near)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfbcf31a",
      "metadata": {
        "id": "dfbcf31a"
      },
      "outputs": [],
      "source": [
        "def temporal_feat(input_df, input_unit):\n",
        "    output_df = input_df\n",
        "    if input_unit=='day':\n",
        "        cycle_length = 365.25\n",
        "        value = output_df.index.dayofyear\n",
        "    elif input_unit=='month':\n",
        "        cycle_length = 12\n",
        "        value = output_df.index.month\n",
        "    elif input_unit=='hour':\n",
        "        cycle_length = 24\n",
        "        value = output_df.index.hour\n",
        "    elif input_unit=='minute':\n",
        "        cycle_length = 60\n",
        "        value = output_df.index.minute\n",
        "\n",
        "    output_df[f'{input_unit}_sin'] = np.sin(2 * np.pi * value / cycle_length).astype(np.float32)\n",
        "    output_df[f'{input_unit}_cos'] = np.cos(2 * np.pi * value / cycle_length).astype(np.float32)\n",
        "\n",
        "    return output_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72593493",
      "metadata": {
        "id": "72593493"
      },
      "outputs": [],
      "source": [
        "data_united = temporal_feat(data_united, 'minute')\n",
        "data_united = temporal_feat(data_united, 'hour')\n",
        "data_united = temporal_feat(data_united, 'day')\n",
        "data_united = temporal_feat(data_united, 'month')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd20a0d9",
      "metadata": {
        "id": "bd20a0d9"
      },
      "source": [
        "## Train/Test split\n",
        "\n",
        "80/20 initial split, with expanding sliding window for training/validation for hyperparameters, model stability, and feature selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1741c777",
      "metadata": {
        "id": "1741c777"
      },
      "outputs": [],
      "source": [
        "# REMOVE NAs\n",
        "data_united = data_united.dropna(subset=[VAR_OF_INTEREST])\n",
        "\n",
        "X_all = data_united.drop(VAR_OF_INTEREST, axis=1).copy()\n",
        "y_all = data_united[VAR_OF_INTEREST].copy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Random data type fixes for GPU usage\n",
        "\n",
        "# Fix for inferred later\n",
        "# y_all = y_all.astype(bool)\n",
        "y_all = y_all.astype(np.float32)\n",
        "y_all = y_all.as_gpu_object()\n",
        "\n",
        "for col in X_all.columns:\n",
        "  if str(X_all[col].dtype) == ('Int32'):\n",
        "    X_all[col] = X_all[col].astype(np.float32)\n",
        "\n",
        "# print(y_all.__class__)\n",
        "# y_all = cudf.Series.from_pandas(y_all)\n",
        "print(y_all.__class__)\n",
        "X_all.info()"
      ],
      "metadata": {
        "id": "X67HeQIcaYxM"
      },
      "id": "X67HeQIcaYxM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df5ab1ce",
      "metadata": {
        "id": "df5ab1ce"
      },
      "outputs": [],
      "source": [
        "y_len = len(y_all)\n",
        "\n",
        "print(\n",
        "    y_len, \"\\n\",\n",
        "    (round(.2*y_len) + round(.8*y_len)),\n",
        "    \"\\nTrain:\\t80p of \", y_len, \" is \", round(.8*y_len),\n",
        "    \"\\nTest:\\t20p of \", y_len, \" is \", round(.2*y_len),\n",
        "    sep=\"\"\n",
        ")\n",
        "\n",
        "del y_len"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7daf36fd",
      "metadata": {
        "id": "7daf36fd"
      },
      "source": [
        "Unlike the typical approach for train/test splits, temporal data in this context must _not_ be randomly split as it would lead to severe leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e023cfb",
      "metadata": {
        "id": "7e023cfb"
      },
      "outputs": [],
      "source": [
        "# Conduct the split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.2, shuffle=False)\n",
        "# Conduct an inner split for tuning\n",
        "# X_train_inner, X_test_inner, y_train_inner, y_test_inner = train_test_split(X_train, y_train, test_size = 0.2, shuffle=False)\n",
        "\n",
        "# Cleanup\n",
        "del X_all, y_all\n",
        "\n",
        "print(\n",
        "    \"Train:\\t\", len(X_train), \"\\t\", X_train.index[0], \"thru\", X_train.index[-1],\n",
        "    \"\\nTest:\\t\", len(X_test), \"\\t\", X_test.index[0], \"thru\", X_test.index[-1]\n",
        "    # len(x_train), len(x_test), \"\\n\",\n",
        "    # x_train.index[-1]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11921d04",
      "metadata": {
        "id": "11921d04"
      },
      "source": [
        "### Expanding Window\n",
        "\n",
        "For tuning, an expanding window approach will be used. This is similar to how a model would act once deployed, as it will only gain more data over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "661eeac5",
      "metadata": {
        "id": "661eeac5"
      },
      "outputs": [],
      "source": [
        "# Initialize the split function\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "# print(tscv)\n",
        "\n",
        "for i, (train_index, val_index) in enumerate(tscv.split(X_train)):\n",
        "    print(f\"Fold {i}:\")\n",
        "    print(f\"  Train: index={train_index}\")\n",
        "    print(f\"  Test:  index={val_index}\")\n",
        "    # print(\"  Train: index=\", mini_x.index[train_index])\n",
        "    # print(f\"  Test:  index={val_index}\")\n",
        "    print(\"------------------------------------------------------------\")\n",
        "\n",
        "del i, train_index, val_index"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter selection\n",
        "\n",
        "Highly correlated features can be removed prior to model selection.\n",
        "So long as the _same_ features are dropped in the test set as well, this will not result in data leakage.\n",
        "\n",
        "This takes a bit of time, so will load any cached list from a prior run."
      ],
      "metadata": {
        "id": "cxGzUYnhpuHW"
      },
      "id": "cxGzUYnhpuHW"
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists('to_drop.pkl') == True:\n",
        "  print(\"Vars to drop previously cached...\")\n",
        "  with open('to_drop.pkl', 'rb') as file:\n",
        "    to_drop = pickle.load(file)\n",
        "else:\n",
        "  print(\"Finding corr vars...\")\n",
        "\n",
        "  X_train_corrblock = X_train.copy() # Backup\n",
        "\n",
        "  corr = X_train_corrblock.corr().abs()\n",
        "  # corr = np.corrcoef(X_train_corrblock.values, rowvar=False)\n",
        "  upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
        "\n",
        "  to_drop = [col for col in upper.columns if any(upper[col] > CORR_CUTOFF)]\n",
        "\n",
        "  with open('to_drop.pkl', \"wb\") as fp:\n",
        "    pickle.dump(to_drop, fp)\n",
        "    # print(\"Importing model from saved files...\")\n",
        "    # final_model = joblib.load(model_path(model_name))\n",
        "  del X_train_corrblock\n",
        "\n",
        "print(to_drop)"
      ],
      "metadata": {
        "id": "Lk_uyovMcNPW"
      },
      "id": "Lk_uyovMcNPW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_backup = X_train.copy()\n",
        "X_train = X_train.drop(columns=to_drop)\n",
        "\n",
        "X_test_backup = X_test.copy()\n",
        "X_test = X_test.drop(columns=to_drop)"
      ],
      "metadata": {
        "id": "4b7FHR4ouAlW"
      },
      "id": "4b7FHR4ouAlW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9c343fa3",
      "metadata": {
        "id": "9c343fa3"
      },
      "source": [
        "## Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a8a3293",
      "metadata": {
        "id": "0a8a3293"
      },
      "source": [
        "As per XGBoosting documentation/tutorials, early stopping with random search for hyperparameter tuning must be iterated upon manually, as `RandomizedSearchCV` does not support using a separate validation set within each CV fold.\n",
        "\n",
        "Source: https://xgboosting.com/xgboost-early-stopping-with-random-search/\n",
        "\n",
        "\n",
        "The area under the precision-recall curve (AUC-PR) can be used to evaluate the performance, since it considers a range of classification thresholds.\n",
        "This is better than an ROC AUC metric since there is greater class imbalance (i.e., `True` is more rare).\n",
        "\n",
        "Source: https://xgboosting.com/evaluate-xgboost-performance-with-precision-recall-curve/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "351db094",
      "metadata": {
        "id": "351db094"
      },
      "outputs": [],
      "source": [
        "# Code modified from\n",
        "# https://xgboosting.com/xgboost-early-stopping-with-random-search/\n",
        "\n",
        "# Define hyperparameter distributions for random search\n",
        "param_distributions = {\n",
        "    'learning_rate': ('uniform', 0.01, 0.3),\n",
        "    'max_depth': ('choice', [2, 3, 4, 5, 6]),\n",
        "    'subsample': ('uniform', 0.5, 1.0),\n",
        "    'colsample_bytree': ('uniform', 0.4, 1.0),\n",
        "    'scale_pos_weight':('choice', [5, 7, 9, 10, 11]),\n",
        "    'gamma': ('uniform', 0, 0.5),\n",
        "    'reg_alpha': ('uniform', 0, 1.0)\n",
        "}\n",
        "\n",
        "# Define seed again\n",
        "rng = np.random.default_rng(SEED)\n",
        "\n",
        "# Function to sample parameters based on their distribution type\n",
        "def sample_param(distribution):\n",
        "    if distribution[0] == 'uniform':\n",
        "        # Use seeded generator to create the scipy distribution\n",
        "        return uniform(loc=distribution[1], scale=distribution[2] - distribution[1]).rvs(random_state=rng)\n",
        "    elif distribution[0] == 'choice':\n",
        "        # Use seeded generator choice method\n",
        "        return rng.choice(distribution[1])\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported distribution type: {distribution[0]}\")\n",
        "\n",
        "best_params = None\n",
        "best_score = 0\n",
        "best_avg_rounds = 0\n",
        "\n",
        "for _ in range(N_ITERATIONS):\n",
        "    test_scores = []\n",
        "    best_rounds = []\n",
        "    optimal_rounds_list = []\n",
        "    params = {k: sample_param(v) for k, v in param_distributions.items()}\n",
        "\n",
        "    for train_index, test_index in tscv.split(X_train):\n",
        "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
        "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
        "\n",
        "        # Split train set into train and validation\n",
        "        X_train_fold, X_val, y_train_fold, y_val = train_test_split(X_train_fold, y_train_fold, test_size=0.2, shuffle=False)\n",
        "\n",
        "        # # Prep hyperparam\n",
        "        # neg_count_fold = (y_train_fold == 0).sum()\n",
        "        # pos_count_fold = (y_train_fold == 1).sum()\n",
        "\n",
        "        # Prepare the model\n",
        "        model = xgb.XGBClassifier(\n",
        "            n_estimators=N_ESTIMATORS,\n",
        "            learning_rate=params['learning_rate'],\n",
        "            max_depth=int(params['max_depth']),  # max_depth should be an int\n",
        "            subsample=params['subsample'],\n",
        "            colsample_bytree=params['colsample_bytree'],\n",
        "            objective='binary:logistic',\n",
        "            gamma=params['gamma'],\n",
        "            reg_alpha=params['reg_alpha'],\n",
        "            scale_pos_weight=params['scale_pos_weight'],\n",
        "            ## SETTINGS FOR GPU\n",
        "            seed_per_iteration = True,\n",
        "            tree_method='hist',\n",
        "            device='cuda',\n",
        "            # scale_pos_weight= neg_count_fold/pos_count_fold,\n",
        "            ##\n",
        "            # Custom eval metric: AUC-PR\n",
        "            eval_metric='aucpr',\n",
        "            random_state=SEED,\n",
        "            n_jobs=-1,\n",
        "            early_stopping_rounds=EARLY_STOPPING_ROUNDS # fixed early stopping\n",
        "        )\n",
        "\n",
        "        # Fit model on train fold and use validation for early stopping\n",
        "        model.fit(X_train_fold, y_train_fold, eval_set=[(X_val, y_val)], verbose=False)\n",
        "\n",
        "        # Find optimal number of iterations\n",
        "        optimal_rounds_list.append(model.best_iteration)\n",
        "\n",
        "        # # Predict on test set\n",
        "        # ## Using F1\n",
        "        # y_pred_test = model.predict(X_test_fold)\n",
        "        # # test_score = accuracy_score(y_test_fold, y_pred_test)\n",
        "        # # test_score = f1_score(y_test_fold, y_pred_test)\n",
        "        # test_score = f1_score(y_test_fold.to_cupy().get(), y_pred_test)\n",
        "        # test_scores.append(test_score)\n",
        "\n",
        "        # ## Using ROC AUC\n",
        "        # y_pred_test = model.predict_proba(X_test_fold)[:,1]\n",
        "        # # y_pred_test = y_pred_test.to_cupy()\n",
        "        # # y_pred_test = cp.array(y_pred_test)\n",
        "        # y_pred_test = cudf.Series(y_pred_test)\n",
        "        # # test_score = roc_auc_score(y_test_fold, y_pred_test)\n",
        "        # test_score = cuml.metrics.roc_auc_score(y_test_fold, y_pred_test)\n",
        "        # test_scores.append(test_score)\n",
        "        # ##\n",
        "\n",
        "        ## Using AUC PR\n",
        "        # Predict on test set\n",
        "        y_pred_test = model.predict_proba(X_test_fold)[:,1]\n",
        "        # test_score = accuracy_score(y_test_fold, y_pred_test)\n",
        "        # test_score = f1_score(y_test_fold, y_pred_test)\n",
        "        y_prec, y_rec, _ = precision_recall_curve(y_test_fold.to_cupy().get(), y_pred_test)\n",
        "        test_score = auc(y_rec, y_prec)\n",
        "        test_scores.append(test_score)\n",
        "\n",
        "    # Compute average score across all folds\n",
        "    average_score = np.mean(test_scores)\n",
        "    average_optimal_rounds = np.mean(optimal_rounds_list)\n",
        "\n",
        "    if average_score > best_score:\n",
        "        best_score = average_score\n",
        "        best_params = params\n",
        "        best_avg_rounds = int(round(average_optimal_rounds)) # Store the integer average\n",
        "        ## Maybe??\n",
        "        # best_f1 = test_score\n",
        "        # best_model = model\n",
        "\n",
        "\n",
        "# print(f\"Best Parameters: {best_params}\")\n",
        "print(\"Best Parameters:\")\n",
        "# aligned printing code from stack overflow,\n",
        "# https://stackoverflow.com/a/54573735/23486987\n",
        "for key, value in best_params.items():\n",
        "    print(f'{key:20}{value}')\n",
        "\n",
        "# print(f\"Best CV Average Accuracy: {best_score}\")\n",
        "# print(f\"CV Average ROC AUC: {best_score:.4f}\")\n",
        "print(f\"CV Average: {average_score:.4f}\")\n",
        "print(f\"Best Avg Rounds: {best_avg_rounds}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5f41280",
      "metadata": {
        "id": "e5f41280"
      },
      "outputs": [],
      "source": [
        "play_chime()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del X_train_fold, X_val, y_train_fold, y_val\n",
        "del y_pred_test\n",
        "del test_scores, best_rounds, optimal_rounds_list\n",
        "# del n_splits, early_stopping_rounds, n_iterations, test_scores, best_rounds, optimal_rounds_list\n",
        "del average_score, average_optimal_rounds\n",
        "del y_prec, y_rec"
      ],
      "metadata": {
        "id": "EE6EUGQHjU-3"
      },
      "id": "EE6EUGQHjU-3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model"
      ],
      "metadata": {
        "id": "LCwADUVwdHvc"
      },
      "id": "LCwADUVwdHvc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4cd5517",
      "metadata": {
        "id": "e4cd5517"
      },
      "outputs": [],
      "source": [
        "# model_name = MODEL_NAME\n",
        "\n",
        "if os.path.exists(model_path(MODEL_NAME)) == False:\n",
        "    print(\"Creating final model...\")\n",
        "\n",
        "    # # Prep hyperparam\n",
        "    # neg_count_it = (y_train_inner == 0).sum()\n",
        "    # pos_count_it = (y_train_inner == 1).sum()\n",
        "\n",
        "    final_model = xgb.XGBClassifier(\n",
        "        n_estimators=best_avg_rounds,\n",
        "        # n_estimators=100, # avg optimal n_estimators if known, or reasonable default\n",
        "        random_state=SEED,\n",
        "        learning_rate=best_params['learning_rate'],\n",
        "        max_depth=int(best_params['max_depth']),\n",
        "        subsample=best_params['subsample'],\n",
        "        colsample_bytree=best_params['colsample_bytree'],\n",
        "        objective='binary:logistic',\n",
        "        scale_pos_weight=best_params['scale_pos_weight'],\n",
        "        gamma=best_params['gamma'],\n",
        "        reg_alpha=best_params['reg_alpha'],\n",
        "        ## SETTINGS FOR GPU\n",
        "        seed_per_iteration = True,\n",
        "        tree_method='hist',\n",
        "        device='cuda',\n",
        "        ##\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Do not fit until OOF tuning has been conducted\n",
        "    # final_model.fit(X_train_inner, y_train_inner)\n",
        "    joblib.dump(final_model, model_path(MODEL_NAME))\n",
        "\n",
        "    # Local download\n",
        "    from google.colab import files\n",
        "    files.download(model_path(MODEL_NAME))\n",
        "\n",
        "else:\n",
        "    print(\"Importing model from saved files...\")\n",
        "    final_model = joblib.load(model_path(MODEL_NAME))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OOF Predictions\n",
        "\n",
        "Out-of-fold predictions will be used to tune smoothing and thresholding parameters. To do this, the model with tuned hyperparameters will predict each sliding window set from before. This reflects the real-world performance of the model as it can fit to more data."
      ],
      "metadata": {
        "id": "g1eAAvS2Zup3"
      },
      "id": "g1eAAvS2Zup3"
    },
    {
      "cell_type": "code",
      "source": [
        "oof_pred = np.full(len(y_train), np.nan)\n",
        "\n",
        "for train_index, test_index in tscv.split(X_train):\n",
        "  X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
        "  y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
        "\n",
        "  # Prepare the model\n",
        "  model = clone(final_model)\n",
        "\n",
        "  model.fit(X_train_fold, y_train_fold)\n",
        "  y_pred_test = model.predict_proba(X_test_fold)[:,1]\n",
        "  oof_pred[test_index] = y_pred_test\n",
        "\n",
        "# Isolate the OOF truth values\n",
        "# because the first training set is not included\n",
        "valid_mask = ~np.isnan(oof_pred)\n",
        "y_train_oof = y_train.values[valid_mask]\n",
        "oof_pred = oof_pred[valid_mask]\n",
        "\n",
        "del X_train_fold, y_train_fold, X_test_fold, y_test_fold, y_pred_test\n",
        "del model, valid_mask"
      ],
      "metadata": {
        "id": "5EULm8GkZuTU"
      },
      "id": "5EULm8GkZuTU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Smoothing & Thresholding\n",
        "\n",
        "Windowing can help smooth predictions by preventing standalone points that differ from their neighbors (e.g., having a sequence of `True` interrupted by one `False`, or vice-versa, both of which are unlikely in this context due to how weir blockages occur).\n",
        "\n",
        "By default, a threshold of 0.5 will be selected for categorizing a point as `True` or `False`. However, in this context a model more sensitive to `True` may make final results more accurate.\n",
        "\n",
        "These measures can be found by finding the window and corresponding threshold that maximizes the F1 score."
      ],
      "metadata": {
        "id": "RRLWFSfdg4ph"
      },
      "id": "RRLWFSfdg4ph"
    },
    {
      "cell_type": "code",
      "source": [
        "# def smooth_window(probs, window_size=3):\n",
        "#     return pd.Series(probs).rolling(window=window_size, min_periods=1, center=True).mean().values"
      ],
      "metadata": {
        "id": "MALjzPT9g_we"
      },
      "id": "MALjzPT9g_we",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_cust(input_true, input_pred):\n",
        "  \"\"\"Report the F1 score using inputs that might be mixed type.\n",
        "\n",
        "    Args:\n",
        "        input_true (np.ndarray or cp.ndarray): Array of true y values.\n",
        "        input_parent (np.ndarray or cp.ndarray): Array of predicted y values.\n",
        "\n",
        "    Returns:\n",
        "        float: F1 score.\n",
        "    \"\"\"\n",
        "  output_true = input_true.copy()\n",
        "  output_pred = input_pred.copy()\n",
        "  if not isinstance(output_true, np.ndarray):\n",
        "    output_true = output_true.get()\n",
        "  if not isinstance(output_pred, np.ndarray):\n",
        "    output_pred = output_pred.get()\n",
        "\n",
        "  output_f1 = f1_score(output_true, output_pred)\n",
        "  # prec, rec, _ = precision_recall_curve(output_true, output_pred)\n",
        "  # output_f1 = auc(rec, prec)\n",
        "  return output_f1"
      ],
      "metadata": {
        "id": "AOb3QdMGwT0o"
      },
      "id": "AOb3QdMGwT0o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start w no smoothing\n",
        "best_window_size = 1\n",
        "# Start w default threshold\n",
        "threshold = 0.5\n",
        "\n",
        "y_bin = (oof_pred >= threshold).astype(np.int32)\n",
        "# y_bin = cp.array(y_bin)\n",
        "best_f1 = f1_cust(y_train_oof, y_bin)\n",
        "# print(f\"No smoothing, 0.5 threshold:\\t{best_f1:.4f}\")\n",
        "\n",
        "best_threshold = threshold\n",
        "thresholds = np.linspace(0.01, 0.99, 100)\n",
        "\n",
        "# # Smallest window size\n",
        "# window_min = 1\n",
        "# # Largest window size\n",
        "# window_max = 35\n",
        "\n",
        "print(f\"Window 1\\tCurr best->\\5W:1\\tT:{best_threshold}\\t\\tF1:{best_f1:.4}\")\n",
        "\n",
        "# Test range of odd window sizes\n",
        "for current_window in range(WINDOW_MIN, WINDOW_MAX+1, 2):\n",
        "    # print(\"Testing window =\", current_window)\n",
        "    # smoothed_preds = smooth_window(oof_pred, window_size=current_window)\n",
        "    smoothed_preds = medfilt(oof_pred, kernel_size=current_window)\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        y_bin = (smoothed_preds >= threshold).astype(np.int32)\n",
        "        current_f1 = f1_cust(y_train_oof, y_bin)\n",
        "        # print(f\"{threshold:.4}\\t{current_f1:.5}\")\n",
        "\n",
        "        if current_f1 > best_f1:\n",
        "          best_f1 = current_f1\n",
        "          # print(f\"New best F1:{best_f1:.4}\")\n",
        "          best_window = current_window\n",
        "          best_threshold = threshold\n",
        "    # print(f\"Current best F1:{best_f1:.4}\")\n",
        "    print(f\"Window {current_window}\\tCurr best->\\tW:{best_window}\\tT:{best_threshold:.4}\\tF1:{best_f1:.4}\")\n",
        "    # print(f\"Window {current_window}\\tCurr best--\\tW:{best_window}\\tT:{best_threshold:.4}\\tF1:{best_f1:.4}\")\n",
        "\n",
        "# oof_pred_adj = smooth_window(oof_pred, window_size=best_window)\n",
        "oof_pred_adj = medfilt(oof_pred, kernel_size=best_window)\n",
        "oof_pred_adj = (oof_pred_adj >= best_threshold).astype(int)\n",
        "\n",
        "f1_rez = f1_cust(y_train_oof, oof_pred_adj)\n",
        "# print(\"----------Final results----------\")\n",
        "print(f\"Final results----->\\t\\tW:{best_window}\\tT:{best_threshold:.4f}\\tF1:{f1_rez:.4}\")\n",
        "# print(f\"Final results:\\t\\tWindow: {best_window},\\tThreshold: {best_threshold:.4f}\\tF1:{f1_rez:.4f}\")\n",
        "\n",
        "# del current_f1, current_window, threshold, f1_rez, window_min, window_max, y_bin, best_f1\n",
        "del current_f1, current_window, threshold, f1_rez, y_bin, best_f1"
      ],
      "metadata": {
        "id": "dOOgqbxMbQ4N"
      },
      "id": "dOOgqbxMbQ4N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9IqsRtK8yhim"
      },
      "id": "9IqsRtK8yhim"
    },
    {
      "cell_type": "code",
      "source": [
        "play_chime()"
      ],
      "metadata": {
        "id": "KSHxChI1jpt0"
      },
      "id": "KSHxChI1jpt0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# # Start w no smoothing\n",
        "# best_window_size = 1\n",
        "# # Start w default threshold\n",
        "# threshold = 0.5\n",
        "\n",
        "# y_bin = (oof_pred >= threshold).astype(np.int32)\n",
        "# # y_bin = cp.array(y_bin)\n",
        "# best_f1 = f1_cust(y_train_oof, y_bin)\n",
        "# # print(f\"No smoothing, 0.5 threshold:\\t{best_f1:.4f}\")\n",
        "\n",
        "# best_threshold = threshold\n",
        "# thresholds = np.linspace(0.01, 0.99, 100)\n",
        "# # thresholds = np.linspace(0.01, 0.99, 10)\n",
        "\n",
        "# # Smallest window size\n",
        "# window_min = 1\n",
        "# # Largest window size\n",
        "# window_max = 35\n",
        "# # window_max = 10\n",
        "\n",
        "# print(f\"Window 1\\tCurr best: W:1\\tT:{best_threshold}\\t\\tF1:{best_f1:.4}\")\n",
        "\n",
        "# # Test range of odd window sizes\n",
        "# for current_window in range(window_min, window_max+1, 2):\n",
        "#     # print(\"Testing window =\", current_window)\n",
        "#     smoothed_preds = smooth_window(oof_pred, window_size=current_window)\n",
        "\n",
        "#     for threshold in thresholds:\n",
        "#         y_bin = (smoothed_preds >= threshold).astype(np.int32)\n",
        "#         current_f1 = f1_cust(y_train_oof, y_bin)\n",
        "#         # print(f\"{threshold:.4}\\t{current_f1:.5}\")\n",
        "\n",
        "#         if current_f1 > best_f1:\n",
        "#           best_f1 = current_f1\n",
        "#           # print(f\"New best F1:{best_f1:.4}\")\n",
        "#           best_window = current_window\n",
        "#           best_threshold = threshold\n",
        "#     # print(f\"Current best F1:{best_f1:.4}\")\n",
        "#     print(f\"Window {current_window}\\tCurr best: W:{best_window}\\tT:{best_threshold:.4}\\tF1:{best_f1:.4}\")\n",
        "\n",
        "# oof_pred_adj = smooth_window(oof_pred, window_size=best_window)\n",
        "# oof_pred_adj = (oof_pred_adj >= best_threshold).astype(int)\n",
        "\n",
        "# f1_rez = f1_cust(y_train_oof, oof_pred_adj)\n",
        "# print(f\"Window {best_window}, threshold {best_threshold:.4f}:\\t{f1_rez:.4f}\")\n",
        "\n",
        "# del current_f1, current_window, threshold, f1_rez, window_min, window_max, y_bin, best_f1"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nzBsaP7chJaL"
      },
      "id": "nzBsaP7chJaL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying to test set"
      ],
      "metadata": {
        "id": "YIouoQwgij8-"
      },
      "id": "YIouoQwgij8-"
    },
    {
      "cell_type": "code",
      "source": [
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "final_pred_y = final_model.predict_proba(X_test)[:,1]\n",
        "\n",
        "final_pred_y_base = (final_pred_y >= 0.5).astype(np.int32)\n",
        "\n",
        "y_test_conv = y_test.copy().to_cupy()\n",
        "y_test_conv = y_test_conv.get()\n",
        "\n",
        "\n",
        "print(\"Base model (0.5 threshold, no smoothing)\")\n",
        "\n",
        "print(\n",
        "    f\"F1:\\t{f1_score(y_test_conv, final_pred_y_base):.4f}\",\n",
        "    f\"Acc:\\t{accuracy_score(y_test_conv, final_pred_y_base):.4f}\",\n",
        "    f\"Pre:\\t{precision_score(y_test_conv, final_pred_y_base):.4f}\",\n",
        "    f\"Rec:\\t{recall_score(y_test_conv, final_pred_y_base):.4f}\",\n",
        "    \"-----------------------------------\",\n",
        "    sep=\"\\n\"\n",
        ")\n",
        "\n",
        "# t_x_pred_f = t_tuner.predict(X_test)\n",
        "print(\"Windowed with\", best_window)\n",
        "\n",
        "final_pred_y_win = medfilt(final_pred_y, kernel_size=best_window)\n",
        "# final_pred_y_win = smooth_window(final_pred_y, window_size=best_window)\n",
        "final_pred_y_win = (final_pred_y_win >= 0.5).astype(np.int32)\n",
        "\n",
        "print(\n",
        "    f\"F1:\\t{f1_score(y_test_conv, final_pred_y_win):.4f}\",\n",
        "    f\"Acc:\\t{accuracy_score(y_test_conv, final_pred_y_win):.4f}\",\n",
        "    f\"Pre:\\t{precision_score(y_test_conv, final_pred_y_win):.4f}\",\n",
        "    f\"Rec:\\t{recall_score(y_test_conv, final_pred_y_win):.4f}\",\n",
        "    \"-----------------------------------\",\n",
        "    sep=\"\\n\"\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"Windowed & optmized threshold of {best_threshold:.4f}\")\n",
        "\n",
        "# final_pred_y = (final_pred_y >= best_threshold).astype(np.int32)\n",
        "# final_pred_y_opt = smooth_window(final_pred_y, window_size=best_window)\n",
        "final_pred_y_opt = medfilt(final_pred_y, kernel_size=best_window)\n",
        "final_pred_y_opt = (final_pred_y_opt >= best_threshold).astype(np.int32)\n",
        "\n",
        "print(\n",
        "    f\"F1:\\t{f1_score(y_test_conv, final_pred_y_opt):.4f}\",\n",
        "    f\"Acc:\\t{accuracy_score(y_test_conv, final_pred_y_opt):.4f}\",\n",
        "    f\"Pre:\\t{precision_score(y_test_conv, final_pred_y_opt):.4f}\",\n",
        "    f\"Rec:\\t{recall_score(y_test_conv, final_pred_y_opt):.4f}\",\n",
        "    \"-----------------------------------\",\n",
        "    sep=\"\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "kO7qTt_m1IuZ"
      },
      "id": "kO7qTt_m1IuZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_chime()"
      ],
      "metadata": {
        "id": "8JC3K93z368_"
      },
      "id": "8JC3K93z368_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save results"
      ],
      "metadata": {
        "id": "JJ7_8VdnkD2E"
      },
      "id": "JJ7_8VdnkD2E"
    },
    {
      "cell_type": "code",
      "source": [
        "# p_proba, p_none, p_w, p_t, p_wt = apply_model(X_test, final_model, best_window, best_threshold, return_all=True)\n",
        "\n",
        "# print(\n",
        "#     \"F1\\tAcc\\tPre\\tRec\",\n",
        "#     report_scores(y_test_conv, p_none, 4),\n",
        "#     report_scores(y_test_conv, p_w, 4),\n",
        "#     report_scores(y_test_conv, p_t, 4),\n",
        "#     report_scores(y_test_conv, p_wt, 4),\n",
        "#     sep=\"\\n\"\n",
        "# )\n",
        "\n",
        "for item in apply_model(X_test, final_model, best_window, best_threshold, \"all\"):\n",
        "  print(report_scores(y_test_conv, item, 4))"
      ],
      "metadata": {
        "id": "EWsuYMHF73gJ"
      },
      "id": "EWsuYMHF73gJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fitted_model_name = \"xgb_full_fitted\"\n",
        "\n",
        "if os.path.exists(model_path(FITTED_MODEL_NAME)) == False:\n",
        "    print(\"Saving final model...\")\n",
        "\n",
        "    joblib.dump(final_model, model_path(FITTED_MODEL_NAME))\n",
        "\n",
        "    # Local download\n",
        "    from google.colab import files\n",
        "    files.download(model_path(FITTED_MODEL_NAME))"
      ],
      "metadata": {
        "id": "--1oS7ossyeD"
      },
      "id": "--1oS7ossyeD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report_scores(y_test_conv, apply_model(X_test, final_model, best_window, best_threshold, \"adj\"), 4)"
      ],
      "metadata": {
        "id": "h7UYFuKTAZnk"
      },
      "id": "h7UYFuKTAZnk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_model(input_set, input_model, input_window, input_threshold, input_return=\"adj\"):\n",
        "\n",
        "  pred_y_proba = input_model.predict_proba(input_set)[:,1]\n",
        "\n",
        "  if input_return == \"proba\":\n",
        "    return pred_y_proba\n",
        "\n",
        "  pred_y_wt = medfilt(pred_y_proba, kernel_size = input_window)\n",
        "  pred_y_wt = (pred_y_wt >= input_threshold).astype(np.int32)\n",
        "\n",
        "  if input_return==\"adj\":\n",
        "    return pred_y_wt\n",
        "\n",
        "  if input_return==\"all\":\n",
        "\n",
        "    # No window, default threshold\n",
        "    pred_y_none = (pred_y_proba >= 0.5).astype(np.int32)\n",
        "\n",
        "    # Best window, default threshold\n",
        "    pred_y_w = medfilt(pred_y_proba, kernel_size=input_window)\n",
        "    pred_y_w = (pred_y_w >= 0.5).astype(np.int32)\n",
        "\n",
        "    # No window, best threshold\n",
        "    pred_y_t = (pred_y_proba >= input_threshold).astype(np.int32)\n",
        "\n",
        "    # # Best window, best threshold\n",
        "    # pred_y_wt = medfilt(pred_y_proba, kernel_size = input_window)\n",
        "    # pred_y_wt = (pred_y_wt >= input_threshold).astype(np.int32)\n",
        "    return pred_y_none, pred_y_w, pred_y_t, pred_y_wt\n",
        "\n",
        "\n",
        "def report_scores(input_true, input_pred, input_round=None):\n",
        "  output_f1 = f1_score(input_true, input_pred)\n",
        "  output_acc = accuracy_score(input_true, input_pred)\n",
        "  output_pre = precision_score(input_true, input_pred)\n",
        "  output_rec = recall_score(input_true, input_pred)\n",
        "\n",
        "  if input_round is not None:\n",
        "    output_f1 = round(output_f1, input_round)\n",
        "    output_acc = round(output_acc, input_round)\n",
        "    output_pre = round(output_pre, input_round)\n",
        "    output_rec = round(output_rec, input_round)\n",
        "\n",
        "  return output_f1, output_acc, output_pre, output_rec"
      ],
      "metadata": {
        "id": "aQctU5PLtIaK"
      },
      "id": "aQctU5PLtIaK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Save to wd\n",
        "\n",
        "# Indeces of test dates\n",
        "test_dates = y_test.index.to_frame()\n",
        "test_dates.to_parquet(get_path('results/test_index.parquet', 'outputs'))\n",
        "\n",
        "# y predictions\n",
        "np.save(get_path('results/y_test_pred.npy', 'outputs'), final_pred_y_opt)\n",
        "np.save(get_path('results/y_test_pred_proba.npy', 'outputs'), final_pred_y_opt)\n",
        "\n",
        "# actual test vals\n",
        "np.save(get_path('results/y_test_true.npy', 'outputs'), y_test_conv)\n",
        "# np.save(get_path('results/X_test_true.npy', 'outputs'), X_test)"
      ],
      "metadata": {
        "id": "1xmmhddNkGVl"
      },
      "id": "1xmmhddNkGVl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save locally\n",
        "files.download(get_path('results/test_index.parquet', 'outputs'))\n",
        "\n",
        "files.download(get_path('results/y_test_pred.npy', 'outputs'))\n",
        "files.download(get_path('results/y_test_pred_proba.npy', 'outputs'))\n",
        "\n",
        "files.download(get_path('results/y_test_true.npy', 'outputs'))\n",
        "# files.download(get_path('results/X_test_true.npy', 'outputs'))"
      ],
      "metadata": {
        "id": "CW4YkmilzNCV"
      },
      "id": "CW4YkmilzNCV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4d614dcc",
      "metadata": {
        "id": "4d614dcc"
      },
      "source": [
        "## (draft code)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature importance"
      ],
      "metadata": {
        "id": "psgzh9vM03IA"
      },
      "id": "psgzh9vM03IA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ae4e7c0",
      "metadata": {
        "id": "3ae4e7c0"
      },
      "outputs": [],
      "source": [
        "feature_importances = final_model.feature_importances_\n",
        "# map scores to feature names\n",
        "# feature_importances\n",
        "feature_names = X_train.columns.tolist()\n",
        "\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'feat': feature_names,\n",
        "    'importance': feature_importances\n",
        "})\n",
        "\n",
        "# sort importance\n",
        "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
        "\n",
        "# print(feature_importance_df)\n",
        "feature_importance_df\n",
        "\n",
        "# most important features\n",
        "# print(feature_importance_df.head(25))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd348692",
      "metadata": {
        "id": "fd348692"
      },
      "outputs": [],
      "source": [
        "threshold_importance = 0.95\n",
        "# calculate most important 90 percent of the importance\n",
        "feature_importance_df['cumulative_imp'] = feature_importance_df['importance'].cumsum()\n",
        "features_percent = feature_importance_df[feature_importance_df['cumulative_imp'] <= threshold_importance].shape[0] + 1\n",
        "features_percent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4517ba9a",
      "metadata": {
        "id": "4517ba9a"
      },
      "outputs": [],
      "source": [
        "# Most important features:\n",
        "print(round(threshold_importance*100), \"% (most important features):\", features_percent)\n",
        "feature_importance_df.head(features_percent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f820c002",
      "metadata": {
        "id": "f820c002"
      },
      "outputs": [],
      "source": [
        "# feature_importance_df.tail(1)\n",
        "# Least important features:\n",
        "print(\"Remaining\", round((1-threshold_importance)*100), \"% (least important features):\", len(feature_names)-features_percent)\n",
        "feature_importance_df.tail(len(feature_names)-features_percent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1eb8627",
      "metadata": {
        "id": "a1eb8627"
      },
      "outputs": [],
      "source": [
        "# Features with 0 importance:\n",
        "print(\"Features with 0 importance:\", len(feature_importance_df[feature_importance_df['importance']==0]))\n",
        "\n",
        "feature_importance_df[feature_importance_df['importance']==0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a0dc876",
      "metadata": {
        "id": "1a0dc876"
      },
      "source": [
        "Feature importance by type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d908cbbe",
      "metadata": {
        "id": "d908cbbe"
      },
      "outputs": [],
      "source": [
        "mapping_dict = {\n",
        "    'soil': '_deep|_shallow',\n",
        "    'runoff':'ro',\n",
        "    'rain':'rain',\n",
        "    'calibration':'_cal'\n",
        "}\n",
        "\n",
        "for col_name, pattern in mapping_dict.items():\n",
        "    feature_importance_df[col_name] = feature_importance_df['feat'].str.contains(pattern, case=False, regex=True)\n",
        "\n",
        "feature_importance_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31c6b4b8",
      "metadata": {
        "id": "31c6b4b8"
      },
      "outputs": [],
      "source": [
        "feature_importance_df['most'] = (feature_importance_df['cumulative_imp'] <= threshold_importance)\n",
        "feature_importance_df['zero'] = (feature_importance_df['importance'] == 0)\n",
        "\n",
        "cat_cols = list(mapping_dict.keys())\n",
        "\n",
        "table_feature_cat_importance = pd.DataFrame({\n",
        "    'Total features': feature_importance_df[cat_cols].sum(),\n",
        "    'Above threshold': feature_importance_df[feature_importance_df['most']][cat_cols].sum(),\n",
        "    'Below threshold': feature_importance_df[~feature_importance_df['most']][cat_cols].sum(),\n",
        "    'Zero importance': feature_importance_df[feature_importance_df['zero']][cat_cols].sum()\n",
        "}).fillna(0).astype(int)\n",
        "\n",
        "del cat_cols, mapping_dict\n",
        "\n",
        "table_feature_cat_importance.index.name = 'Category'\n",
        "\n",
        "table_feature_cat_importance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Threshold"
      ],
      "metadata": {
        "id": "OIHYWuCTdpnk"
      },
      "id": "OIHYWuCTdpnk"
    },
    {
      "cell_type": "code",
      "source": [
        "# threshold = 0.5\n",
        "# y_bin = (oof_pred >= threshold).astype(np.int32)\n",
        "# best_f1 = f1_score(y_train_oof, y_bin)\n",
        "# print(f\"F1 at default 0.5 threshold: {best_f1:.4f}\")\n",
        "\n",
        "# best_threshold = 0\n",
        "# thresholds = np.linspace(0.01, 0.99, 100)\n",
        "\n",
        "# for threshold in thresholds:\n",
        "#     y_bin = (y_proba >= threshold).astype(np.int32)\n",
        "#     current_f1 = f1_score(y_train_oof, y_bin)\n",
        "\n",
        "#     if current_f1 > best_f1:\n",
        "#         best_f1 = current_f1\n",
        "#         best_threshold = threshold\n",
        "\n",
        "# print(f\"Optimal threshold:\\t{best_threshold:.4f}\")\n",
        "# print(f\"Threshold F1:\\t{best_f1:.4f}\")"
      ],
      "metadata": {
        "id": "gsyUqcVJP5t4"
      },
      "id": "gsyUqcVJP5t4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "701b4a53",
      "metadata": {
        "id": "701b4a53",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "# X_itest_mini, X_holdout, y_itest_mini, y_holdout = train_test_split(X_test_inner, y_test_inner, test_size=0.2, random_state=SEED, shuffle=False)\n",
        "\n",
        "# t_tuner = TunedThresholdClassifierCV(\n",
        "#     estimator=final_model,\n",
        "#     scoring=make_scorer(f1_score),\n",
        "#     cv=\"prefit\",\n",
        "#     thresholds=100,\n",
        "#     refit=False,\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "\n",
        "# t_tuner.fit(X_itest_mini, y_itest_mini)\n",
        "\n",
        "# print(\n",
        "#     \"Threshold:\", t_tuner.best_threshold_,\n",
        "#     \"F1:\", t_tuner.best_score_\n",
        "# )\n",
        "\n",
        "# t_x_pred = t_tuner.predict(X_holdout)\n",
        "\n",
        "# print(\n",
        "#     f1_score(y_holdout, t_x_pred),\n",
        "#     accuracy_score(y_holdout, t_x_pred),\n",
        "#     precision_score(y_holdout, t_x_pred),\n",
        "#     recall_score(y_holdout, t_x_pred),\n",
        "#     sep=\"\\n\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# # Thresholds\n",
        "# X_itest_train, X_itest_test, y_itest_train, y_itest_test = train_test_split(X_test_inner, y_test_inner, test_size=0.2, shuffle=False)\n",
        "\n",
        "# threshold = 0.5\n",
        "# y_proba = final_model.predict_proba(X_itest_train)[:,1]\n",
        "# y_bin = (y_proba >= threshold).astype(np.int32)\n",
        "# best_f1 = f1_score(y_itest_train, y_bin)\n",
        "# print(f\"F1 at default 0.5 threshold: {best_f1:.4f}\")\n",
        "\n",
        "# best_threshold = 0\n",
        "# thresholds = np.linspace(0.01, 0.99, 100)\n",
        "\n",
        "# for threshold in thresholds:\n",
        "#     y_bin = (y_proba >= threshold).astype(np.int32)\n",
        "#     current_f1 = f1_score(y_itest_train, y_bin)\n",
        "\n",
        "#     if current_f1 > best_f1:\n",
        "#         best_f1 = current_f1\n",
        "#         best_threshold = threshold\n",
        "\n",
        "# print(f\"Optimal threshold:\\t{best_threshold:.4f}\")\n",
        "# print(f\"Threshold F1:\\t{best_f1:.4f}\")\n",
        "\n",
        "# y_proba_test = final_model.predict_proba(X_itest_test)[:,1]\n",
        "# final_predictions = (y_proba_test >= best_threshold).astype(np.int32)\n",
        "\n",
        "# print(\n",
        "#     \"\\n--- Metrics on Holdout Set (using optimal threshold) ---\",\n",
        "#     f\"F1:\\t{f1_score(y_itest_test, final_predictions):.4f}\",\n",
        "#     f\"Acc:\\t{accuracy_score(y_itest_test, final_predictions):.4f}\",\n",
        "#     f\"Pre:\\t{precision_score(y_itest_test, final_predictions):.4f}\",\n",
        "#     f\"Rec:\\t{recall_score(y_itest_test, final_predictions):.4f}\",\n",
        "#     sep=\"\\n\"\n",
        "# )\n",
        "# # print(f\"F1:\\t{f1_score(y_itest_test, final_predictions):.4f}\")\n",
        "# # print(f\"Acc:\\t{accuracy_score(y_itest_test, final_predictions):.4f}\")\n",
        "# # print(f\"Pre:\\t{precision_score(y_itest_test, final_predictions):.4f}\")\n",
        "# # print(f\"Rec:\\t{recall_score(y_itest_test, final_predictions):.4f}\")\n",
        "\n",
        "# del y_proba, threshold, thresholds, best_f1\n",
        "# del X_itest_train, y_itest_train"
      ],
      "metadata": {
        "id": "GZXN5p8gg1RN",
        "cellView": "form"
      },
      "id": "GZXN5p8gg1RN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e734f9b6",
      "metadata": {
        "id": "e734f9b6"
      },
      "source": [
        "### Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To smooth the post-processing results, windows can be tested to determine the optimal majority-vote,\n",
        "\n",
        "This function will run on CPU."
      ],
      "metadata": {
        "id": "kGoWSrififBU"
      },
      "id": "kGoWSrififBU"
    },
    {
      "cell_type": "code",
      "source": [
        "# def apply_smoothing(predictions_array, window_size=5):\n",
        "#     predictions_series = pd.Series(predictions_array)\n",
        "#     smoothed = predictions_series.rolling(\n",
        "#         window=window_size,\n",
        "#         center=True,\n",
        "#         min_periods=1\n",
        "#     ).apply(lambda x: np.bincount(x.astype(int)).argmax(), raw=False).astype(int)\n",
        "#     return smoothed.values\n",
        "\n",
        "\n",
        "# # Smallest window size\n",
        "# window_min = 3\n",
        "# # Largest window size\n",
        "# window_max = 25\n",
        "\n",
        "# # Start w no smoothing\n",
        "# best_window_size = 1\n",
        "# best_smoothing_f1 = f1_score(y_itest_test, final_predictions)\n",
        "# # print(\"No window F1:\\t\", best_smoothing_f1)\n",
        "# print(f\"No window F1:\\t{best_smoothing_f1:.4f}\")\n",
        "\n",
        "# # Test range of odd window sizes\n",
        "# for window_size in range(window_min, window_max+1, 2):\n",
        "#     # print(\"Testing window =\", window_size)\n",
        "#     smoothed_preds = apply_smoothing(final_predictions, window_size=window_size)\n",
        "#     f1 = f1_score(y_itest_test, smoothed_preds)\n",
        "#     # print(\"Window\", window_size, \"F1:\\t\", f1)\n",
        "#     print(f\"Window {window_size} F1:\\t{f1:.4f}\")\n",
        "\n",
        "#     if f1 > best_smoothing_f1:\n",
        "#         # print(\"F1 improved with smoothing!\")\n",
        "#         best_smoothing_f1 = f1\n",
        "#         best_window_size = window_size\n",
        "\n",
        "# print(f\"Optimal window size: {best_window_size}\")\n",
        "# print(f\"Best F1: {best_smoothing_f1:.4f}\")\n",
        "\n",
        "# del window_min, window_max, f1, window_size"
      ],
      "metadata": {
        "id": "CGP2sDpCgE4M"
      },
      "id": "CGP2sDpCgE4M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b82de6d5",
      "metadata": {
        "id": "b82de6d5"
      },
      "outputs": [],
      "source": [
        "# def apply_smoothing(predictions_array, window_size=5):\n",
        "#     predictions_series = pd.Series(predictions_array)\n",
        "#     smoothed = predictions_series.rolling(\n",
        "#         window=window_size,\n",
        "#         center=True,\n",
        "#         min_periods=1\n",
        "#     ).apply(lambda x: np.bincount(x.astype(int)).argmax(), raw=False).astype(int)\n",
        "#     return smoothed.values\n",
        "\n",
        "# # def apply_smoothing(predictions_array, window_size=5):\n",
        "# #     # 1. Convert NumPy array to CuPy array (if it isn't already on the GPU)\n",
        "# #     predictions_cp = cp.asarray(predictions_array)\n",
        "\n",
        "# #     # 2. Convert CuPy array to cuDF Series\n",
        "# #     predictions_series_gpu = cudf.Series(predictions_cp)\n",
        "\n",
        "# #     # Define the rolling operation\n",
        "# #     smoothed_gpu = predictions_series_gpu.rolling(\n",
        "# #         window=window_size,\n",
        "# #         center=True,\n",
        "# #         min_periods=1\n",
        "# #     )\n",
        "\n",
        "# #     # 3. Apply the majority vote function using CuPy operations within a custom apply\n",
        "# #     # We define a custom function using cupy's bincount\n",
        "# #     def majority_vote_gpu(x):\n",
        "# #         # Convert to int, count occurrences, and find the index of the max count (argmax)\n",
        "# #         return cp.bincount(x.astype(cp.int32)).argmax()\n",
        "\n",
        "# #     # Apply the function\n",
        "# #     # Note: cuDF's rolling apply works best with simple cupy aggregations.\n",
        "# #     # For complex custom lambdas like this, performance may vary,\n",
        "# #     # but it keeps the data on the GPU.\n",
        "# #     smoothed_gpu = smoothed_gpu.apply(majority_vote_gpu)\n",
        "\n",
        "# #     # 4. Convert back to CPU NumPy array for final use if needed (optional)\n",
        "# #     return smoothed_gpu.values.get() # .get() moves data from GPU (CuPy array) back to CPU (NumPy array)\n",
        "# #     # return smoothed_gpu.values # Keep as CuPy array on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "079ec9bc",
      "metadata": {
        "id": "079ec9bc"
      },
      "outputs": [],
      "source": [
        "# # Smallest window size\n",
        "# window_min = 3\n",
        "# # Largest window size\n",
        "# window_max = 25\n",
        "\n",
        "# # Start w no smoothing\n",
        "# best_window_size = 1\n",
        "# best_smoothing_f1 = f1_score(y_itest_test, final_predictions)\n",
        "# # print(\"No window F1:\\t\", best_smoothing_f1)\n",
        "# print(f\"No window F1:\\t{best_smoothing_f1:.4f}\")\n",
        "\n",
        "# # Test range of odd window sizes\n",
        "# for window_size in range(window_min, window_max+1, 2):\n",
        "#     # print(\"Testing window =\", window_size)\n",
        "#     smoothed_preds = apply_smoothing(final_predictions, window_size=window_size)\n",
        "#     f1 = f1_score(y_itest_test, smoothed_preds)\n",
        "#     # print(\"Window\", window_size, \"F1:\\t\", f1)\n",
        "#     print(f\"Window {window_size} F1:\\t{f1:.4f}\")\n",
        "\n",
        "#     if f1 > best_smoothing_f1:\n",
        "#         # print(\"F1 improved with smoothing!\")\n",
        "#         best_smoothing_f1 = f1\n",
        "#         best_window_size = window_size\n",
        "\n",
        "# print(f\"Optimal window size: {best_window_size}\")\n",
        "# print(f\"Best F1: {best_smoothing_f1:.4f}\")\n",
        "\n",
        "# del window_min, window_max, f1, window_size"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}