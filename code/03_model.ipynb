{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "68a3c4a0",
      "metadata": {
        "id": "68a3c4a0"
      },
      "source": [
        "# Data Splitting and Modelling\n",
        "\n",
        "Author: Gillian A. McGinnis, final-semester M.S. Information Science - Machine Learning  \n",
        "The University of Arizona College of Information  \n",
        "INFO 698 - Capstone  \n",
        "Start date: 21 October 2025  \n",
        "Last updated: 25 November 2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d80b978",
      "metadata": {
        "id": "6d80b978"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Module providing code for test/train split and sliding window creation. Relies on 01_clean.ipynb completion.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de84a462",
      "metadata": {
        "id": "de84a462"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33ab0f88",
      "metadata": {
        "id": "33ab0f88"
      },
      "outputs": [],
      "source": [
        "var_of_interest = \"obstruction_ro\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7208265e",
      "metadata": {
        "id": "7208265e"
      },
      "source": [
        "### Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU Setup\n",
        "%load_ext cudf.pandas\n",
        "import pandas as pd\n",
        "import cudf\n",
        "import cupy as cp\n",
        "\n",
        "import cuml.accel\n",
        "cuml.accel.install()\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit, train_test_split, RandomizedSearchCV, TunedThresholdClassifierCV\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, precision_recall_curve, make_scorer, roc_auc_score, auc\n",
        "# For help with model tuning\n",
        "from sklearn.base import clone"
      ],
      "metadata": {
        "id": "-2FCjcFbVrfq"
      },
      "id": "-2FCjcFbVrfq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import userdata\n",
        "gh_pat = userdata.get('gh_pat')\n",
        "gh_user = userdata.get('gh_user')\n",
        "gh_repo = userdata.get('gh_repo')\n",
        "import os\n",
        "if not os.path.exists(f'{gh_repo}'):\n",
        "  print(\"Cloning repo...\")\n",
        "  repo_url = f'https://{gh_pat}@github.com/{gh_user}/{gh_repo}'\n",
        "  !git clone {repo_url}\n",
        "\n",
        "if os.getcwd() == '/content':\n",
        "    print(\"Changing wd...\")\n",
        "    os.chdir(f'{gh_repo}/code')\n",
        "\n",
        "# # Verify the current working directory\n",
        "print(f\"Current working directory is: {os.getcwd()}\")\n",
        "\n",
        "del gh_pat, gh_user, gh_repo"
      ],
      "metadata": {
        "id": "dxJXyRMZHzAc"
      },
      "id": "dxJXyRMZHzAc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # from google.colab import drive\n",
        "# # drive.mount('/content/drive')\n",
        "\n",
        "# from google.colab import userdata\n",
        "# gh_pat = userdata.get('gh_pat')\n",
        "# gh_repo = userdata.get('gh_repo')\n",
        "# repo_url = f'https://{gh_pat}@github.com/{gh_repo}'\n",
        "# !git clone {repo_url}"
      ],
      "metadata": {
        "id": "RHYanMjnWKQb"
      },
      "id": "RHYanMjnWKQb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# if os.getcwd() == '/content':\n",
        "#     print(\"Changing wd...\")\n",
        "#     os.chdir('info-698-capstone/code')\n",
        "\n",
        "# # # Verify the current working directory\n",
        "# print(f\"Current working directory is: {os.getcwd()}\")"
      ],
      "metadata": {
        "id": "OuqM0JgLV-5J"
      },
      "id": "OuqM0JgLV-5J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bb557fb",
      "metadata": {
        "id": "0bb557fb"
      },
      "outputs": [],
      "source": [
        "# General packages\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "from scipy.stats import randint, uniform\n",
        "# from sklearn.model_selection import TimeSeriesSplit, train_test_split, RandomizedSearchCV, TunedThresholdClassifierCV\n",
        "# from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, precision_recall_curve, make_scorer, roc_auc_score\n",
        "\n",
        "# from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, make_scorer, roc_auc_score\n",
        "# from cuml.metrics import precision_recall_curve\n",
        "\n",
        "# For saving models\n",
        "import joblib\n",
        "\n",
        "# For data importing and exporting\n",
        "from helper_utils import get_path, model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcb5d8f6",
      "metadata": {
        "id": "dcb5d8f6"
      },
      "outputs": [],
      "source": [
        "## (Optional chunk)\n",
        "# Current session information\n",
        "\n",
        "# From StackOverflow,\n",
        "# https://stackoverflow.com/a/62128239/23486987\n",
        "try:\n",
        "    import session_info\n",
        "except:\n",
        "    !pip install session_info\n",
        "    import session_info\n",
        "# !pip install session_info\n",
        "# import session_info\n",
        "session_info.show(dependencies=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c40baf7",
      "metadata": {
        "id": "9c40baf7"
      },
      "outputs": [],
      "source": [
        "# # To make it easier to tell when processes have completed -- can delete later\n",
        "# # From StackOverflow,\n",
        "# # https://stackoverflow.com/a/62128239/23486987\n",
        "# try:\n",
        "#     from playsound3 import playsound\n",
        "# except:\n",
        "#     !pip install playsound3\n",
        "#     from playsound3 import playsound\n",
        "\n",
        "# Google colab compatible:\n",
        "# https://stackoverflow.com/a/68582785/23486987\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "def play_chime():\n",
        "  return Audio(get_path('completed.mp3', 'code'), autoplay=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e7c4847",
      "metadata": {
        "id": "1e7c4847"
      },
      "outputs": [],
      "source": [
        "# Set seed\n",
        "np.random.seed(42)\n",
        "cp.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure GPU active\n",
        "# !nvidia-smi\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"Warning: GPU not found!\")"
      ],
      "metadata": {
        "id": "1t3ZBgBzJqaU"
      },
      "id": "1t3ZBgBzJqaU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "30ef4832",
      "metadata": {
        "id": "30ef4832"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd2e4fca",
      "metadata": {
        "id": "cd2e4fca"
      },
      "outputs": [],
      "source": [
        "# united_water = pd.read_parquet('data/clean/water_nocal.parquet')\n",
        "# data_cal = pd.read_parquet('data/clean/calibration.parquet')\n",
        "# data_cal = data_cal.rename(columns={'weir_level':'weir_level_cal'})\n",
        "\n",
        "# united_soil = pd.read_parquet('data/clean/soil.parquet')\n",
        "\n",
        "united_water = pd.read_parquet(get_path('clean/water_nocal.parquet'))\n",
        "united_soil = pd.read_parquet(get_path('clean/soil.parquet'))\n",
        "\n",
        "# united_water = pd.read_parquet('data/clean/water_nocal.parquet')\n",
        "data_cal = pd.read_parquet(get_path('clean/calibration.parquet'))\n",
        "data_cal = data_cal.rename(columns={'weir_level':'weir_level_cal'})\n",
        "\n",
        "# united_soil = pd.read_parquet('data/clean/soil.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_cal.info()"
      ],
      "metadata": {
        "id": "9QEIHj-wNiBd"
      },
      "id": "9QEIHj-wNiBd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d98f94e4",
      "metadata": {
        "id": "d98f94e4"
      },
      "source": [
        "### Cleanup\n",
        "\n",
        "Small amount of data wrangling for memory improvements (some as a consequence of importing)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebf48710",
      "metadata": {
        "id": "ebf48710"
      },
      "source": [
        "#### Memory improvements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "689c32ff",
      "metadata": {
        "id": "689c32ff"
      },
      "outputs": [],
      "source": [
        "# Select columns of interest\n",
        "data_water = united_water.drop(columns=['raw_rain', 'chk_note_rain', 'chk_fail_rain', 'chk_note_ro', 'chk_fail_ro', 'comment_ro', 'source_ro'])\n",
        "\n",
        "# Cleanup\n",
        "del united_water\n",
        "\n",
        "# Remove duplicate entries\n",
        "data_water = data_water.reset_index().drop_duplicates(keep='first').set_index('datetime')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "357e91e4",
      "metadata": {
        "id": "357e91e4"
      },
      "outputs": [],
      "source": [
        "water_drops = ['level_ro', 'obstruction_ro', 'gap_fill_ro', 'weir_cleaning_ro', 'spike_ro', 'calibration_ro']\n",
        "water_drops.remove(var_of_interest)\n",
        "\n",
        "data_water = data_water.drop(water_drops, axis=1)\n",
        "\n",
        "del water_drops\n",
        "\n",
        "data_water.info(memory_usage=\"deep\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1706a0d9",
      "metadata": {
        "id": "1706a0d9"
      },
      "outputs": [],
      "source": [
        "united_soil['sample'] = united_soil['sample'].astype('category')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "168e27bd",
      "metadata": {
        "id": "168e27bd"
      },
      "source": [
        "## Prepare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa5776e5",
      "metadata": {
        "id": "aa5776e5"
      },
      "outputs": [],
      "source": [
        "# temp_subset_start= None\n",
        "# temp_subset_end = None\n",
        "# ### Note ###\n",
        "# # REMOVE this later -- just a smaller subset for feature engineering testing!!!\n",
        "# # temp_subset_start = '2000-01-01 00:00:00'\n",
        "temp_subset_start = '2001-02-01 00:00:00'\n",
        "temp_subset_end = '2011-12-31 23:59:59'\n",
        "# ######\n",
        "data_water = data_water[temp_subset_start:temp_subset_end]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecb9dc61",
      "metadata": {
        "id": "ecb9dc61"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afa843b0",
      "metadata": {
        "id": "afa843b0"
      },
      "source": [
        "### Distance from Event"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24851c9b",
      "metadata": {
        "id": "24851c9b"
      },
      "outputs": [],
      "source": [
        "def timesince_feat(input_df, input_col, input_unit):\n",
        "    # output_df = input_df.copy()#[input_col].to_frame()\n",
        "    output_df = input_df\n",
        "    instances = output_df[input_col].notna()\n",
        "    # Create groupings based on most recent instance\n",
        "    group_id = instances.cumsum()\n",
        "    # Exclude the first grouping\n",
        "    # otherwise it assumes there was an event just prior to the first entry\n",
        "    group_id = group_id.replace(0, np.nan)\n",
        "    # Create new column to count the distance in days since the point\n",
        "    # which resets to 0 at each new point\n",
        "    output_df['timestamp'] = pd.to_datetime(output_df.index)\n",
        "    # Get start timestamp of the group\n",
        "    output_df['ts_start'] = output_df.groupby(group_id)['timestamp'].transform('min')\n",
        "    # Calculate the distance\n",
        "    if input_unit == \"minutes\":\n",
        "        # output_df[f\"minsince_{input_col}\"] = (output_df['timestamp'] - output_df['ts_start']).dt.total_seconds().div(60).astype('Int32')\n",
        "        output_df[f\"minsince_{input_col}\"] = (output_df['timestamp'] - output_df['ts_start']).dt.total_seconds().div(60).astype(np.float32)\n",
        "        # output_df[f\"minsince_{input_col}\"] = output_df[f\"minsince_{input_col}\"].astype(np.float32)\n",
        "    elif input_unit == \"days\":\n",
        "        # output_df[f\"daysince_{input_col}\"] = (output_df['timestamp'] - output_df['ts_start']).dt.days.astype('Int32')\n",
        "        output_df[f\"daysince_{input_col}\"] = (output_df['timestamp'] - output_df['ts_start']).dt.days.astype(np.float32)\n",
        "        # output_df[f\"minsince_{input_col}\"] = output_df[f\"minsince_{input_col}\"].astype(np.float32)\n",
        "        # output_df[f\"daysince_{input_col}\"] = output_df[f\"daysince_{input_col}\"].astype('Int32')\n",
        "    # Remove extra cols\n",
        "    output_df = output_df.drop(columns=['timestamp', 'ts_start'])\n",
        "    return output_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "830a5cbc",
      "metadata": {
        "id": "830a5cbc"
      },
      "source": [
        "#### Rain\n",
        "Create feature which tracks how recent a rain event occurred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0a3b363",
      "metadata": {
        "id": "e0a3b363"
      },
      "outputs": [],
      "source": [
        "data_water = timesince_feat(data_water, 'ra_rain', \"minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e786a7e9",
      "metadata": {
        "id": "e786a7e9"
      },
      "source": [
        "### Rain event\n",
        "\n",
        "Keep track of cumulative rainfall during a specific event."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59b7c231",
      "metadata": {
        "id": "59b7c231"
      },
      "outputs": [],
      "source": [
        "# Create index of instances where there is a data point\n",
        "rain_event = (data_water['ra_rain'].isnull() & ((data_water['minsince_ra_rain'] >= 5.0) & (data_water['minsince_ra_rain'] != 0)))\n",
        "# Create groupings based on most recent instance\n",
        "rain_event_id = rain_event.cumsum()\n",
        "# Create new column to count number of records since the point\n",
        "# which resets to 0 at each new point\n",
        "data_water['eventsum_ra_rain'] = data_water.groupby(rain_event_id)['ra_rain'].cumsum()\n",
        "\n",
        "del rain_event, rain_event_id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96a81ae4",
      "metadata": {
        "id": "96a81ae4"
      },
      "source": [
        "### Decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46f01638",
      "metadata": {
        "id": "46f01638"
      },
      "outputs": [],
      "source": [
        "def decay_feat(input_df, input_col, input_dec_rate = -0.1):\n",
        "    output_df = input_df\n",
        "    if f\"minsince_{input_col}\" not in output_df.columns:\n",
        "        output_df = timesince_feat(input_df = output_df, input_col = input_col, input_unit = \"minutes\")\n",
        "    # Update for GPU for overflow fix\n",
        "    output_df[f\"minsince_{input_col}\"] = output_df[f\"minsince_{input_col}\"].astype(np.float64)\n",
        "\n",
        "    output_df[f\"decayrate{input_dec_rate}_{input_col}\"] = np.exp(input_dec_rate * output_df[f\"minsince_{input_col}\"]).astype(np.float32)\n",
        "    output_df[f\"ffill_{input_col}\"] = output_df[input_col].ffill()\n",
        "    output_df[f\"decay{input_dec_rate}_{input_col}\"] = (output_df[f\"ffill_{input_col}\"] * output_df[f\"decayrate{input_dec_rate}_{input_col}\"])\n",
        "\n",
        "    return output_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "947bae8b",
      "metadata": {
        "id": "947bae8b"
      },
      "outputs": [],
      "source": [
        "# Replace NAs in rain with 0\n",
        "data_water['ra_rain'] = data_water['ra_rain'].fillna(0)\n",
        "\n",
        "# Apply decay function\n",
        "data_water = decay_feat(data_water, 'eventsum_ra_rain')\n",
        "\n",
        "# Drop extra column\n",
        "# minutes since rain event will be the same as minutes since most recent rain\n",
        "data_water = data_water.drop('minsince_eventsum_ra_rain', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6154e3b7",
      "metadata": {
        "id": "6154e3b7"
      },
      "source": [
        "### Lag features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c5d3c7b",
      "metadata": {
        "id": "8c5d3c7b"
      },
      "source": [
        "#### Consistent cols\n",
        "\n",
        "Modify the rows to prevent inappropriate data shifts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0e75eb6",
      "metadata": {
        "id": "a0e75eb6"
      },
      "outputs": [],
      "source": [
        "original_indices = data_water.index.copy()\n",
        "\n",
        "new_index = pd.date_range(start = data_water.index.min(),\n",
        "                          end = data_water.index.max(),\n",
        "                          freq = '5min')\n",
        "\n",
        "# Reindex\n",
        "data_water = data_water.reindex(new_index)\n",
        "\n",
        "# Cleanup\n",
        "del new_index\n",
        "\n",
        "# # Return\n",
        "# data_water = data_water.loc[original_indices]\n",
        "# del original_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62d8945f",
      "metadata": {
        "id": "62d8945f"
      },
      "source": [
        "Get values from other recent time stamps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09c4b606",
      "metadata": {
        "id": "09c4b606"
      },
      "outputs": [],
      "source": [
        "def lag_feats(input_df, input_cols, input_lags):\n",
        "    output_df = input_df#.copy()\n",
        "    for col in input_cols:\n",
        "        for lag in input_lags:\n",
        "            output_df[f\"{col}_lag{lag}\"] = output_df[col].shift(lag)\n",
        "    return output_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "797920ef",
      "metadata": {
        "id": "797920ef"
      },
      "outputs": [],
      "source": [
        "# Columns to get temporal stats on\n",
        "cols_to_shift = ['raw_ro', 'ra_rain']\n",
        "\n",
        "# data at 5-min increments -- lag to record values at 5m, 10m, 15m, 20m, 25m, 30m, 1h, 2h, 3h prior\n",
        "lags_of_interest = [1, 2, 3, 4, 5, 6, 12, 24, 36]\n",
        "\n",
        "data_water = lag_feats(data_water, cols_to_shift, lags_of_interest)\n",
        "\n",
        "# data_water.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ddce5f3",
      "metadata": {
        "id": "0ddce5f3"
      },
      "source": [
        "### Rolling stats\n",
        "\n",
        "Get stat values from range of recent time stamps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb010731",
      "metadata": {
        "id": "eb010731"
      },
      "outputs": [],
      "source": [
        "def rolling_feats(input_df, input_cols, input_windows, input_mtype = \"mean\"):\n",
        "    output_df = input_df#.copy()\n",
        "\n",
        "    # Create a dummy series of index values (0, 1, 2, ... N) once\n",
        "    # 'x' represents the position within the dataframe for the regression calculation\n",
        "    x_series = pd.Series(np.arange(len(output_df)), index=output_df.index)\n",
        "\n",
        "    for col in input_cols:\n",
        "        for window in input_windows:\n",
        "            # 1. Calculate mean and std\n",
        "            if input_mtype == \"mean\":\n",
        "                output_df[f\"{col}_rollmean_{window}\"] = output_df[col].rolling(window).mean().astype(np.float32)\n",
        "            elif input_mtype == \"sum\":\n",
        "                output_df[f\"{col}_rollsum_{window}\"] = output_df[col].rolling(window).sum().astype(np.float32)\n",
        "            elif input_mtype == \"both\":\n",
        "                output_df[f\"{col}_rollmean_{window}\"] = output_df[col].rolling(window).mean().astype(np.float32)\n",
        "                output_df[f\"{col}_rollsum_{window}\"] = output_df[col].rolling(window).sum().astype(np.float32)\n",
        "            output_df[f\"{col}_rollstd_{window}\"] = output_df[col].rolling(window).std().astype(np.float32)\n",
        "\n",
        "            # 2. Calculate Slope using vectorized operations\n",
        "            # Calculate Covariance of Y (data) vs X (index position)\n",
        "            rolling_cov = output_df[col].rolling(window).cov(x_series)\n",
        "            # Calculate Variance of X (index position)\n",
        "            rolling_var_x = x_series.rolling(window).var()\n",
        "            # Slope = Cov(Y, X) / Var(X)\n",
        "            output_df[f\"{col}_rollslope_{window}\"] = (rolling_cov / rolling_var_x).astype(np.float32)\n",
        "\n",
        "            # Note on edge cases:\n",
        "            # The initial 'window-1' values for rolling_var_x will be NaN/incorrect.\n",
        "            # Pandas automatically handles alignment, so the division result will also be NaN where appropriate.\n",
        "            # This method works very well for standard time series analysis.\n",
        "    return output_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf7eaaaf",
      "metadata": {
        "id": "cf7eaaaf"
      },
      "outputs": [],
      "source": [
        "# Inclusive of current point--\n",
        "# 10m, 15m, 20m, 25m, 30m, 1h, 3h, 6h, 12h, 24h\n",
        "windows_of_interest = [2, 3, 4, 5, 6, 12, 36, 72, 144, 288]\n",
        "\n",
        "# data_water = rolling_feats(data_water, ['raw_ro'], windows_of_interest, \"mean\")\n",
        "data_water = rolling_feats(data_water, ['raw_ro'], windows_of_interest, \"both\")\n",
        "data_water = rolling_feats(data_water, ['ra_rain'], windows_of_interest, \"sum\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "596fa05d",
      "metadata": {
        "id": "596fa05d"
      },
      "source": [
        "Change since last value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccb6f5c9",
      "metadata": {
        "id": "ccb6f5c9"
      },
      "outputs": [],
      "source": [
        "data_water['raw_ro_change'] = data_water['raw_ro'].diff()\n",
        "data_water['ra_rain_change'] = data_water['ra_rain'].diff()\n",
        "# data_water['raw_ro_rollmean_2']\n",
        "data_water['raw_ro_rollmean_2_change'] = data_water['raw_ro_rollmean_2'].diff()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Revert index\n",
        "# (adjusted for GPU)\n",
        "\n",
        "data_water_reset = data_water.reset_index()\n",
        "index_col_name = data_water_reset.columns[0]\n",
        "indices_df = original_indices.to_frame(name=index_col_name)\n",
        "\n",
        "filtered_data_water = cudf.merge(\n",
        "    data_water_reset,\n",
        "    indices_df,\n",
        "    on=index_col_name,\n",
        "    how='inner'\n",
        ")\n",
        "data_water = filtered_data_water.set_index(index_col_name)\n",
        "\n",
        "del original_indices, filtered_data_water, index_col_name, data_water_reset, indices_df"
      ],
      "metadata": {
        "id": "MH0ZMuxQGFwO"
      },
      "id": "MH0ZMuxQGFwO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0d8524e",
      "metadata": {
        "id": "b0d8524e"
      },
      "outputs": [],
      "source": [
        "# # Return\n",
        "# data_water = data_water.loc[original_indices]\n",
        "\n",
        "# del original_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4794b582",
      "metadata": {
        "id": "4794b582"
      },
      "source": [
        "## Soil"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f8e3f87",
      "metadata": {
        "id": "5f8e3f87"
      },
      "source": [
        "Pivot the soil data such that each sample has its own columns, and separated by depth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd40d185",
      "metadata": {
        "id": "fd40d185"
      },
      "outputs": [],
      "source": [
        "# Drop irrelevant column\n",
        "data_soil_shallow = united_soil.copy().drop('h2o_by_wet_deep', axis=1)\n",
        "data_soil_shallow['sample'] = data_soil_shallow['sample'].astype('float32')\n",
        "# Pivot wider\n",
        "data_soil_shallow = data_soil_shallow.pivot(columns='sample', values='h2o_by_wet_shallow')\n",
        "\n",
        "# Drop irrelevant column\n",
        "data_soil_deep = united_soil.copy().drop('h2o_by_wet_shallow', axis=1)\n",
        "\n",
        "data_soil_deep['sample'] = data_soil_deep['sample'].astype('float32')\n",
        "# Pivot wider\n",
        "data_soil_deep = data_soil_deep.pivot(columns='sample', values='h2o_by_wet_deep')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21b91e64",
      "metadata": {
        "id": "21b91e64"
      },
      "outputs": [],
      "source": [
        "data_soil = pd.merge(\n",
        "    data_soil_shallow,\n",
        "    data_soil_deep,\n",
        "    left_index = True,\n",
        "    right_index = True,\n",
        "    suffixes = (\"_shallow\", \"_deep\"),\n",
        "    how = \"outer\"\n",
        ")\n",
        "\n",
        "del data_soil_shallow, data_soil_deep\n",
        "del united_soil"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47024efe",
      "metadata": {
        "id": "47024efe"
      },
      "source": [
        "## Unite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a80c556f",
      "metadata": {
        "id": "a80c556f"
      },
      "outputs": [],
      "source": [
        "data_united = pd.merge(\n",
        "    data_water,\n",
        "    # REMOVE LATER\n",
        "    data_cal[temp_subset_start:temp_subset_end],\n",
        "    #\n",
        "    left_index = True,\n",
        "    right_index = True,\n",
        "    how = 'outer'\n",
        ")\n",
        "\n",
        "data_united = pd.merge(\n",
        "    data_united,\n",
        "    # REMOVE LATER\n",
        "    data_soil[temp_subset_start:temp_subset_end],\n",
        "    #\n",
        "    left_index = True,\n",
        "    right_index = True,\n",
        "    how = 'outer'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f814e925",
      "metadata": {
        "id": "f814e925"
      },
      "source": [
        "### United features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90f96840",
      "metadata": {
        "id": "90f96840"
      },
      "outputs": [],
      "source": [
        "# Difference compared to calibration point (infrequent)\n",
        "data_united['diff_ro_cal'] = (data_united['weir_level_cal'] - data_united['raw_ro'])\n",
        "\n",
        "# Convert to float\n",
        "data_united['diff_ro_cal'] = data_united['diff_ro_cal'].astype(np.float32)\n",
        "\n",
        "# Time since last calibration point\n",
        "data_united = timesince_feat(data_united, 'weir_level_cal', \"minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1621fcb7",
      "metadata": {
        "id": "1621fcb7"
      },
      "source": [
        "### Temporal features\n",
        "Modify temporal features to be based on sine and cosine transformations, which allows for the model to be based on the cyclical patterns of time rather than abrupt distances\n",
        "\n",
        "(e.g., the raw values Day 365 of the year is 'far' from Day 001, but in reality they are very near)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfbcf31a",
      "metadata": {
        "id": "dfbcf31a"
      },
      "outputs": [],
      "source": [
        "def temporal_feat(input_df, input_unit):\n",
        "    output_df = input_df\n",
        "    if input_unit=='day':\n",
        "        cycle_length = 365.25\n",
        "        value = output_df.index.dayofyear\n",
        "    elif input_unit=='month':\n",
        "        cycle_length = 12\n",
        "        value = output_df.index.month\n",
        "    elif input_unit=='hour':\n",
        "        cycle_length = 24\n",
        "        value = output_df.index.hour\n",
        "    elif input_unit=='minute':\n",
        "        cycle_length = 60\n",
        "        value = output_df.index.minute\n",
        "\n",
        "    output_df[f'{input_unit}_sin'] = np.sin(2 * np.pi * value / cycle_length).astype(np.float32)\n",
        "    output_df[f'{input_unit}_cos'] = np.cos(2 * np.pi * value / cycle_length).astype(np.float32)\n",
        "\n",
        "    return output_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72593493",
      "metadata": {
        "id": "72593493"
      },
      "outputs": [],
      "source": [
        "data_united = temporal_feat(data_united, 'minute')\n",
        "data_united = temporal_feat(data_united, 'hour')\n",
        "data_united = temporal_feat(data_united, 'day')\n",
        "data_united = temporal_feat(data_united, 'month')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a4e0dc4",
      "metadata": {
        "id": "0a4e0dc4"
      },
      "outputs": [],
      "source": [
        "# create features to track soil value staleness\n",
        "cols_soil = [col for col in data_united.columns if (col.endswith('shallow') | col.endswith('deep'))]\n",
        "\n",
        "for col in cols_soil:\n",
        "# for col in data_united.columns:\n",
        "    # if (col.endswith('shallow') | col.endswith('deep')):\n",
        "    # data_united = minsince_feat(data_united, col)\n",
        "    data_united = timesince_feat(data_united, col, \"days\")\n",
        "\n",
        "# Extend soil vals\n",
        "data_united[cols_soil] = data_united[cols_soil].ffill()\n",
        "\n",
        "del col, cols_soil"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd20a0d9",
      "metadata": {
        "id": "bd20a0d9"
      },
      "source": [
        "## Train/Test split\n",
        "\n",
        "80/20 initial split, with expanding sliding window for training/validation for hyperparameters, model stability, and feature selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1741c777",
      "metadata": {
        "id": "1741c777"
      },
      "outputs": [],
      "source": [
        "# REMOVE NAs\n",
        "data_united = data_united.dropna(subset=[var_of_interest])\n",
        "\n",
        "X_all = data_united.drop(var_of_interest, axis=1).copy()\n",
        "y_all = data_united[var_of_interest].copy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix for inferred later\n",
        "# y_all = y_all.astype(bool)\n",
        "y_all = y_all.astype(np.float32)\n",
        "y_all = y_all.as_gpu_object()\n",
        "\n",
        "for col in X_all.columns:\n",
        "  if str(X_all[col].dtype) == ('Int32'):\n",
        "    X_all[col] = X_all[col].astype(np.float32)\n",
        "\n",
        "# print(y_all.__class__)\n",
        "# y_all = cudf.Series.from_pandas(y_all)\n",
        "print(y_all.__class__)\n",
        "X_all.info()"
      ],
      "metadata": {
        "id": "X67HeQIcaYxM"
      },
      "id": "X67HeQIcaYxM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df5ab1ce",
      "metadata": {
        "id": "df5ab1ce"
      },
      "outputs": [],
      "source": [
        "y_len = len(y_all)\n",
        "\n",
        "print(\n",
        "    y_len, \"\\n\",\n",
        "    (round(.2*y_len) + round(.8*y_len)),\n",
        "    \"\\nTrain:\\t80p of \", y_len, \" is \", round(.8*y_len),\n",
        "    \"\\nTest:\\t20p of \", y_len, \" is \", round(.2*y_len),\n",
        "    sep=\"\"\n",
        ")\n",
        "\n",
        "del y_len"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7daf36fd",
      "metadata": {
        "id": "7daf36fd"
      },
      "source": [
        "Unlike the typical approach for train/test splits, temporal data in this context must _not_ be randomly split as it would lead to severe leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e023cfb",
      "metadata": {
        "id": "7e023cfb"
      },
      "outputs": [],
      "source": [
        "# Conduct the split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.2, shuffle=False)\n",
        "# Conduct an inner split for tuning\n",
        "# X_train_inner, X_test_inner, y_train_inner, y_test_inner = train_test_split(X_train, y_train, test_size = 0.2, shuffle=False)\n",
        "\n",
        "# Cleanup\n",
        "del X_all, y_all\n",
        "\n",
        "print(\n",
        "    \"Train:\\t\", len(X_train), \"\\t\", X_train.index[0], \"thru\", X_train.index[-1],\n",
        "    \"\\nTest:\\t\", len(X_test), \"\\t\", X_test.index[0], \"thru\", X_test.index[-1]\n",
        "    # len(x_train), len(x_test), \"\\n\",\n",
        "    # x_train.index[-1]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11921d04",
      "metadata": {
        "id": "11921d04"
      },
      "source": [
        "### Expanding Window\n",
        "\n",
        "For tuning, an expanding window approach will be used. This is similar to how a model would act once deployed, as it will only gain more data over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "661eeac5",
      "metadata": {
        "id": "661eeac5"
      },
      "outputs": [],
      "source": [
        "# Initialize the split function\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "# print(tscv)\n",
        "\n",
        "for i, (train_index, val_index) in enumerate(tscv.split(X_train)):\n",
        "    print(f\"Fold {i}:\")\n",
        "    print(f\"  Train: index={train_index}\")\n",
        "    print(f\"  Test:  index={val_index}\")\n",
        "    # print(\"  Train: index=\", mini_x.index[train_index])\n",
        "    # print(f\"  Test:  index={val_index}\")\n",
        "    print(\"------------------------------------------------------------\")\n",
        "\n",
        "del i, train_index, val_index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c343fa3",
      "metadata": {
        "id": "9c343fa3"
      },
      "source": [
        "## Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a8a3293",
      "metadata": {
        "id": "0a8a3293"
      },
      "source": [
        "As per XGBoosting documentation/tutorials, early stopping with random search for hyperparameter tuning must be iterated upon manually, as `RandomizedSearchCV` does not support using a separate validation set within each CV fold.\n",
        "\n",
        "Source: https://xgboosting.com/xgboost-early-stopping-with-random-search/\n",
        "\n",
        "\n",
        "The area under the precision-recall curve (AUC-PR) can be used to evaluate the performance, since it considers a range of classification thresholds.\n",
        "This is better than an ROC AUC metric since there is greater class imbalance (i.e., `True` is more rare).\n",
        "\n",
        "Source: https://xgboosting.com/evaluate-xgboost-performance-with-precision-recall-curve/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "351db094",
      "metadata": {
        "id": "351db094"
      },
      "outputs": [],
      "source": [
        "# Code modified from\n",
        "# https://xgboosting.com/xgboost-early-stopping-with-random-search/\n",
        "\n",
        "# Define hyperparameter distributions for random search\n",
        "n_est = 1000\n",
        "\n",
        "param_distributions = {\n",
        "    'learning_rate': ('uniform', 0.01, 0.3),\n",
        "    'max_depth': ('choice', [2, 3, 4, 5, 7]),\n",
        "    'subsample': ('uniform', 0.5, 1.0),\n",
        "    'colsample_bytree': ('uniform', 0.4, 1.0),\n",
        "    'scale_pos_weight':('choice', [1, 5, 10, 15, 20]),\n",
        "    'gamma': ('uniform', 0, 0.5),\n",
        "    'reg_alpha': ('uniform', 0, 1.0)\n",
        "}\n",
        "\n",
        "# # Function to sample parameters based on their distribution type\n",
        "# def sample_param(distribution):\n",
        "#     if distribution[0] == 'uniform':\n",
        "#         return uniform(distribution[1], distribution[2] - distribution[1]).rvs()\n",
        "#     elif distribution[0] == 'choice':\n",
        "#         return np.random.choice(distribution[1])\n",
        "#     else:\n",
        "#         raise ValueError(f\"Unsupported distribution type: {distribution[0]}\")\n",
        "\n",
        "\n",
        "# Define seed again\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "# Function to sample parameters based on their distribution type\n",
        "def sample_param(distribution):\n",
        "    if distribution[0] == 'uniform':\n",
        "        # Use seeded generator to create the scipy distribution\n",
        "        return uniform(loc=distribution[1], scale=distribution[2] - distribution[1]).rvs(random_state=rng)\n",
        "    elif distribution[0] == 'choice':\n",
        "        # Use seeded generator choice method\n",
        "        return rng.choice(distribution[1])\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported distribution type: {distribution[0]}\")\n",
        "\n",
        "# Configure CV and early stopping\n",
        "n_splits = 5\n",
        "# early_stopping_rounds = 10\n",
        "early_stopping_rounds = 25\n",
        "\n",
        "# Perform random search with early stopping\n",
        "# n_iterations = 3\n",
        "n_iterations = 50\n",
        "best_params = None\n",
        "best_score = 0\n",
        "best_avg_rounds = 0\n",
        "\n",
        "for _ in range(n_iterations):\n",
        "    test_scores = []\n",
        "    best_rounds = []\n",
        "    optimal_rounds_list = []\n",
        "    params = {k: sample_param(v) for k, v in param_distributions.items()}\n",
        "\n",
        "    for train_index, test_index in tscv.split(X_train):\n",
        "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
        "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
        "\n",
        "        # Split train set into train and validation\n",
        "        X_train_fold, X_val, y_train_fold, y_val = train_test_split(X_train_fold, y_train_fold, test_size=0.2, shuffle=False)\n",
        "\n",
        "        # # Prep hyperparam\n",
        "        # neg_count_fold = (y_train_fold == 0).sum()\n",
        "        # pos_count_fold = (y_train_fold == 1).sum()\n",
        "\n",
        "        # Prepare the model\n",
        "        model = xgb.XGBClassifier(\n",
        "            n_estimators=n_est,\n",
        "            learning_rate=params['learning_rate'],\n",
        "            max_depth=int(params['max_depth']),  # max_depth should be an int\n",
        "            subsample=params['subsample'],\n",
        "            colsample_bytree=params['colsample_bytree'],\n",
        "            objective='binary:logistic',\n",
        "            random_state=42,\n",
        "            ## SETTINGS FOR GPU\n",
        "            seed_per_iteration = True,\n",
        "            tree_method='hist',\n",
        "            device='cuda',\n",
        "            # scale_pos_weight= neg_count_fold/pos_count_fold,\n",
        "            scale_pos_weight=params['scale_pos_weight'],\n",
        "            gamma=params['gamma'],\n",
        "            eval_metric='aucpr',\n",
        "            reg_alpha=params['reg_alpha'],\n",
        "            # disable_default_eval_metric=True,\n",
        "            ##\n",
        "            n_jobs=-1,\n",
        "            early_stopping_rounds=early_stopping_rounds # fixed early stopping\n",
        "        )\n",
        "\n",
        "        # Fit model on train fold and use validation for early stopping\n",
        "        model.fit(X_train_fold, y_train_fold, eval_set=[(X_val, y_val)], verbose=False)\n",
        "\n",
        "        # Find optimal number of iterations\n",
        "        optimal_rounds_list.append(model.best_iteration)\n",
        "\n",
        "        # # Predict on test set\n",
        "        # y_pred_test = model.predict(X_test_fold)\n",
        "        # # test_score = accuracy_score(y_test_fold, y_pred_test)\n",
        "        # # test_score = f1_score(y_test_fold, y_pred_test)\n",
        "        # test_score = f1_score(y_test_fold.to_cupy().get(), y_pred_test)\n",
        "        # test_scores.append(test_score)\n",
        "\n",
        "        # ## UDPATE -- using ROC AUC\n",
        "        # y_pred_test = model.predict_proba(X_test_fold)[:,1]\n",
        "        # # y_pred_test = y_pred_test.to_cupy()\n",
        "        # # y_pred_test = cp.array(y_pred_test)\n",
        "        # y_pred_test = cudf.Series(y_pred_test)\n",
        "        # # test_score = roc_auc_score(y_test_fold, y_pred_test)\n",
        "        # test_score = cuml.metrics.roc_auc_score(y_test_fold, y_pred_test)\n",
        "        # test_scores.append(test_score)\n",
        "        # ##\n",
        "\n",
        "        ## Using AUC PR\n",
        "        # Predict on test set\n",
        "        y_pred_test = model.predict(X_test_fold)\n",
        "        # test_score = accuracy_score(y_test_fold, y_pred_test)\n",
        "        # test_score = f1_score(y_test_fold, y_pred_test)\n",
        "        prec, rec, _ = precision_recall_curve(y_test_fold.to_cupy().get(), y_pred_test)\n",
        "        test_score = auc(rec, prec)\n",
        "        test_scores.append(test_score)\n",
        "\n",
        "    # Compute average score across all folds\n",
        "    average_score = np.mean(test_scores)\n",
        "    average_optimal_rounds = np.mean(optimal_rounds_list)\n",
        "\n",
        "    if average_score > best_score:\n",
        "        best_score = average_score\n",
        "        best_params = params\n",
        "        best_avg_rounds = int(round(average_optimal_rounds)) # Store the integer average\n",
        "        ## Maybe??\n",
        "        # best_f1 = test_score\n",
        "        # best_model = model\n",
        "\n",
        "\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "# print(f\"Best CV Average Accuracy: {best_score}\")\n",
        "# print(f\"CV Average ROC AUC: {best_score:.4f}\")\n",
        "print(f\"CV Average: {average_score:.4f}\")\n",
        "print(f\"Best Avg Rounds: {best_avg_rounds}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5f41280",
      "metadata": {
        "id": "e5f41280"
      },
      "outputs": [],
      "source": [
        "# _ = playsound(get_path('completed.mp3', 'code'), block=False)\n",
        "play_chime()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"Best ROC AUC: {test_score:.4f}\")\n",
        "del X_train_fold, X_val, y_train_fold, y_val\n",
        "# del y_pred_test, test_score\n",
        "del y_pred_test\n",
        "del n_splits, early_stopping_rounds, n_iterations, test_scores, best_rounds, optimal_rounds_list\n",
        "# del neg_count_fold, pos_count_fold\n",
        "del average_score, average_optimal_rounds"
      ],
      "metadata": {
        "id": "EE6EUGQHjU-3"
      },
      "id": "EE6EUGQHjU-3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model"
      ],
      "metadata": {
        "id": "LCwADUVwdHvc"
      },
      "id": "LCwADUVwdHvc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4cd5517",
      "metadata": {
        "id": "e4cd5517"
      },
      "outputs": [],
      "source": [
        "model_name = \"rs_xgb_cpu_rocauc_tt_full_aucrc_regz_iter\"\n",
        "\n",
        "if os.path.exists(model_path(model_name)) == False:\n",
        "    print(\"Fitting final model...\")\n",
        "\n",
        "    # XGBoost requires int for certain params\n",
        "    # best_params['max_depth'] = int(best_params['max_depth'])\n",
        "\n",
        "    # # Prep hyperparam\n",
        "    # neg_count_it = (y_train_inner == 0).sum()\n",
        "    # pos_count_it = (y_train_inner == 1).sum()\n",
        "\n",
        "    final_model = xgb.XGBClassifier(\n",
        "        n_estimators=best_avg_rounds,\n",
        "        # n_estimators=100, # avg optimal n_estimators if known, or reasonable default\n",
        "        learning_rate=best_params['learning_rate'],\n",
        "        max_depth=int(best_params['max_depth']),\n",
        "        subsample=best_params['subsample'],\n",
        "        colsample_bytree=best_params['colsample_bytree'],\n",
        "        objective='binary:logistic',\n",
        "        random_state=42,\n",
        "        ## SETTINGS FOR GPU\n",
        "        seed_per_iteration = True,\n",
        "        tree_method='hist',\n",
        "        device='cuda',\n",
        "        scale_pos_weight=best_params['scale_pos_weight'],\n",
        "        gamma=best_params['gamma'],\n",
        "        reg_alpha=best_params['reg_alpha'],\n",
        "        ##\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # final_model.fit(X_train_inner, y_train_inner)\n",
        "    joblib.dump(final_model, model_path(model_name))\n",
        "\n",
        "    # del neg_count_it, pos_count_it\n",
        "\n",
        "    # Local download\n",
        "    from google.colab import files\n",
        "    files.download(model_path(model_name))\n",
        "\n",
        "else:\n",
        "    print(\"Importing model from saved files...\")\n",
        "    final_model = joblib.load(model_path(model_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OOF Predictions"
      ],
      "metadata": {
        "id": "g1eAAvS2Zup3"
      },
      "id": "g1eAAvS2Zup3"
    },
    {
      "cell_type": "code",
      "source": [
        "# oof_pred = []\n",
        "from sklearn.base import clone\n",
        "\n",
        "oof_pred = np.full(len(y_train), np.nan)\n",
        "\n",
        "for train_index, test_index in tscv.split(X_train):\n",
        "  X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
        "  y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
        "\n",
        "  # Prepare the model\n",
        "  # model = final_model.copy()\n",
        "  model = clone(final_model)\n",
        "\n",
        "  model.fit(X_train_fold, y_train_fold)\n",
        "  # model.pred_proba(X_test_fold, y_test_fold)\n",
        "  y_pred_test = model.predict_proba(X_test_fold)[:,1]\n",
        "  # oof_pred.update(y_pred_test, index=test_index)\n",
        "  oof_pred[test_index] = y_pred_test\n",
        "\n",
        "valid_mask = ~np.isnan(oof_pred)\n",
        "y_train_oof = y_train.values[valid_mask]\n",
        "oof_pred = oof_pred[valid_mask]\n",
        "\n",
        "del model, X_train_fold, y_train_fold, X_test_fold, y_test_fold, y_pred_test\n",
        "del valid_mask\n",
        "# final_model.fit(X_train_inner, y_train_inner)"
      ],
      "metadata": {
        "id": "5EULm8GkZuTU"
      },
      "id": "5EULm8GkZuTU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Smoothing & Thresholding\n",
        "\n",
        "Windowing can help smooth predictions by preventing standalone points that differ from their neighbors (e.g., having a sequence of `True` interrupted by one `False`, or vice-versa, both of which are unlikely in this context due to how weir blockages occur).\n",
        "\n",
        "By default, a threshold of 0.5 will be selected for categorizing a point as `True` or `False`. However, in this context a model more sensitive to `True` may make final results more accurate.\n",
        "\n",
        "These measures can be found by finding the window and corresponding threshold that maximizes the F1 score."
      ],
      "metadata": {
        "id": "RRLWFSfdg4ph"
      },
      "id": "RRLWFSfdg4ph"
    },
    {
      "cell_type": "code",
      "source": [
        "def smooth_window(probs, window_size=3):\n",
        "    return pd.Series(probs).rolling(window=window_size, min_periods=1, center=True).mean().values"
      ],
      "metadata": {
        "id": "MALjzPT9g_we"
      },
      "id": "MALjzPT9g_we",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_cust(input_true, input_pred):\n",
        "  output_true = input_true.copy()\n",
        "  output_pred = input_pred.copy()\n",
        "  if not isinstance(output_true, np.ndarray):\n",
        "    output_true = output_true.get()\n",
        "  if not isinstance(output_pred, np.ndarray):\n",
        "    output_pred = output_pred.get()\n",
        "\n",
        "  output_f1 = f1_score(output_true, output_pred)\n",
        "  # prec, rec, _ = precision_recall_curve(output_true, output_pred)\n",
        "  # output_f1 = auc(rec, prec)\n",
        "  return output_f1"
      ],
      "metadata": {
        "id": "AOb3QdMGwT0o"
      },
      "id": "AOb3QdMGwT0o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prec, rec, _ = precision_recall_curve(y_train_oof.get(), oof_pred)\n",
        "# # auc(rec, prec)\n",
        "# print(\n",
        "#     best_score,\n",
        "#     auc(rec, prec)\n",
        "#   )"
      ],
      "metadata": {
        "id": "1x4rieKRYJVt"
      },
      "id": "1x4rieKRYJVt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.signal import medfilt\n"
      ],
      "metadata": {
        "id": "Q54JgJc8baj9"
      },
      "id": "Q54JgJc8baj9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start w no smoothing\n",
        "best_window_size = 1\n",
        "# Start w default threshold\n",
        "threshold = 0.5\n",
        "\n",
        "y_bin = (oof_pred >= threshold).astype(np.int32)\n",
        "# y_bin = cp.array(y_bin)\n",
        "best_f1 = f1_cust(y_train_oof, y_bin)\n",
        "# print(f\"No smoothing, 0.5 threshold:\\t{best_f1:.4f}\")\n",
        "\n",
        "best_threshold = threshold\n",
        "thresholds = np.linspace(0.01, 0.99, 100)\n",
        "# thresholds = np.linspace(0.01, 0.99, 10)\n",
        "\n",
        "# Smallest window size\n",
        "window_min = 1\n",
        "# Largest window size\n",
        "window_max = 35\n",
        "# window_max = 10\n",
        "\n",
        "print(f\"Window 1\\tCurr best: W:1\\tT:{best_threshold}\\t\\tF1:{best_f1:.4}\")\n",
        "\n",
        "# Test range of odd window sizes\n",
        "for current_window in range(window_min, window_max+1, 2):\n",
        "    # print(\"Testing window =\", current_window)\n",
        "    # smoothed_preds = smooth_window(oof_pred, window_size=current_window)\n",
        "    smoothed_preds = medfilt(oof_pred, kernel_size=current_window)\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        y_bin = (smoothed_preds >= threshold).astype(np.int32)\n",
        "        current_f1 = f1_cust(y_train_oof, y_bin)\n",
        "        # print(f\"{threshold:.4}\\t{current_f1:.5}\")\n",
        "\n",
        "        if current_f1 > best_f1:\n",
        "          best_f1 = current_f1\n",
        "          # print(f\"New best F1:{best_f1:.4}\")\n",
        "          best_window = current_window\n",
        "          best_threshold = threshold\n",
        "    # print(f\"Current best F1:{best_f1:.4}\")\n",
        "    print(f\"Window {current_window}\\tCurr best: W:{best_window}\\tT:{best_threshold:.4}\\tF1:{best_f1:.4}\")\n",
        "\n",
        "# oof_pred_adj = smooth_window(oof_pred, window_size=best_window)\n",
        "oof_pred_adj = medfilt(oof_pred, kernel_size=best_window)\n",
        "oof_pred_adj = (oof_pred_adj >= best_threshold).astype(int)\n",
        "\n",
        "f1_rez = f1_cust(y_train_oof, oof_pred_adj)\n",
        "print(f\"Window {best_window}, threshold {best_threshold:.4f}:\\t{f1_rez:.4f}\")\n",
        "\n",
        "del current_f1, current_window, threshold, f1_rez, window_min, window_max, y_bin, best_f1"
      ],
      "metadata": {
        "id": "dOOgqbxMbQ4N"
      },
      "id": "dOOgqbxMbQ4N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start w no smoothing\n",
        "best_window_size = 1\n",
        "# Start w default threshold\n",
        "threshold = 0.5\n",
        "\n",
        "y_bin = (oof_pred >= threshold).astype(np.int32)\n",
        "# y_bin = cp.array(y_bin)\n",
        "best_f1 = f1_cust(y_train_oof, y_bin)\n",
        "# print(f\"No smoothing, 0.5 threshold:\\t{best_f1:.4f}\")\n",
        "\n",
        "best_threshold = threshold\n",
        "thresholds = np.linspace(0.01, 0.99, 100)\n",
        "# thresholds = np.linspace(0.01, 0.99, 10)\n",
        "\n",
        "# Smallest window size\n",
        "window_min = 1\n",
        "# Largest window size\n",
        "window_max = 35\n",
        "# window_max = 10\n",
        "\n",
        "print(f\"Window 1\\tCurr best: W:1\\tT:{best_threshold}\\t\\tF1:{best_f1:.4}\")\n",
        "\n",
        "# Test range of odd window sizes\n",
        "for current_window in range(window_min, window_max+1, 2):\n",
        "    # print(\"Testing window =\", current_window)\n",
        "    smoothed_preds = smooth_window(oof_pred, window_size=current_window)\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        y_bin = (smoothed_preds >= threshold).astype(np.int32)\n",
        "        current_f1 = f1_cust(y_train_oof, y_bin)\n",
        "        # print(f\"{threshold:.4}\\t{current_f1:.5}\")\n",
        "\n",
        "        if current_f1 > best_f1:\n",
        "          best_f1 = current_f1\n",
        "          # print(f\"New best F1:{best_f1:.4}\")\n",
        "          best_window = current_window\n",
        "          best_threshold = threshold\n",
        "    # print(f\"Current best F1:{best_f1:.4}\")\n",
        "    print(f\"Window {current_window}\\tCurr best: W:{best_window}\\tT:{best_threshold:.4}\\tF1:{best_f1:.4}\")\n",
        "\n",
        "oof_pred_adj = smooth_window(oof_pred, window_size=best_window)\n",
        "oof_pred_adj = (oof_pred_adj >= best_threshold).astype(int)\n",
        "\n",
        "f1_rez = f1_cust(y_train_oof, oof_pred_adj)\n",
        "print(f\"Window {best_window}, threshold {best_threshold:.4f}:\\t{f1_rez:.4f}\")\n",
        "\n",
        "del current_f1, current_window, threshold, f1_rez, window_min, window_max, y_bin, best_f1"
      ],
      "metadata": {
        "id": "nzBsaP7chJaL"
      },
      "id": "nzBsaP7chJaL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Threshold"
      ],
      "metadata": {
        "id": "OIHYWuCTdpnk"
      },
      "id": "OIHYWuCTdpnk"
    },
    {
      "cell_type": "code",
      "source": [
        "# threshold = 0.5\n",
        "# y_bin = (oof_pred >= threshold).astype(np.int32)\n",
        "# best_f1 = f1_score(y_train_oof, y_bin)\n",
        "# print(f\"F1 at default 0.5 threshold: {best_f1:.4f}\")\n",
        "\n",
        "# best_threshold = 0\n",
        "# thresholds = np.linspace(0.01, 0.99, 100)\n",
        "\n",
        "# for threshold in thresholds:\n",
        "#     y_bin = (y_proba >= threshold).astype(np.int32)\n",
        "#     current_f1 = f1_score(y_train_oof, y_bin)\n",
        "\n",
        "#     if current_f1 > best_f1:\n",
        "#         best_f1 = current_f1\n",
        "#         best_threshold = threshold\n",
        "\n",
        "# print(f\"Optimal threshold:\\t{best_threshold:.4f}\")\n",
        "# print(f\"Threshold F1:\\t{best_f1:.4f}\")"
      ],
      "metadata": {
        "id": "gsyUqcVJP5t4"
      },
      "id": "gsyUqcVJP5t4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "701b4a53",
      "metadata": {
        "id": "701b4a53",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "# X_itest_mini, X_holdout, y_itest_mini, y_holdout = train_test_split(X_test_inner, y_test_inner, test_size=0.2, random_state=42, shuffle=False)\n",
        "\n",
        "# t_tuner = TunedThresholdClassifierCV(\n",
        "#     estimator=final_model,\n",
        "#     scoring=make_scorer(f1_score),\n",
        "#     cv=\"prefit\",\n",
        "#     thresholds=100,\n",
        "#     refit=False,\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "\n",
        "# t_tuner.fit(X_itest_mini, y_itest_mini)\n",
        "\n",
        "# print(\n",
        "#     \"Threshold:\", t_tuner.best_threshold_,\n",
        "#     \"F1:\", t_tuner.best_score_\n",
        "# )\n",
        "\n",
        "# t_x_pred = t_tuner.predict(X_holdout)\n",
        "\n",
        "# print(\n",
        "#     f1_score(y_holdout, t_x_pred),\n",
        "#     accuracy_score(y_holdout, t_x_pred),\n",
        "#     precision_score(y_holdout, t_x_pred),\n",
        "#     recall_score(y_holdout, t_x_pred),\n",
        "#     sep=\"\\n\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# # Thresholds\n",
        "# X_itest_train, X_itest_test, y_itest_train, y_itest_test = train_test_split(X_test_inner, y_test_inner, test_size=0.2, shuffle=False)\n",
        "\n",
        "# threshold = 0.5\n",
        "# y_proba = final_model.predict_proba(X_itest_train)[:,1]\n",
        "# y_bin = (y_proba >= threshold).astype(np.int32)\n",
        "# best_f1 = f1_score(y_itest_train, y_bin)\n",
        "# print(f\"F1 at default 0.5 threshold: {best_f1:.4f}\")\n",
        "\n",
        "# best_threshold = 0\n",
        "# thresholds = np.linspace(0.01, 0.99, 100)\n",
        "\n",
        "# for threshold in thresholds:\n",
        "#     y_bin = (y_proba >= threshold).astype(np.int32)\n",
        "#     current_f1 = f1_score(y_itest_train, y_bin)\n",
        "\n",
        "#     if current_f1 > best_f1:\n",
        "#         best_f1 = current_f1\n",
        "#         best_threshold = threshold\n",
        "\n",
        "# print(f\"Optimal threshold:\\t{best_threshold:.4f}\")\n",
        "# print(f\"Threshold F1:\\t{best_f1:.4f}\")\n",
        "\n",
        "# y_proba_test = final_model.predict_proba(X_itest_test)[:,1]\n",
        "# final_predictions = (y_proba_test >= best_threshold).astype(np.int32)\n",
        "\n",
        "# print(\n",
        "#     \"\\n--- Metrics on Holdout Set (using optimal threshold) ---\",\n",
        "#     f\"F1:\\t{f1_score(y_itest_test, final_predictions):.4f}\",\n",
        "#     f\"Acc:\\t{accuracy_score(y_itest_test, final_predictions):.4f}\",\n",
        "#     f\"Pre:\\t{precision_score(y_itest_test, final_predictions):.4f}\",\n",
        "#     f\"Rec:\\t{recall_score(y_itest_test, final_predictions):.4f}\",\n",
        "#     sep=\"\\n\"\n",
        "# )\n",
        "# # print(f\"F1:\\t{f1_score(y_itest_test, final_predictions):.4f}\")\n",
        "# # print(f\"Acc:\\t{accuracy_score(y_itest_test, final_predictions):.4f}\")\n",
        "# # print(f\"Pre:\\t{precision_score(y_itest_test, final_predictions):.4f}\")\n",
        "# # print(f\"Rec:\\t{recall_score(y_itest_test, final_predictions):.4f}\")\n",
        "\n",
        "# del y_proba, threshold, thresholds, best_f1\n",
        "# del X_itest_train, y_itest_train"
      ],
      "metadata": {
        "id": "GZXN5p8gg1RN",
        "cellView": "form"
      },
      "id": "GZXN5p8gg1RN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e734f9b6",
      "metadata": {
        "id": "e734f9b6"
      },
      "source": [
        "## Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To smooth the post-processing results, windows can be tested to determine the optimal majority-vote,\n",
        "\n",
        "This function will run on CPU."
      ],
      "metadata": {
        "id": "kGoWSrififBU"
      },
      "id": "kGoWSrififBU"
    },
    {
      "cell_type": "code",
      "source": [
        "# def apply_smoothing(predictions_array, window_size=5):\n",
        "#     predictions_series = pd.Series(predictions_array)\n",
        "#     smoothed = predictions_series.rolling(\n",
        "#         window=window_size,\n",
        "#         center=True,\n",
        "#         min_periods=1\n",
        "#     ).apply(lambda x: np.bincount(x.astype(int)).argmax(), raw=False).astype(int)\n",
        "#     return smoothed.values\n",
        "\n",
        "\n",
        "# # Smallest window size\n",
        "# window_min = 3\n",
        "# # Largest window size\n",
        "# window_max = 25\n",
        "\n",
        "# # Start w no smoothing\n",
        "# best_window_size = 1\n",
        "# best_smoothing_f1 = f1_score(y_itest_test, final_predictions)\n",
        "# # print(\"No window F1:\\t\", best_smoothing_f1)\n",
        "# print(f\"No window F1:\\t{best_smoothing_f1:.4f}\")\n",
        "\n",
        "# # Test range of odd window sizes\n",
        "# for window_size in range(window_min, window_max+1, 2):\n",
        "#     # print(\"Testing window =\", window_size)\n",
        "#     smoothed_preds = apply_smoothing(final_predictions, window_size=window_size)\n",
        "#     f1 = f1_score(y_itest_test, smoothed_preds)\n",
        "#     # print(\"Window\", window_size, \"F1:\\t\", f1)\n",
        "#     print(f\"Window {window_size} F1:\\t{f1:.4f}\")\n",
        "\n",
        "#     if f1 > best_smoothing_f1:\n",
        "#         # print(\"F1 improved with smoothing!\")\n",
        "#         best_smoothing_f1 = f1\n",
        "#         best_window_size = window_size\n",
        "\n",
        "# print(f\"Optimal window size: {best_window_size}\")\n",
        "# print(f\"Best F1: {best_smoothing_f1:.4f}\")\n",
        "\n",
        "# del window_min, window_max, f1, window_size"
      ],
      "metadata": {
        "id": "CGP2sDpCgE4M"
      },
      "id": "CGP2sDpCgE4M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b82de6d5",
      "metadata": {
        "id": "b82de6d5"
      },
      "outputs": [],
      "source": [
        "# def apply_smoothing(predictions_array, window_size=5):\n",
        "#     predictions_series = pd.Series(predictions_array)\n",
        "#     smoothed = predictions_series.rolling(\n",
        "#         window=window_size,\n",
        "#         center=True,\n",
        "#         min_periods=1\n",
        "#     ).apply(lambda x: np.bincount(x.astype(int)).argmax(), raw=False).astype(int)\n",
        "#     return smoothed.values\n",
        "\n",
        "# # def apply_smoothing(predictions_array, window_size=5):\n",
        "# #     # 1. Convert NumPy array to CuPy array (if it isn't already on the GPU)\n",
        "# #     predictions_cp = cp.asarray(predictions_array)\n",
        "\n",
        "# #     # 2. Convert CuPy array to cuDF Series\n",
        "# #     predictions_series_gpu = cudf.Series(predictions_cp)\n",
        "\n",
        "# #     # Define the rolling operation\n",
        "# #     smoothed_gpu = predictions_series_gpu.rolling(\n",
        "# #         window=window_size,\n",
        "# #         center=True,\n",
        "# #         min_periods=1\n",
        "# #     )\n",
        "\n",
        "# #     # 3. Apply the majority vote function using CuPy operations within a custom apply\n",
        "# #     # We define a custom function using cupy's bincount\n",
        "# #     def majority_vote_gpu(x):\n",
        "# #         # Convert to int, count occurrences, and find the index of the max count (argmax)\n",
        "# #         return cp.bincount(x.astype(cp.int32)).argmax()\n",
        "\n",
        "# #     # Apply the function\n",
        "# #     # Note: cuDF's rolling apply works best with simple cupy aggregations.\n",
        "# #     # For complex custom lambdas like this, performance may vary,\n",
        "# #     # but it keeps the data on the GPU.\n",
        "# #     smoothed_gpu = smoothed_gpu.apply(majority_vote_gpu)\n",
        "\n",
        "# #     # 4. Convert back to CPU NumPy array for final use if needed (optional)\n",
        "# #     return smoothed_gpu.values.get() # .get() moves data from GPU (CuPy array) back to CPU (NumPy array)\n",
        "# #     # return smoothed_gpu.values # Keep as CuPy array on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "079ec9bc",
      "metadata": {
        "id": "079ec9bc"
      },
      "outputs": [],
      "source": [
        "# # Smallest window size\n",
        "# window_min = 3\n",
        "# # Largest window size\n",
        "# window_max = 25\n",
        "\n",
        "# # Start w no smoothing\n",
        "# best_window_size = 1\n",
        "# best_smoothing_f1 = f1_score(y_itest_test, final_predictions)\n",
        "# # print(\"No window F1:\\t\", best_smoothing_f1)\n",
        "# print(f\"No window F1:\\t{best_smoothing_f1:.4f}\")\n",
        "\n",
        "# # Test range of odd window sizes\n",
        "# for window_size in range(window_min, window_max+1, 2):\n",
        "#     # print(\"Testing window =\", window_size)\n",
        "#     smoothed_preds = apply_smoothing(final_predictions, window_size=window_size)\n",
        "#     f1 = f1_score(y_itest_test, smoothed_preds)\n",
        "#     # print(\"Window\", window_size, \"F1:\\t\", f1)\n",
        "#     print(f\"Window {window_size} F1:\\t{f1:.4f}\")\n",
        "\n",
        "#     if f1 > best_smoothing_f1:\n",
        "#         # print(\"F1 improved with smoothing!\")\n",
        "#         best_smoothing_f1 = f1\n",
        "#         best_window_size = window_size\n",
        "\n",
        "# print(f\"Optimal window size: {best_window_size}\")\n",
        "# print(f\"Best F1: {best_smoothing_f1:.4f}\")\n",
        "\n",
        "# del window_min, window_max, f1, window_size"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying to test set"
      ],
      "metadata": {
        "id": "YIouoQwgij8-"
      },
      "id": "YIouoQwgij8-"
    },
    {
      "cell_type": "code",
      "source": [
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "final_pred_y = final_model.predict_proba(X_test)[:,1]\n",
        "\n",
        "final_pred_y_base = (final_pred_y >= 0.5).astype(np.int32)\n",
        "\n",
        "y_test_conv = y_test.copy().to_cupy()\n",
        "y_test_conv = y_test_conv.get()\n",
        "\n",
        "\n",
        "print(\"Base model (0.5 threshold, no smoothing)\")\n",
        "\n",
        "print(\n",
        "    f\"F1:\\t{f1_score(y_test_conv, final_pred_y_base):.4f}\",\n",
        "    f\"Acc:\\t{accuracy_score(y_test_conv, final_pred_y_base):.4f}\",\n",
        "    f\"Pre:\\t{precision_score(y_test_conv, final_pred_y_base):.4f}\",\n",
        "    f\"Rec:\\t{recall_score(y_test_conv, final_pred_y_base):.4f}\",\n",
        "    \"-----------------------------------\",\n",
        "    sep=\"\\n\"\n",
        ")\n",
        "\n",
        "# t_x_pred_f = t_tuner.predict(X_test)\n",
        "print(\"Windowed with\", best_window)\n",
        "\n",
        "final_pred_y_win = smooth_window(final_pred_y, window_size=best_window)\n",
        "final_pred_y_win = (final_pred_y_win >= 0.5).astype(np.int32)\n",
        "\n",
        "print(\n",
        "    f\"F1:\\t{f1_score(y_test_conv, final_pred_y_win):.4f}\",\n",
        "    f\"Acc:\\t{accuracy_score(y_test_conv, final_pred_y_win):.4f}\",\n",
        "    f\"Pre:\\t{precision_score(y_test_conv, final_pred_y_win):.4f}\",\n",
        "    f\"Rec:\\t{recall_score(y_test_conv, final_pred_y_win):.4f}\",\n",
        "    \"-----------------------------------\",\n",
        "    sep=\"\\n\"\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"Optmized threshold of {best_threshold:.4f}\")\n",
        "\n",
        "# final_pred_y = (final_pred_y >= best_threshold).astype(np.int32)\n",
        "final_pred_y_opt = smooth_window(final_pred_y, window_size=best_window)\n",
        "final_pred_y_opt = (final_pred_y_opt >= best_threshold).astype(np.int32)\n",
        "\n",
        "print(\n",
        "    f\"F1:\\t{f1_score(y_test_conv, final_pred_y_opt):.4f}\",\n",
        "    f\"Acc:\\t{accuracy_score(y_test_conv, final_pred_y_opt):.4f}\",\n",
        "    f\"Pre:\\t{precision_score(y_test_conv, final_pred_y_opt):.4f}\",\n",
        "    f\"Rec:\\t{recall_score(y_test_conv, final_pred_y_opt):.4f}\",\n",
        "    \"-----------------------------------\",\n",
        "    sep=\"\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "kO7qTt_m1IuZ"
      },
      "id": "kO7qTt_m1IuZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_chime()"
      ],
      "metadata": {
        "id": "8JC3K93z368_"
      },
      "id": "8JC3K93z368_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4d614dcc",
      "metadata": {
        "id": "4d614dcc"
      },
      "source": [
        "## Feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ae4e7c0",
      "metadata": {
        "id": "3ae4e7c0"
      },
      "outputs": [],
      "source": [
        "feature_importances = final_model.feature_importances_\n",
        "# map scores to feature names\n",
        "# feature_importances\n",
        "feature_names = X_train.columns.tolist()\n",
        "\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'feat': feature_names,\n",
        "    'importance': feature_importances\n",
        "})\n",
        "\n",
        "# sort importance\n",
        "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
        "\n",
        "# print(feature_importance_df)\n",
        "feature_importance_df\n",
        "\n",
        "# most important features\n",
        "# print(feature_importance_df.head(25))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd348692",
      "metadata": {
        "id": "fd348692"
      },
      "outputs": [],
      "source": [
        "threshold_importance = 0.95\n",
        "# calculate most important 90 percent of the importance\n",
        "feature_importance_df['cumulative_imp'] = feature_importance_df['importance'].cumsum()\n",
        "features_percent = feature_importance_df[feature_importance_df['cumulative_imp'] <= threshold_importance].shape[0] + 1\n",
        "features_percent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4517ba9a",
      "metadata": {
        "id": "4517ba9a"
      },
      "outputs": [],
      "source": [
        "# Most important features:\n",
        "print(round(threshold_importance*100), \"% (most important features):\", features_percent)\n",
        "feature_importance_df.head(features_percent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f820c002",
      "metadata": {
        "id": "f820c002"
      },
      "outputs": [],
      "source": [
        "# feature_importance_df.tail(1)\n",
        "# Least important features:\n",
        "print(\"Remaining\", round((1-threshold_importance)*100), \"% (least important features):\", len(feature_names)-features_percent)\n",
        "feature_importance_df.tail(len(feature_names)-features_percent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1eb8627",
      "metadata": {
        "id": "a1eb8627"
      },
      "outputs": [],
      "source": [
        "# Features with 0 importance:\n",
        "print(\"Features with 0 importance:\", len(feature_importance_df[feature_importance_df['importance']==0]))\n",
        "\n",
        "feature_importance_df[feature_importance_df['importance']==0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a0dc876",
      "metadata": {
        "id": "1a0dc876"
      },
      "source": [
        "Feature importance by type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d908cbbe",
      "metadata": {
        "id": "d908cbbe"
      },
      "outputs": [],
      "source": [
        "mapping_dict = {\n",
        "    'soil': '_deep|_shallow',\n",
        "    'runoff':'ro',\n",
        "    'rain':'rain',\n",
        "    'calibration':'_cal'\n",
        "}\n",
        "\n",
        "for col_name, pattern in mapping_dict.items():\n",
        "    feature_importance_df[col_name] = feature_importance_df['feat'].str.contains(pattern, case=False, regex=True)\n",
        "\n",
        "feature_importance_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31c6b4b8",
      "metadata": {
        "id": "31c6b4b8"
      },
      "outputs": [],
      "source": [
        "feature_importance_df['most'] = (feature_importance_df['cumulative_imp'] <= threshold_importance)\n",
        "feature_importance_df['zero'] = (feature_importance_df['importance'] == 0)\n",
        "\n",
        "cat_cols = list(mapping_dict.keys())\n",
        "\n",
        "table_feature_cat_importance = pd.DataFrame({\n",
        "    'Total features': feature_importance_df[cat_cols].sum(),\n",
        "    'Above threshold': feature_importance_df[feature_importance_df['most']][cat_cols].sum(),\n",
        "    'Below threshold': feature_importance_df[~feature_importance_df['most']][cat_cols].sum(),\n",
        "    'Zero importance': feature_importance_df[feature_importance_df['zero']][cat_cols].sum()\n",
        "}).fillna(0).astype(int)\n",
        "\n",
        "del cat_cols, mapping_dict\n",
        "\n",
        "table_feature_cat_importance.index.name = 'Category'\n",
        "\n",
        "table_feature_cat_importance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64fc7875",
      "metadata": {
        "id": "64fc7875"
      },
      "source": [
        "## Classification Threshold\n",
        "\n",
        "By default, a threshold of 0.5 will be selected for categorizing a point as `True` or `False`. However, in this context a model more sensitive to `True` may make final results more accurate.\n",
        "\n",
        "This measure can be found by finding the threshold that maximizes the F1 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6851e234",
      "metadata": {
        "id": "6851e234"
      },
      "outputs": [],
      "source": [
        "# classifier_tuned = TunedThresholdClassifierCV(best_model, scoring=\"balanced_accuracy\").fit(X_train_two, y_train_two)\n",
        "# print(f\"Cut-off point found at {classifier_tuned.best_threshold_:.3f}\")\n",
        "t_tuner = TunedThresholdClassifierCV(\n",
        "    estimator=best_model,\n",
        "    scoring=make_scorer(f1_score),\n",
        "    cv=\"prefit\",\n",
        "    thresholds=100,\n",
        "    refit=False\n",
        ")\n",
        "\n",
        "t_tuner.fit(X_test_inner, y_test_inner)\n",
        "\n",
        "print(\n",
        "    \"Threshold:\", t_tuner.best_threshold_,\n",
        "    \"F1:\", t_tuner.best_score_\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a31854c6",
      "metadata": {
        "id": "a31854c6"
      },
      "outputs": [],
      "source": [
        "t_x_pred = t_tuner.predict(X_test_inner)\n",
        "\n",
        "print(\n",
        "    f1_score(y_test_inner, t_x_pred),\n",
        "    accuracy_score(y_test_inner, t_x_pred),\n",
        "    precision_score(y_test_inner, t_x_pred),\n",
        "    recall_score(y_test_inner, t_x_pred),\n",
        "    sep=\"\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "349fefba",
      "metadata": {
        "id": "349fefba"
      },
      "outputs": [],
      "source": [
        "_ = playsound(get_path('completed.mp3', 'code'), block=False)\n",
        "# X_train_two, X_val, y_train_two, y_val = train_test_split(X_train, y_train, test_size = 0.2, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41ff6a31",
      "metadata": {
        "id": "41ff6a31"
      },
      "outputs": [],
      "source": [
        "all_f1 = []\n",
        "all_accuracy = []\n",
        "all_precision = []\n",
        "all_recall = []\n",
        "optimal_thresholds = []\n",
        "performance_df = {}\n",
        "# performance_df = DataFrame()\n",
        "\n",
        "for fold, (index_train, index_val) in enumerate(tscv.split(X_train)):\n",
        "    print(\"Split\", fold)\n",
        "    X_sub_train, X_sub_val = X_train.iloc[index_train].copy(), X_train.iloc[index_val].copy()\n",
        "    y_sub_train, y_sub_val = y_train.iloc[index_train].copy(), y_train.iloc[index_val].copy()\n",
        "\n",
        "    # y_sub_train = y_sub_train.drop(i_to_drop, errors='ignore')\n",
        "\n",
        "    # if len(y_sub_train.unique()) != 2:\n",
        "    #     print(\"Skipping fold\", fold)\n",
        "    #     continue\n",
        "\n",
        "    # model = xgb.XGBClassifier(\n",
        "    #     objective='binary:logistic',\n",
        "    #     eval_metric='logloss',\n",
        "    #     tree_method = \"hist\",\n",
        "    #     random_state = 42,\n",
        "    #     scale_pos_weight = (y_sub_train.value_counts()[False] / y_sub_train.value_counts()[True]).item()\n",
        "    # )\n",
        "\n",
        "    print(\"Fitting model...\")\n",
        "    best_model.fit(\n",
        "        X_sub_train, y_sub_train,\n",
        "        # Evaluation set\n",
        "        eval_set = [(X_sub_val, y_sub_val)],\n",
        "        # Weight of False vs True\n",
        "        # early_stopping_rounds = 50,\n",
        "        # Silence messages\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    print(\"Predicting probabilities...\")\n",
        "    # Get probabilities for the positive class (class 1)\n",
        "    y_pred_proba = best_model.predict_proba(X_sub_val)[:, 1]\n",
        "\n",
        "    # 1. Calculate precision, recall, and thresholds for the current fold\n",
        "    precision, recall, thresholds = precision_recall_curve(y_sub_val, y_pred_proba)\n",
        "\n",
        "    # 2. Find the optimal threshold based on F1-score (or whichever metric you prefer)\n",
        "    fscores = (2 * precision * recall) / (precision + recall)\n",
        "    fscores[np.isnan(fscores)] = 0\n",
        "    optimal_idx = np.argmax(fscores)\n",
        "\n",
        "    # Need to handle the fact that thresholds array is one element shorter than P/R/F1 arrays\n",
        "    # Best practice is often to use the threshold just after the optimal index is found in P/R/F1 arrays\n",
        "    best_threshold = thresholds[optimal_idx]\n",
        "    optimal_thresholds.append(best_threshold)\n",
        "\n",
        "    print(f\"Fold {fold} Optimal Threshold (Max F1): {best_threshold:.4f}\")\n",
        "\n",
        "    # 3. Apply the *optimal* threshold to the validation predictions for THIS fold\n",
        "    y_pred_optimal = (y_pred_proba >= best_threshold).astype(int)\n",
        "\n",
        "    # 4. Calculate metrics using the *optimally thresholded* predictions\n",
        "    print(\"Getting metrics using optimal threshold...\")\n",
        "    fold_f1 = f1_score(y_sub_val, y_pred_optimal)\n",
        "    fold_accuracy = accuracy_score(y_sub_val, y_pred_optimal)\n",
        "    fold_precision = precision_score(y_sub_val, y_pred_optimal)\n",
        "    fold_recall = recall_score(y_sub_val, y_pred_optimal)\n",
        "\n",
        "    all_f1.append(fold_f1)\n",
        "    all_accuracy.append(fold_accuracy)\n",
        "    all_precision.append(fold_precision)\n",
        "    all_recall.append(fold_recall)\n",
        "\n",
        "    # You might want to use a more structured DataFrame for performance_df construction\n",
        "    performance_df[fold] = {\n",
        "        'Threshold': best_threshold,\n",
        "        'F1': fold_f1,\n",
        "        'Accuracy': fold_accuracy,\n",
        "        'Precision': fold_precision,\n",
        "        'Recall': fold_recall\n",
        "    }\n",
        "\n",
        "    print(f\"{fold}\\tOptimal F1: {fold_f1:.4f}\\tAcc: {fold_accuracy:.4f}\\tPrec: {fold_precision:.4f}\\tRec: {fold_recall:.4f}\")\n",
        "    # print(\"Predicting...\")\n",
        "    # y_pred = best_model.predict(X_sub_val)\n",
        "    # y_pred_proba = best_model.predict_proba(X_sub_val)[:, 1]\n",
        "\n",
        "    # # 2. Calculate precision, recall, and thresholds for the current fold\n",
        "    # precision, recall, thresholds = precision_recall_curve(y_sub_val, y_pred_proba)\n",
        "\n",
        "    # # 3. Find the optimal threshold based on F1-score (a common choice for balance)\n",
        "    # # Calculate F1-score for every possible threshold\n",
        "    # fscores = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    # # Handle potential division by zero warnings if no positive predictions were made\n",
        "    # fscores[np.isnan(fscores)] = 0\n",
        "\n",
        "    # # Locate the index of the highest F1-score\n",
        "    # optimal_idx = np.argmax(fscores)\n",
        "    # best_threshold = thresholds[optimal_idx] # Note: thresholds array is one element shorter than P/R arrays\n",
        "\n",
        "    # optimal_thresholds.append(best_threshold)\n",
        "\n",
        "    # print(f\"Fold {fold} Optimal Threshold (Max F1): {best_threshold:.4f}\")\n",
        "\n",
        "\n",
        "    # print(\"Getting metrics...\")\n",
        "    # fold_f1 = f1_score(y_sub_val, y_pred)\n",
        "    # fold_accuracy = accuracy_score(y_sub_val, y_pred)\n",
        "    # fold_precision = precision_score(y_sub_val, y_pred)\n",
        "    # fold_recall = recall_score(y_sub_val, y_pred)\n",
        "\n",
        "    # all_f1.append(fold_f1)\n",
        "    # all_accuracy.append(fold_accuracy)\n",
        "    # all_precision.append(fold_precision)\n",
        "    # all_recall.append(fold_recall)\n",
        "    # performance_df[fold] = {best_threshold, fold_f1, fold_accuracy, fold_precision, fold_recall}\n",
        "\n",
        "    # print(f\"{fold}\\tF1: {fold_f1:.4f}\\tAcc{fold_accuracy:.4f}\\tPrec{fold_precision:.4f}\\tRec: {fold_recall:.4f}\")\n",
        "\n",
        "print(performance_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5e29f22",
      "metadata": {
        "id": "e5e29f22"
      },
      "outputs": [],
      "source": [
        "_ = playsound(get_path('completed.mp3', 'code'), block=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9585c2a0",
      "metadata": {
        "id": "9585c2a0"
      },
      "outputs": [],
      "source": [
        "# import pprint\n",
        "\n",
        "# pprint.pprint(performance_df)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}