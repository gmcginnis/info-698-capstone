{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc37c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined data\n",
    "# # Checking if the dataset is already loaded into the workspace\n",
    "# try:\n",
    "#     if data_combined.empty == False:\n",
    "#         print(\"Data loaded, random sample shown below\")\n",
    "#         print(data_combined.sample(n=5))\n",
    "# except NameError:\n",
    "#     print(\"Data has not yet been read in, loading now...\")\n",
    "#     data_combined = pd.read_csv(\n",
    "#         \"data/bci_lutzweir_combined.csv\",\n",
    "#         usecols = ['datetime', 'level', 'raw', 'chk_note', 'chk_fail', 'comment', 'source'],\n",
    "#         parse_dates=['datetime'],\n",
    "#         dtype = {'source':'category', 'chk_note':'category', 'chk_fail':'str', 'comment':'str'},\n",
    "#         date_format='%d/%m/%Y %H:%M:%S'\n",
    "#     )\n",
    "\n",
    "data_all_combined = pd.read_csv(\n",
    "    \"data/bci_lutzweir_combined.csv\",\n",
    "    usecols = ['datetime', 'level', 'raw', 'chk_note', 'chk_fail', 'comment', 'source'],\n",
    "    parse_dates=['datetime'],\n",
    "    dtype = {'source':'category', 'chk_note':'category', 'chk_fail':'str', 'comment':'str'},\n",
    "    date_format='%d/%m/%Y %H:%M:%S',\n",
    "    index_col='datetime'\n",
    ")\n",
    "\n",
    "data_all_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aadb3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soil datasets\n",
    "\n",
    "# Shallow\n",
    "data_all_soil_shallow = pd.read_csv(\n",
    "    \"data/bci_manual_soilh/bci_lutz_shallow_gsm_man.csv\",\n",
    "    parse_dates=['date'],\n",
    "    usecols = ['date', 'depth', 'sample', 'h2o_by_wet', 'chk_note', 'chk_fail'],\n",
    "    dtype = {'depth':'category', 'sample':'category', 'chk_note':'category', 'chk_fail':'str'},\n",
    "    date_format='%d/%m/%Y',\n",
    "    index_col='date'\n",
    ")\n",
    "\n",
    "# Deep\n",
    "data_all_soil_deep = pd.read_csv(\n",
    "    \"data/bci_manual_soilh/bci_lutz_deep_gsm_man.csv\",\n",
    "    parse_dates=['date'],\n",
    "    usecols = ['date', 'depth', 'sample', 'h2o_by_wet', 'chk_note', 'chk_fail'],\n",
    "    dtype = {'depth':'category', 'sample':'category', 'chk_note':'category', 'chk_fail':'str'},\n",
    "    date_format='%d/%m/%Y',\n",
    "    index_col='date'\n",
    ")\n",
    "\n",
    "# print(data_soil_shallow['sample'].value_counts(dropna = False))\n",
    "# print(data_soil_shallow['depth'].value_counts(dropna = False))\n",
    "# print(data_soil_deep['depth'].value_counts(dropna = False))\n",
    "# print(data_soil_deep['sample'].value_counts(dropna = False))\n",
    "\n",
    "# print(\"Shallow\",\n",
    "#       data_all_soil_shallow['sample'].value_counts(dropna = False),\n",
    "#       data_all_soil_shallow['depth'].value_counts(dropna = False)\n",
    "#     )\n",
    "# print(\"Deep\",\n",
    "#       data_all_soil_deep['depth'].value_counts(dropna = False),\n",
    "#       data_all_soil_deep['sample'].value_counts(dropna = False)\n",
    "#       )\n",
    "\n",
    "data_all_soil_shallow.info()\n",
    "data_all_soil_deep.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09173b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering data sets for relevant dates\n",
    "\n",
    "# Exclude old chart data\n",
    "# data_combined = data_all_combined[~data_all_combined['source'].str.contains(\"CHART\", na=False)]\n",
    "# data_combined = data_all_combined[~data_all_combined['source']==\"CHART\"]\n",
    "data_combined = data_all_combined[data_all_combined['source']!='CHART']\n",
    "# Remove missing values\n",
    "data_combined = data_combined[data_combined['chk_note']!='missing']\n",
    "\n",
    "# Arrange for visualization & indexing\n",
    "data_combined = data_combined.sort_index()\n",
    "# Remove a few extra points\n",
    "data_combined = data_combined['1978-01-01 00:00:01':]\n",
    "\n",
    "# Get earliest and latest dates\n",
    "date_weir_start = data_combined.index[0]\n",
    "date_weir_end = data_combined.index[-1]\n",
    "\n",
    "# Create function to filter dates\n",
    "def filter_dates(input_dataset, input_date_start = date_weir_start, input_date_end = date_weir_end):\n",
    "    # Sort the dataframe\n",
    "    data_subset = input_dataset.sort_index()\n",
    "    # Filter between dates\n",
    "    data_subset = data_subset.loc[input_date_start:input_date_end]\n",
    "    return data_subset\n",
    "\n",
    "# Apply filter\n",
    "# data_rainfall = filter_dates(data_all_rainfall)\n",
    "data_soil_deep = filter_dates(data_all_soil_deep)\n",
    "data_soil_shallow = filter_dates(data_all_soil_shallow)\n",
    "# data_nochart_soil_shallow[~data_nochart_soil_shallow['sample'].isin([\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669da55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shallow_explore = data_soil_shallow[~data_soil_shallow['sample'].isin([\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"])]\n",
    "# data_shallow_explore.index.unique()[0]\n",
    "data_shallow_explore = data_soil_shallow.loc[data_shallow_explore.index.unique()[0]]\n",
    "data_shallow_explore['sample'] = data_shallow_explore['sample'].astype('int')\n",
    "data_shallow_explore.sort_values(by='sample')\n",
    "# data_shallow_explore\n",
    "\n",
    "# data_shallow_explore\n",
    "\n",
    "\n",
    "# data_soil_shallow['2005-03-02 00:00:00':'2005-03-02 23:59:59'].sort_values(by='sample')\n",
    "# data_soil_shallow['2005-03-02 00:00:00':'2005-03-02 23:59:59']\n",
    "# data_soil_shallow[data_shallow_explore.index[0]]\n",
    "# print(\"Shallow\",\n",
    "#       data_soil_shallow['sample'].value_counts(),\n",
    "#       data_soil_shallow['depth'].value_counts(),\n",
    "#       sep=\"\\n\"\n",
    "#     )\n",
    "# print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_deep_explore\n",
    "data_deep_explore = data_soil_deep[data_soil_deep['depth']=='0-10']\n",
    "data_deep_explore = data_soil_deep.loc[data_deep_explore.index.unique()]\n",
    "data_deep_explore['sample'] = data_deep_explore['sample'].astype('int')\n",
    "data_deep_explore = data_deep_explore.sort_values(by='sample')\n",
    "data_deep_explore.sort_index()\n",
    "# print(\"Deep\",\n",
    "#       data_soil_deep['depth'].value_counts(),\n",
    "#       data_soil_deep['sample'].value_counts(),\n",
    "#       sep=\"\\n\"\n",
    "#       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fc7728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all rows that are duplicated based on the 'date' index and 'categoryB' column\n",
    "# data_deep_explore['sample'] = data_deep_explore['sample'].astype('category')\n",
    "# data_deep_explore.index.name\n",
    "temp_df = data_deep_explore.reset_index()\n",
    "# temp_df\n",
    "duplicates_mask = temp_df.duplicated(subset=['date', \"sample\"], keep=False)\n",
    "\n",
    "# Filter the DataFrame to show only the entries that have duplicates\n",
    "\n",
    "filtered_df = temp_df[duplicates_mask]\n",
    "\n",
    "print(\"Filtered DataFrame showing entries with repeated categoryB on the same date:\")\n",
    "filtered_df = filtered_df.set_index('date')\n",
    "filtered_df['sample'] = filtered_df['sample'].astype('int')\n",
    "filtered_df = filtered_df.sort_values(by='sample')\n",
    "filtered_df.sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d0186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_deep_explore\n",
    "# pd.merge(data_soil_deep, data_soil_shallow, left_index=True, right_index=True, how='inner')\n",
    "data_deep_join = data_soil_deep.reset_index()\n",
    "data_deep_join['source'] = 'deep'\n",
    "data_shallow_join = data_soil_shallow.reset_index()\n",
    "data_shallow_join['source'] = 'shallow'\n",
    "# pd.merge(data_deep_join, data_shallow_join, left_on = [\"date\", \"depth\"], right_on = [\"date\", \"depth\"])\n",
    "# pd.merge(data_deep_join, data_shallow_join, on='date', how='inner')\n",
    "# data_joined = pd.concat([data_deep_join, data_shallow_join], names=['source'])\n",
    "data_joined = pd.concat([data_deep_join, data_shallow_join])\n",
    "data_joined = data_joined.sort_values(by=['date', 'sample'])\n",
    "\n",
    "# print(\"Filtered DataFrame showing entries with repeated categoryB on the same date:\")\n",
    "# data_joined = data_joined.set_index('date')\n",
    "# filtered_df['sample'] = filtered_df['sample'].astype('int')\n",
    "# filtered_df = filtered_df.sort_values(by='sample')\n",
    "# filtered_df = filtered_df.sort_index()\n",
    "data_joined = data_joined[data_joined['depth'].isin([\"0-10\",\"1-10\"])]\n",
    "data_joined = data_joined.set_index('date')\n",
    "data_joined = data_joined.loc[data_deep_explore.index.unique()]\n",
    "data_joined\n",
    "# data_soil_shallow.loc[filtered_df.index.unique()].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6ec2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_deep_filtered = data_soil_deep[data_soil_deep['depth'] == \"0-10\"].sort_index()\n",
    "data_deep_filtered = data_deep_filtered.drop('depth', axis=1)\n",
    "data_deep_filtered = data_deep_filtered.reset_index()\n",
    "\n",
    "data_shallow_filtered = data_soil_shallow.sort_index()[data_deep_filtered.index[0]:data_deep_filtered.index[-1]]\n",
    "data_shallow_filtered = data_shallow_filtered.drop('depth', axis=1)\n",
    "data_shallow_filtered = data_shallow_filtered.reset_index()\n",
    "# # data_shallow_filtered['depth'].unique()\n",
    "# pd.merge(data_deep_filtered, data_shallow_filtered, on=['date', 'sample', 'depth'], how='inner')\n",
    "# data_deep_filtered.compare(data_shallow_filtered)\n",
    "# pd.concat([data_deep_filtered, data_shallow_filtered]).drop_duplicates(keep=False)\n",
    "\n",
    "# data_joined_mini = data_joined.drop('depth', axis=1).drop('source',axis=1)\n",
    "data_joined_mini = data_joined.drop('depth', axis=1)\n",
    "# data_joined_mini = data_joined_mini.drop_duplicates(keep=False)\n",
    "data_joined_mini = data_joined_mini.drop_duplicates(subset=data_joined_mini.columns.difference(['source']), keep=False)\n",
    "data_joined_mini = data_joined_mini[['sample', 'source', 'h2o_by_wet', 'chk_note', 'chk_fail']]\n",
    "data_joined_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e0f544",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_deep_match = data_soil_deep[data_soil_deep[\"depth\"] == \"0-10\"].sort_index().drop('depth', axis=1)\n",
    "data_deep_match = data_deep_match.reset_index()\n",
    "# dates_deep = data_deep_match.index\n",
    "data_shallow_match = data_soil_shallow[data_soil_shallow.sort_index().index.isin(data_deep_match.index)].drop('depth', axis=1)\n",
    "data_shallow_match = data_shallow_match.reset_index()\n",
    "\n",
    "# data_deep_match\n",
    "# pd.merge(data_deep_match, data_shallow_match, on=[\"date\", \"sample\"], suffixes=(\"_deep\", \"_shallow\"), how=\"inner\")\n",
    "match_result = pd.merge(data_deep_match, data_soil_shallow.reset_index().drop('depth', axis=1), on=[\"date\", \"sample\"], suffixes=(\"_deep\", \"_shallow\"), how=\"inner\")\n",
    "match_result[\"match\"] = (match_result[\"h2o_by_wet_deep\"] == match_result[\"h2o_by_wet_shallow\"])\n",
    "match_result[\"sample\"] = match_result[\"sample\"].astype('int')\n",
    "match_result = match_result.sort_values(by=['date', 'sample'])\n",
    "match_result = match_result.drop(['chk_fail_shallow', 'chk_fail_deep'], axis=1)\n",
    "match_result[match_result[\"match\"]==False]\n",
    "# match_result[[\"date\", \"match\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0450b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_result = data_soil_deep.sort_index().loc[\"2005-06-16 00:00:00\"]\n",
    "deep_result[\"sample\"] = deep_result[\"sample\"].astype('int')\n",
    "deep_result.sort_values(by=['date', 'depth', 'sample'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de401252",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_soil_shallow.sort_index().loc[\"2006-03-24 00:00:00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d67f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_all = pd.read_csv(\n",
    "    \"data/bci_manual_soilh/bci_lutz_shallow_gsm_man.csv\",\n",
    "    parse_dates=['date'],\n",
    "    # nrows=100,\n",
    "    usecols = ['date', 'depth', 'sample', 'h2o_by_wet', 'h2o_by_dry', 'chk_note', 'chk_fail'],\n",
    "    dtype = {'depth':'category', 'sample':'category', 'chk_note':'category', 'chk_fail':'str'},\n",
    "    date_format='%d/%m/%Y',\n",
    "    index_col='date'\n",
    ")\n",
    "\n",
    "# shallow_all = shallow_all.sort_index().loc[date_weir_start:date_weir_end]\n",
    "shallow_all = filter_dates(shallow_all)\n",
    "# shallow_all = shallow_all.reset_index()\n",
    "\n",
    "# # Deep\n",
    "deep_all = pd.read_csv(\n",
    "    \"data/bci_manual_soilh/bci_lutz_deep_gsm_man.csv\",\n",
    "    parse_dates=['date'],\n",
    "    usecols = ['date', 'depth', 'sample', 'h2o_by_wet', 'h2o_by_dry', 'chk_note', 'chk_fail'],\n",
    "    dtype = {'depth':'category', 'sample':'category', 'chk_note':'category', 'chk_fail':'str'},\n",
    "    date_format='%d/%m/%Y',\n",
    "    index_col='date'\n",
    ")\n",
    "\n",
    "deep_all = filter_dates(deep_all)\n",
    "deep_all = deep_all[deep_all[\"depth\"] != \"30-40\"]#.reset_index()\n",
    "\n",
    "# Filter set to only be of dates where deep set has shallow values\n",
    "shallow_all = shallow_all[shallow_all.index.isin(deep_all.index)]\n",
    "shallow_all.reset_index()\n",
    "\n",
    "match_all = pd.merge(deep_all.reset_index(), shallow_all.reset_index(), on=[\"date\", \"sample\"], suffixes=(\"_deep\", \"_shallow\"), how=\"inner\")\n",
    "match_all[\"match_wet\"] = (match_all[\"h2o_by_wet_deep\"] == match_all[\"h2o_by_wet_shallow\"])\n",
    "match_all[\"match_dry\"] = (match_all[\"h2o_by_dry_deep\"] == match_all[\"h2o_by_dry_shallow\"])\n",
    "match_all[\"sample\"] = match_all[\"sample\"].astype('int')\n",
    "match_all = match_all.sort_values(by=['date', 'sample'])\n",
    "match_all = match_all.drop(['chk_fail_shallow', 'chk_fail_deep'], axis=1)\n",
    "# match_all[match_all[\"match_wet\"] & match_all[\"match_dry\"]]\n",
    "match_all = match_all[((match_all[\"match_wet\"]==False) | (match_all[\"match_wet\"]==False))]\n",
    "match_all = match_all[['date', 'depth_shallow', 'depth_deep', 'sample', 'h2o_by_wet_shallow', 'h2o_by_wet_deep', 'h2o_by_dry_shallow', 'h2o_by_dry_deep', 'chk_note_shallow', 'chk_note_deep']]\n",
    "# match_all = match_all.drop([\"depth_deep\", \"depth_shallow\"],axis=1)\n",
    "# match_all = match_all[['date', 'sample', 'h2o_by_wet_shallow', 'h2o_by_wet_deep', 'h2o_by_dry_shallow', 'h2o_by_dry_deep', 'chk_note_shallow', 'chk_note_deep']]\n",
    "\n",
    "# match_all = match_all[['date', 'sample', 'h2o_by_wet_shallow', 'h2o_by_wet_deep', 'match_wet', 'h2o_by_dry_shallow', 'h2o_by_dry_deep', 'match_dry', 'chk_note_shallow', 'chk_note_deep']]\n",
    "# # data_deep_match = data_soil_deep[data_soil_deep[\"depth\"] == \"0-10\"].sort_index().drop('depth', axis=1)\n",
    "# # data_deep_match = data_deep_match.reset_index()\n",
    "# # dates_deep = data_deep_match.index\n",
    "# # data_shallow_match = data_soil_shallow[data_soil_shallow.sort_index().index.isin(data_deep_match.index)].drop('depth', axis=1)\n",
    "# data_shallow_match = data_shallow_match.reset_index()\n",
    "\n",
    "# # data_deep_match\n",
    "# # pd.merge(data_deep_match, data_shallow_match, on=[\"date\", \"sample\"], suffixes=(\"_deep\", \"_shallow\"), how=\"inner\")\n",
    "# match_result = pd.merge(data_deep_match, data_soil_shallow.reset_index().drop('depth', axis=1), on=[\"date\", \"sample\"], suffixes=(\"_deep\", \"_shallow\"), how=\"inner\")\n",
    "# match_result[\"match\"] = (match_result[\"h2o_by_wet_deep\"] == match_result[\"h2o_by_wet_shallow\"])\n",
    "# match_result[\"sample\"] = match_result[\"sample\"].astype('int')\n",
    "# match_result = match_result.sort_values(by=['date', 'sample'])\n",
    "# match_result = match_result.drop(['chk_fail_shallow', 'chk_fail_deep'], axis=1)\n",
    "# match_result[match_result[\"match\"]==False]\n",
    "# # match_result[[\"date\", \"match\"]]\n",
    "match_all\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
