{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68a3c4a0",
   "metadata": {},
   "source": [
    "# Data Separation\n",
    "\n",
    "Author: Gillian A. McGinnis, final-semester M.S. Information Science - Machine Learning  \n",
    "The University of Arizona College of Information  \n",
    "INFO 698 - Capstone  \n",
    "Start date: 21 October 2025  \n",
    "Last updated: 21 October 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d80b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module providing code for test/train split and sliding window creation. Relies on 01_eda.ipynb completion.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de84a462",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7208265e",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb557fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# # import matplotlib.ticker as ticker\n",
    "# import matplotlib.dates as mdates\n",
    "# import datetime as dt\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, ConfusionMatrixDisplay, f1_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb5d8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (Optional chunk)\n",
    "# Current session information\n",
    "import session_info\n",
    "session_info.show(dependencies=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef4832",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42966ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "united_water = pd.read_parquet('data/clean/water.parquet')\n",
    "united_soil = pd.read_parquet('data/clean/soil.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168e27bd",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb9dc61",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2442d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns of interest\n",
    "data_water = united_water.drop(columns=['raw_rain', 'chk_note_rain', 'chk_fail_rain', 'chk_note_ro', 'chk_fail_ro', 'comment_ro', 'source_ro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5776e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Note ###\n",
    "# Remove this later -- just a smaller subset for feature engineering testing!!\n",
    "data_water = data_water['2015-01-01 00:00:00':'2016-12-31 23:59:59']\n",
    "# data_water = data_water['2000-01-01 00:00:00':'2015-12-31 23:59:59']\n",
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d5a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "del united_water"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa843b0",
   "metadata": {},
   "source": [
    "### Distance from Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8544fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def since_feat(input_df, input_col):\n",
    "    output_df = input_df.copy()\n",
    "    # Create index of instances where there is a data point\n",
    "    instances = output_df[input_col].notna()\n",
    "    # Create groupings based on most recent instance\n",
    "    group_id = instances.cumsum()\n",
    "    # Exclude the first grouping\n",
    "    # otherwise it assumes there was an event just prior to the first entry\n",
    "    group_id = group_id.replace(0, np.nan)\n",
    "    # Create new column to count number of records since the point\n",
    "    # which resets to 0 at each new point\n",
    "    output_df[f\"since_{input_col}\"] = output_df.groupby(group_id).cumcount()\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddd3302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minsince_feat(input_df, input_col):\n",
    "    output_df = input_df.copy()#[input_col].to_frame()\n",
    "    instances = output_df[input_col].notna()\n",
    "    # Create groupings based on most recent instance\n",
    "    group_id = instances.cumsum()\n",
    "    # Exclude the first grouping\n",
    "    # otherwise it assumes there was an event just prior to the first entry\n",
    "    group_id = group_id.replace(0, np.nan)\n",
    "    # Create new column to count the distance in minutes since the point\n",
    "    # which resets to 0 at each new point\n",
    "    output_df['timestamp'] = pd.to_datetime(output_df.index)\n",
    "    # Get start timestamp of the group\n",
    "    output_df['ts_start'] = output_df.groupby(group_id)['timestamp'].transform('min')\n",
    "    # Calculate the distance\n",
    "    output_df[f\"minsince_{input_col}\"] = (output_df['timestamp'] - output_df['ts_start']).dt.total_seconds()/60\n",
    "    # Remove extra cols\n",
    "    output_df = output_df.drop(columns=['timestamp', 'ts_start'])\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dbffdf",
   "metadata": {},
   "source": [
    "#### Calibration\n",
    "Create feature which tracks how recent a calibration was conducted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a73b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # since_feat(data_water[['ra_rain', 'raw_ro']], 'ra_rain')\n",
    "# # data_water[['ra_rain', 'raw_ro']]\n",
    "# data_w_test = data_water.copy()[['ra_rain', 'raw_ro']]\n",
    "# data_w_test['ra_rain'] = data_w_test['ra_rain'].replace(0, np.nan)\n",
    "\n",
    "# data_w_test = since_feat(data_w_test, 'ra_rain')\n",
    "# data_w_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d6dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create index of instances where there is a calibration point\n",
    "# cal_instances = data_water['weir_level_cal'].notna()\n",
    "# # Create groupings based on most recent instance\n",
    "# cal_group_id = cal_instances.cumsum()\n",
    "# # Create new column to count number of records since the calibration point\n",
    "# # which resets to 0 at each new calibration\n",
    "# data_water['records_since_cal'] = data_water.groupby(cal_group_id).cumcount()\n",
    "\n",
    "# # Clean up environment\n",
    "# del cal_instances, cal_group_id\n",
    "\n",
    "# # data_water\n",
    "\n",
    "# data_water = since_feat(data_water, 'weir_level_cal')\n",
    "data_water = minsince_feat(data_water, 'weir_level_cal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830a5cbc",
   "metadata": {},
   "source": [
    "#### Rain\n",
    "Create feature which tracks how recent a rain event occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21cc478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create index of instances where there is a calibration point\n",
    "# rain_instances = data_water['ra_rain'].notna()\n",
    "# # Create groupings based on most recent instance\n",
    "# rain_group_id = rain_instances.cumsum()\n",
    "# # Create new column to count number of records since the calibration point\n",
    "# # which resets to 0 at each new calibration\n",
    "# data_water['records_since_rain'] = data_water.groupby(rain_group_id).cumcount()\n",
    "\n",
    "# # Clean up environment\n",
    "# del rain_instances, rain_group_id\n",
    "\n",
    "# # Replace NAs with 0\n",
    "# data_water['ra_rain'] = data_water['ra_rain'].fillna(0)\n",
    "\n",
    "# data_water.sample(10)\n",
    "# # data_water.dropna(subset='raw_ro')\n",
    "\n",
    "# data_water = since_feat(data_water, 'ra_rain')\n",
    "data_water = minsince_feat(data_water, 'ra_rain')\n",
    "# data_water.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed8038",
   "metadata": {},
   "source": [
    "Fill missing rain values with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02690142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_water['ra_rain'] = data_water['ra_rain'].fillna(0)\n",
    "# data_water.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e786a7e9",
   "metadata": {},
   "source": [
    "### Rain event\n",
    "\n",
    "Keep track of cumulative rainfall during a specific event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b7c231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index of instances where there is a data point\n",
    "rain_event = data_water['ra_rain'].isnull()\n",
    "# Create groupings based on most recent instance\n",
    "rain_event_id = rain_event.cumsum()\n",
    "# Create new column to count number of records since the point\n",
    "# which resets to 0 at each new point\n",
    "# del group_id, instances\n",
    "# water_mini\n",
    "# group_id = group_id.replace(0, np.nan)\n",
    "# water_mini['since_ra_rain2'] = water_mini.groupby(group_id).cumcount()\n",
    "# water_mini\n",
    "# water_mini.info()\n",
    "data_water['eventsum_ra_rain'] = data_water.groupby(rain_event_id)['ra_rain'].cumsum()\n",
    "\n",
    "# del rain_event, rain_event_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rain_event_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0da2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_water[['ra_rain', 'since_ra_rain', 'rain_event_cumsum']]\n",
    "# data_water\n",
    "# 475"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122cc839",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_mini = data_water.copy()[['weir_level_cal', 'ra_rain', 'raw_ro', 'since_weir_level_cal', 'since_ra_rain']]\n",
    "water_mini.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9dd8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# water_mini = data_water.copy()[['ra_rain', 'level_ro']]\n",
    "water_mini = data_water.copy()['ra_rain'].to_frame()\n",
    "instances = water_mini['ra_rain'].notna()\n",
    "# Create groupings based on most recent instance\n",
    "group_id = instances.cumsum()\n",
    "# Exclude the first grouping\n",
    "# otherwise it assumes there was an event just prior to the first entry\n",
    "group_id = group_id.replace(0, np.nan)\n",
    "# Create new column to count number of records since the point\n",
    "# which resets to 0 at each new point\n",
    "# output_df[f\"since_{input_col}\"] = output_df.groupby(group_id).cumcount()\n",
    "# group_id\n",
    "\n",
    "water_mini['timestamp'] = pd.to_datetime(water_mini.index)\n",
    "water_mini['ts_start'] = water_mini.groupby(group_id)['timestamp'].transform('min')\n",
    "water_mini['ts_dist'] = (water_mini['timestamp'] - water_mini['ts_start']).dt.total_seconds()/60\n",
    "water_mini\n",
    "\n",
    "# water_mini = water_mini.reset_index()\n",
    "# water_mini['ts_start'] = water_mini.groupby(group_id)['datetime'].transform('min')\n",
    "# water_mini['ts_dist'] = (water_mini['datetime'] - water_mini['ts_start']).dt.total_seconds() / 60\n",
    "# water_mini.set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3ee645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index of instances where there is a data point\n",
    "instances = water_mini['ra_rain'].notna()\n",
    "# Create groupings based on most recent instance\n",
    "group_id = instances.cumsum()\n",
    "# Create new column to count number of records since the point\n",
    "# which resets to 0 at each new point\n",
    "# del group_id, instances\n",
    "# water_mini\n",
    "# group_id = group_id.replace(0, np.nan)\n",
    "# water_mini['since_ra_rain2'] = water_mini.groupby(group_id).cumcount()\n",
    "# water_mini\n",
    "# water_mini.info()\n",
    "water_mini['rain_event'] = water_mini.groupby(group_id)['ra_rain'].cumsum()\n",
    "\n",
    "# rain_null_mask = water_mini['ra_rain'].isnull()\n",
    "# rain_group_id = rain_null_mask.cumsum()\n",
    "# water_mini.groupby(rain_group_id)['ra_rain'].cumsum()\n",
    "# # rain_null_mask\n",
    "# # water_mini['rain_event_cumsum'] = water_mini.groupby(rain_group_id)['ra_rain'].cumsum()\n",
    "\n",
    "# # g_id_event = null_mask.cumsum()\n",
    "# # water_m['r_event_sum'] = water_m.groupby(g_id_event)['ra_rain'].cumsum()\n",
    "\n",
    "# # del rain_null_mask, rain_group_id\n",
    "# # data_water['rain_event_sum'] = data_water.groupby(g_id_event)['ra_rain'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24471c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bde8f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# israin = water_mini['ra_rain'].notna()\n",
    "# israin_group_id = israin.cumsum()\n",
    "# # g_id\n",
    "# water_mini['since_rain_2'] = water_mini.groupby(israin_group_id).cumcount()\n",
    "water_mini['dec'] = np.exp(-0.1*water_mini['since_ra_rain'])\n",
    "water_mini['rain_fill'] = water_mini['rain_event_cumsum'].ffill()\n",
    "# data_u['1_shallow_f'] = data_u['1_shallow'].ffill()\n",
    "water_mini['rain_dec'] = (water_mini['rain_fill']*water_mini['dec'])\n",
    "# del israin, israin_group_id\n",
    "water_mini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a81ae4",
   "metadata": {},
   "source": [
    "### Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f01638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_feat(input_df, input_col, input_dec_rate = -0.1):\n",
    "    output_df = input_df.copy()\n",
    "    output_df = since_feat(input_df = output_df, input_col = input_col)\n",
    "    \n",
    "    output_df[f\"decayrate{input_dec_rate}_{input_col}\"] = np.exp(input_dec_rate*output_df[input_col])\n",
    "\n",
    "    return output_df\n",
    "\n",
    "water_m = united_water[['raw_ro', 'level_ro', 'ra_rain', 'obstruction_ro']]\n",
    "\n",
    "null_mask = water_m['ra_rain'].isnull()\n",
    "g_id_event = null_mask.cumsum()\n",
    "water_m['r_event_sum'] = water_m.groupby(g_id_event)['ra_rain'].cumsum()\n",
    "\n",
    "is_rain = water_m['ra_rain'].notna()\n",
    "g_id = is_rain.cumsum()\n",
    "# g_id\n",
    "water_m['since_rain'] = water_m.groupby(g_id).cumcount()\n",
    "water_m['dec'] = np.exp(-0.1*water_m['since_rain'])\n",
    "water_m['rain_fill'] = water_m['r_event_sum'].ffill()\n",
    "# data_u['1_shallow_f'] = data_u['1_shallow'].ffill()\n",
    "water_m['rain_dec'] = (water_m['rain_fill']*water_m['dec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6154e3b7",
   "metadata": {},
   "source": [
    "### Lag features\n",
    "Get values from other recent time stamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c4b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_feats(input_df, input_cols, input_lags):\n",
    "    output_df = input_df.copy()\n",
    "    for col in input_cols:\n",
    "        for lag in input_lags:\n",
    "            output_df[f\"{col}_lag{lag}\"] = output_df[col].shift(lag)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797920ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag_feats(data_water, ['raw_ro'], [1, 2, 3, 24]).dropna(subset='raw_ro')[['raw_ro', 'raw_ro_lag1', 'raw_ro_lag2']]\n",
    "# lag_feats(data_water, ['raw_ro'], [1, 2, 3, 24]).dropna(subset='raw_ro')[['raw_ro', 'raw_ro_lag1', 'raw_ro_lag24']]\n",
    "\n",
    "# Columns to get temporal stats on\n",
    "cols_to_shift = ['raw_ro', 'ra_rain']\n",
    "# data at 5-min increments -- lag to record values at 5m, 10m, 15m, 30m, 1h, and 2h prior\n",
    "lags_of_interest = [1, 2, 3, 6, 12, 24]\n",
    "\n",
    "data_water = lag_feats(data_water, cols_to_shift, lags_of_interest)\n",
    "\n",
    "data_water.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddce5f3",
   "metadata": {},
   "source": [
    "### Rolling stats\n",
    "\n",
    "Get stat values from range of recent time stamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbbea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_feats(input_df, input_cols, input_windows):\n",
    "    output_df = input_df.copy()\n",
    "    for col in input_cols:\n",
    "        for window in input_windows:\n",
    "            output_df[f\"{col}_rollmean_{window}\"] = output_df[col].rolling(window).mean()\n",
    "            output_df[f\"{col}_rollstd_{window}\"] = output_df[col].rolling(window).std()\n",
    "            output_df[f\"{col}_rollslope_{window}\"] = (output_df[col].rolling(window).apply(lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=True))\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931d09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_water_mini = data_water['1990-01-01 00:00:00':'1990-01-30 23:59:59']\n",
    "# rolling_feats(data_water_mini, cols_to_lag, [6, 12, 36])\n",
    "# 10m, 30m, 1h, 6h\n",
    "windows_of_interest = [2, 6, 12, 72]\n",
    "\n",
    "\n",
    "data_water = rolling_feats(data_water, cols_to_shift, windows_of_interest)\n",
    "\n",
    "data_water.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596fa05d",
   "metadata": {},
   "source": [
    "Change since last value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_water['raw_ro_change'] = data_water['raw_ro'].diff()\n",
    "\n",
    "# cal_na_mask = data_water['weir_level_cal'].notna() & data_water['raw_ro'].notna()\n",
    "# # cal_na_mask\n",
    "# (data_water['weir_level_cal'] - data_water['raw_ro']).dropna()\n",
    "# del cal_na_mask\n",
    "data_water['diff_ro_cal'] = (data_water['weir_level_cal'] - data_water['raw_ro'])\n",
    "# data_water['rain_diff']\n",
    "\n",
    "data_water.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c438b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b17b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_water = data_water.dropna(subset='obstruction_ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794b582",
   "metadata": {},
   "source": [
    "## Soil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8e3f87",
   "metadata": {},
   "source": [
    "Pivot the soil data such that each sample has its own columns, and separated by depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd40d185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant column\n",
    "data_soil_shallow = united_soil.copy().drop('h2o_by_wet_deep', axis=1)\n",
    "# Pivot wider\n",
    "data_soil_shallow = data_soil_shallow.pivot(columns='sample', values='h2o_by_wet_shallow')\n",
    "\n",
    "# Drop irrelevant column\n",
    "data_soil_deep = united_soil.copy().drop('h2o_by_wet_shallow', axis=1)\n",
    "# Pivot wider\n",
    "data_soil_deep = data_soil_deep.pivot(columns='sample', values='h2o_by_wet_deep')\n",
    "\n",
    "# Combine\n",
    "data_soil = pd.merge(\n",
    "    data_soil_shallow,\n",
    "    data_soil_deep,\n",
    "    left_index = True,\n",
    "    right_index = True,\n",
    "    suffixes = (\"_shallow\", \"_deep\"),\n",
    "    how = \"outer\"\n",
    ")\n",
    "\n",
    "del data_soil_shallow, data_soil_deep\n",
    "\n",
    "data_soil.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17263955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soil_mini_shallow = united_soil.copy().drop('h2o_by_wet_deep', axis=1)\n",
    "# soil_mini_shallow = soil_mini_shallow.pivot(columns='sample', values='h2o_by_wet_shallow')\n",
    "\n",
    "# soil_mini_deep = united_soil.copy().drop('h2o_by_wet_shallow', axis=1)\n",
    "# soil_mini_deep = soil_mini_deep.pivot(columns='sample', values='h2o_by_wet_deep')\n",
    "\n",
    "# soil_mini = pd.merge(\n",
    "#     soil_mini_shallow,\n",
    "#     soil_mini_deep,\n",
    "#     left_index=True,\n",
    "#     right_index=True,\n",
    "#     # soil_mini_shallow.reset_index(),\n",
    "#     # soil_mini_deep.reset_index(),\n",
    "#     # on = [\"date\", \"sample\"],\n",
    "#     suffixes = (\"_shallow\", \"_deep\"),\n",
    "#     how = \"outer\"\n",
    "#     )\n",
    "\n",
    "# soil_mini.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47024efe",
   "metadata": {},
   "source": [
    "## Unite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56358374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_u_test = pd.merge(\n",
    "#     data_water,\n",
    "#     data_soil['2015-01-01 00:00:00':'2016-12-31 23:59:59'],\n",
    "#     left_index = True,\n",
    "#     right_index = True,\n",
    "#     how = 'outer'\n",
    "# )\n",
    "\n",
    "# data_u_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8de6109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def since_feat(input_df, input_col):\n",
    "# #     output_df = input_df.copy()\n",
    "# #     # Create index of instances where there is a data point\n",
    "# #     instances = output_df[input_col].notna()\n",
    "# #     # Create groupings based on most recent instance\n",
    "# #     group_id = instances.cumsum()\n",
    "# #     # Create new column to count number of records since the point\n",
    "# #     # which resets to 0 at each new point\n",
    "# #     output_df[f\"since_{input_col}\"] = output_df.groupby(group_id).cumcount()\n",
    "# #     return output_df\n",
    "\n",
    "# cols_soil = [col for col in data_u_test.columns if (col.endswith('shallow') | col.endswith('deep'))]\n",
    "# soil_instances = data_u_test[cols_soil].notna()\n",
    "# soil_group_id = soil_instances.cumsum().max(axis=1)\n",
    "# data_u_test[\"since_soil\"] = data_u_test.groupby(soil_group_id).cumcount()\n",
    "# # data_u_test.groupby(soil_group_id).cumcount()\n",
    "# # data_u_test[\"since_soil\"] = data_u_test.groupby(soil_group_id).cumcount()\n",
    "# # data_u_test[cols_soil].notna().cumsum().max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_united = pd.merge(\n",
    "    data_water,\n",
    "    # REMOVE LATER\n",
    "    data_soil['2015-01-01 00:00:00':'2016-12-31 23:59:59'],\n",
    "    # data_soil['2000-01-01 00:00:00':'2015-12-31 23:59:59'],\n",
    "    # data_soil,\n",
    "    #\n",
    "    left_index = True,\n",
    "    right_index = True,\n",
    "    how = 'outer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4b5605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature to track soil value staleness\n",
    "cols_soil = [col for col in data_united.columns if (col.endswith('shallow') | col.endswith('deep'))]\n",
    "soil_instances = data_united[cols_soil].notna()\n",
    "soil_group_id = soil_instances.cumsum().max(axis=1)\n",
    "data_united[\"since_soil\"] = data_united.groupby(soil_group_id).cumcount()\n",
    "\n",
    "del soil_instances, soil_group_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b46c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend soil vals\n",
    "# cols_to_fill = [col for col in data_united.columns if (col.endswith('shallow') | col.endswith('deep'))]\n",
    "# data_united[cols_to_fill] = data_united[cols_to_fill].ffill()\n",
    "data_united[cols_soil] = data_united[cols_soil].ffill()\n",
    "\n",
    "del cols_soil\n",
    "data_united.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd20a0d9",
   "metadata": {},
   "source": [
    "### Train/Test (80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df4136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mini_xy = water_mini[['level_ro', 'raw_ro', 'chk_note_ro', 'source_ro']].dropna()\n",
    "# mini_xy = water_mini.copy().drop('level_ro', axis=1).dropna()\n",
    "# mini_y = mini_xy['obstruction_ro']\n",
    "# # mini_x = mini_xy[['raw_ro', 'chk_note_ro', 'source_ro']]\n",
    "# mini_x = mini_xy.drop('obstruction_ro', axis=1)\n",
    "# mini_xy\n",
    "var_of_interest = 'obstruction_ro'\n",
    "y_drops = ['level_ro', 'obstruction_ro', 'gap_fill_ro', 'weir_cleaning_ro', 'spike_ro', 'calibration_ro']\n",
    "\n",
    "data_filtered = data_united.copy().dropna(subset = var_of_interest)\n",
    "# y_drops.remove(var_of_interest)\n",
    "\n",
    "united_y = data_filtered[var_of_interest]\n",
    "# united_x = data_united.drop([var_of_interest, 'level_ro'], axis=1)\n",
    "united_x = data_filtered.drop(y_drops, axis=1)\n",
    "# united_x.info()\n",
    "\n",
    "del data_filtered\n",
    "\n",
    "# united_x.info()\n",
    "\n",
    "# united_x.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880d8f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test:\\t20p of\", len(united_y), \"is\", round(.2*len(united_y)))\n",
    "print(\"Train:\\t80p of\", len(united_y), \"is\", round(.8*len(united_y)))\n",
    "print(round(.2*len(united_y)) + round(.8*len(united_y)))\n",
    "\n",
    "# mini_x.index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a6ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(united_x, united_y, test_size = 0.2, shuffle=False)\n",
    "\n",
    "print(\n",
    "    \"Train:\\t\", len(x_train), \"\\t\", x_train.index[0], \"thru\", x_train.index[-1],\n",
    "    \"\\nTest:\\t\", len(x_test), \"\\t\\t\", x_test.index[0], \"thru\", x_test.index[-1]\n",
    "    # len(x_train), len(x_test), \"\\n\",\n",
    "    # x_train.index[-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11921d04",
   "metadata": {},
   "source": [
    "### Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc0892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=20)\n",
    "print(tscv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661eeac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tscv)\n",
    "for i, (train_index, val_index) in enumerate(tscv.split(x_train)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={val_index}\")\n",
    "    # print(\"  Train: index=\", mini_x.index[train_index])\n",
    "    # print(f\"  Test:  index={val_index}\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "\n",
    "del i, train_index, val_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6a41c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_tracker = y_train.copy().to_frame()\n",
    "# val_tracker['pred'] = .5\n",
    "# val_tracker.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a7bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preds\n",
    "# y_t = y_t.to_frame()\n",
    "# y_t['preds'] = preds\n",
    "# pd.concat(y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a093645",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f76128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tscv = TimeSeriesSplit(n_splits=15)\n",
    "# val_tracker = y_train.copy()\n",
    "# val_tracker['pred'] = .5\n",
    "# val_tracker = y_train.copy().to_frame()\n",
    "val_tracker = pd.DataFrame()\n",
    "win_tracker = pd.DataFrame(columns=[\"fold\", \"mse\", \"rmse\", \"f1\", \"acc\"])\n",
    "i = 0\n",
    "\n",
    "for train_index, val_index in tscv.split(x_train):\n",
    "    x_t, X_val = x_train.iloc[train_index], x_train.iloc[val_index]\n",
    "    y_t, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    if len(y_t.unique()) != 2:\n",
    "        print(\"Skipping fold\", i)\n",
    "        i += 1\n",
    "        continue\n",
    "    # model = xgb.XGBRegressor(enable_categorical=True, tree_method=\"hist\")\n",
    "    # model = xgb.XGBClassifier(enable_categorical=True, tree_method=\"hist\")\n",
    "    # model = xgb.XGBClassifier(tree_method=\"hist\")\n",
    "    model = xgb.XGBClassifier(\n",
    "        tree_method=\"hist\",\n",
    "        # handle class imbalance -- sum(negative instances) / sum(positive instances)\n",
    "        scale_pos_weight = (y_t.value_counts()[False] / y_t.value_counts()[True]).item()\n",
    "    )\n",
    "    # if len(y_val.unique()) != 2:\n",
    "    #     print(\"Skipping fold\", i)\n",
    "    #     i += 1\n",
    "    #     continue\n",
    "    # i += 1\n",
    "    model.fit(x_t, y_t)\n",
    "    preds = model.predict(X_val)\n",
    "    #\n",
    "    y_val_out = y_val.copy().to_frame()\n",
    "    y_val_out['pred'] = preds\n",
    "    y_val_out['pred_tf'] = np.where(y_val_out['pred'] == 1, True, False)\n",
    "    val_tracker = pd.concat([val_tracker, y_val_out])\n",
    "    #\n",
    "    mse = mean_squared_error(y_val, preds)\n",
    "    f1 = f1_score(y_val_out[var_of_interest].tolist(), y_val_out['pred_tf'].tolist())\n",
    "    accuracy = accuracy_score(y_val_out[var_of_interest].tolist(), y_val_out['pred_tf'].tolist())\n",
    "    # print(\"Validation RMSE:\", mean_squared_error(y_val, preds, squared=False))\n",
    "    print(i, \"\\tMSE:\", round(mse, 4), \"\\tRMSE:\", round(np.sqrt(mse), 4), \"\\tF1:\", round(f1, 4), \"\\tAcc:\", round(accuracy, 4))\n",
    "    win_tracker.loc[len(win_tracker)] = {\"fold\":i, \"mse\": mse, \"rmse\": np.sqrt(mse), \"f1\": f1, \"acc\": accuracy}\n",
    "    i += 1\n",
    "\n",
    "# val_tracker['pred_tf'] = np.where(val_tracker['pred'] >= 0.5, True, False)\n",
    "\n",
    "del i, x_t, X_val, y_t, y_val, model, preds, mse, f1, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1112382",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tracker.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71db6eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(win_tracker, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4d823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_tracker.head()\n",
    "# y_val.to_list()\n",
    "# y_val_out['obstruction_ro']\n",
    "# f1_score(y_val_out['obstruction_ro'].tolist(), y_val_out['pred_tf'].tolist())\n",
    "# f1_score(y_val_out['obstruction_ro'], y_val_out['pred_tf'])\n",
    "# y_val_out['pred_tf'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a7d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train_index, val_index) in enumerate(tscv.split(x_train)):\n",
    "    continue\n",
    "    # print(f\"Fold {i}:\")\n",
    "    # print(f\"  Train: index={train_index}\")\n",
    "    # print(f\"  Test:  index={val_index}\")\n",
    "\n",
    "# print(train_index, \"\\n\", val_index)\n",
    "\n",
    "x_t, X_val = x_train.iloc[train_index], x_train.iloc[val_index]\n",
    "y_t, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "# model = xgb.XGBRegressor(enable_categorical=True, tree_method=\"hist\")\n",
    "model = xgb.XGBClassifier(enable_categorical=True, tree_method=\"hist\")\n",
    "model.fit(x_t, y_t)\n",
    "\n",
    "preds = model.predict(X_val)\n",
    "mse = mean_squared_error(y_val, preds)\n",
    "# f1 = f1_score(y_val_out['obstruction_ro'].tolist(), y_val_out['pred_tf'].tolist())\n",
    "print(\"Validation MSE:\", mse, \"\\tRMSE:\", np.sqrt(mse))\n",
    "\n",
    "del i, train_index, val_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775d2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score(y_val, preds)\n",
    "# preds[1]\n",
    "# y_val_out2 = pd.DataFrame()\n",
    "y_val_out2 = y_val.copy().to_frame()\n",
    "y_val_out2['pred'] = preds\n",
    "y_val_out2['pred_tf'] = np.where(y_val_out2['pred'] == 1, True, False)\n",
    "f1_score(y_val_out2[var_of_interest].tolist(), y_val_out2['pred_tf'].tolist())\n",
    "\n",
    "# del y_val_out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e470b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_val = y_val.copy()\n",
    "mini_val = mini_val.reset_index()\n",
    "mini_val['pred'] = preds\n",
    "mini_val.set_index('index')\n",
    "mini_val['pred_tf'] = np.where(mini_val['pred'] == 1, True, False)\n",
    "mini_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba3b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 1.5))\n",
    "ax.scatter(mini_val['index'], mini_val[var_of_interest], s=25, color='blue', marker=\"|\")\n",
    "ax.scatter(mini_val['index'], mini_val['pred_tf']-.06, s=25, color='orange', marker=\"|\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8da244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_preds(input_date_start, input_date_end, include_preds=True, include_calibration=True):\n",
    "    \"\"\"Plot values between two dates in the style of the Visual FoxPro interface.\n",
    "\n",
    "    Args:\n",
    "        input_date_start (Timestamp): The start date.\n",
    "        input_date_end (Timestamp): The end date.\n",
    "        include_calibration (boolean): Include X-markers for the calibration points.\n",
    "    \n",
    "    Returns:\n",
    "        Time series plot.\n",
    "    \"\"\"\n",
    "    # Filter the data sets\n",
    "    data_subset = data_united.copy()[input_date_start:input_date_end]\n",
    "    # data_subset_rain = data_rainfall.loc[input_date_start:input_date_end]\n",
    "    # data_subset_cal = data_calibration.loc[input_date_start:input_date_end]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.axhline(y=0, color ='grey', linestyle = ':')\n",
    "    # Plot the rain as a bar chart with a multiplier for visibility\n",
    "    ax.vlines(data_subset.index, ymin=0, ymax=data_subset['ra_rain']*3, color = 'blue', label = \"Rain (x3)\")\n",
    "    ax.plot(data_subset.index, data_subset['level_ro'], color = 'red', label = \"Adjusted\")\n",
    "    ax.plot(data_subset.index, data_subset['raw_ro'], color = 'green', label = \"Raw\")\n",
    "    # Include calibration points unless otherwise specified or unless there are none in the subset\n",
    "    if include_calibration == True and not data_subset['weir_level_cal'].empty:\n",
    "        ax.plot(data_subset.index, data_subset['weir_level_cal'], linestyle='none', marker='x', color='red', label = \"Calibration\")\n",
    "    if include_preds == True:\n",
    "        mini_val_subset = mini_val.copy().set_index('index')[input_date_start:input_date_end]\n",
    "        ax.scatter(mini_val_subset.index, (mini_val_subset[var_of_interest]-3)*10, color='blue', marker=\"|\")\n",
    "        ax.scatter(mini_val_subset.index, (mini_val_subset['pred_tf']-5)*10, color='orange', marker=\"|\")\n",
    "\n",
    "    # Plot labels\n",
    "    ax.set_xlabel(\"Date (YYYY-MM-DD)\")\n",
    "    ax.set_ylabel(\"Level (mm)\")\n",
    "    # ax.set_title('Simple Time Series Plot')\n",
    "    ax.set_title(\"Runoff time series from \" + str(input_date_start) + \" through \" + str(input_date_end))\n",
    "    # ax.set_ylim(bottom=0) \n",
    "    # ax.grid(True)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    # Reverse the order of the legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles[::-1], labels[::-1], loc='upper right')\n",
    "    # plt.legend(loc = 'upper right')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe6a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_preds(X_val.index[0], X_val.index[-1])\n",
    "plot_preds('2016-08-01 00:00:00', '2016-08-09 00:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42a7256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_subset = data_united[X_val.index[0]:X_val.index[-1]]\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# plt.axhline(y=0, color ='grey', linestyle = ':')\n",
    "# # Plot the rain as a bar chart with a multiplier for visibility\n",
    "# ax.vlines(data_subset.index, ymin=0, ymax=data_subset['ra_rain']*3, color = 'blue', label = \"Rain (x3)\")\n",
    "# ax.plot(data_subset.index, data_subset['level_ro'], color = 'red', label = \"Adjusted\")\n",
    "# ax.plot(data_subset.index, data_subset['raw_ro'], color = 'green', label = \"Raw\")\n",
    "# # Include calibration points unless otherwise specified or unless there are none in the subset\n",
    "# # if include_calibration == True and not data_subset_cal.empty:\n",
    "# ax.plot(data_subset.index, data_subset['weir_level_cal'], linestyle='none', marker='x', color='red', label = \"Calibration\")\n",
    "\n",
    "# # Plot labels\n",
    "# ax.set_xlabel(\"Date (YYYY-MM-DD)\")\n",
    "# ax.set_ylabel(\"Level (mm)\")\n",
    "# # ax.set_title('Simple Time Series Plot')\n",
    "# # ax.set_title(\"Runoff time series from \" + input_date_start + \" through \" + input_date_end)\n",
    "# # ax.set_ylim(bottom=0) \n",
    "# # ax.grid(True)\n",
    "# plt.xticks(rotation=45, ha='right')\n",
    "# plt.tight_layout()\n",
    "# # Reverse the order of the legend\n",
    "# handles, labels = ax.get_legend_handles_labels()\n",
    "# ax.legend(handles[::-1], labels[::-1], loc='upper right')\n",
    "# # plt.legend(loc = 'upper right')\n",
    "# plt.show()\n",
    "\n",
    "# del data_subset, fig, ax, handles, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score, precision_score, recall_score, accuracy_score\n",
    "y_true = mini_val[var_of_interest].tolist()\n",
    "y_pred = mini_val['pred_tf'].tolist()\n",
    "\n",
    "# Compute the confusion matrix\n",
    "# cm = confusion_matrix(mini_val[var_of_interest].tolist(), mini_val['pred_tf'].tolist())\n",
    "metric_cm = confusion_matrix(y_true, y_pred)\n",
    "# print(\"Confusion Matrix:\\n\", metric_cm)\n",
    "\n",
    "# Precision\n",
    "metric_precision = precision_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "# Recall\n",
    "metric_recall = recall_score(y_true, y_pred)\n",
    "\n",
    "# F1\n",
    "# f1_score = f1_score(mini_val[var_of_interest].tolist(), mini_val['pred_tf'].tolist())\n",
    "metric_f1 = f1_score(y_true, y_pred)\n",
    "# print(\"F1:\\n\", metric_f1)\n",
    "\n",
    "# Accuracy - the total number of correct predictions performed by hte model\n",
    "metric_accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(\n",
    "    \"\\nConfusion Matrix:\\n\", metric_cm,\n",
    "    \"\\nPrecision:\\t\", metric_precision,\n",
    "    \"\\nRecall:\\t\\t\", metric_recall,\n",
    "    \"\\nF1 Score:\\t\", metric_f1,\n",
    "    \"\\nAccuracy:\\t\", metric_accuracy\n",
    ")\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=metric_cm, display_labels=['Negative', 'Positive'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Environment cleanup\n",
    "del y_true, y_pred, metric_cm, metric_precision, metric_recall, metric_f1, metric_accuracy, disp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
