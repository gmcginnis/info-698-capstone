\documentclass[11pt, letterpaper, titlepage]{article}

% Compact lists
\usepackage{enumitem}

% Images
\usepackage{graphicx}

% Tables
\usepackage{booktabs}

% Symbols - for check marks
\usepackage{amssymb}

% Equations
\usepackage{amsmath}

% Document boarders
\usepackage{geometry}
\geometry{
    letterpaper,
    margin=.85in
}

% Remove page numbers
% \pagenumbering{gobble}

% Double spacing
\usepackage{setspace}
% \doublespacing

% Spacing out footnotes
\setlength{\footnotesep}{0.75\baselineskip}

% Put footnotes on bottom of page
\usepackage[bottom]{footmisc}

% % Use alpha for footnotes
% \renewcommand*\thefootnote{\alph{footnote}}
% Use roman footnotes
\renewcommand*\thefootnote{\roman{footnote}}

% Palatino
\usepackage{newpxtext}
\usepackage{newpxmath}

% Nicer monospace font
\usepackage{inconsolata}

% Improved typography
\usepackage{microtype}

% Acronym management
\usepackage{acro}

\DeclareAcronym{bci}{
    short=BCI,
    long=Barro~Colorado~Island
}

\DeclareAcronym{stri}{
    short=STRI,
    long=Smithsonian~Tropical~Research~Institute
}

\DeclareAcronym{vfp}{
    short=VFP,
    long=Visual~FoxPro
}

\DeclareAcronym{lstm}{
    short=LSTM,
    long=Long~Short\nobreak-Term~Memory
}

\DeclareAcronym{xgb}{
    short=XGBoost,
    long=Extreme~Gradient~Boosting
}

\DeclareAcronym{oof}{
    short=OOF,
    long=out\nobreak-of\nobreak-fold
}

\DeclareAcronym{iid}{
    short = \textit{iid},
    long = independently~\&~identically~distributed,
    sort = iid
}

    
\DeclareAcronym{aucpr}{
    short = AUC~PR,
    long = area~under the precision\nobreak-recall~curve
}

\DeclareAcronym{rocauc}{
    short = ROC~AUC,
    long = receiver~operating~characteristic area~under~the~curve
}

\DeclareAcronym{ap}{
    short = AP,
    long = average~precision
}


\DeclareAcronym{tp}{
    short = TP,
    long = true~positive(s)
}

\DeclareAcronym{tn}{
    short = TN,
    long = true~negative(s)
}

\DeclareAcronym{fp}{
    short = FP,
    long = false~positive(s)
}

\DeclareAcronym{fn}{
    short = FN,
    long = false~negative(s)
}

% URL and hyperlink settings
% \usepackage[dvipsnames]{xcolor}
\usepackage[table, dvipsnames]{xcolor}
\usepackage{xurl}
\usepackage[
    colorlinks,
    breaklinks=true,
    urlcolor=NavyBlue,
    citecolor=NavyBlue,
    linkcolor=NavyBlue,
    % breaklinks,
    linktocpage
]{hyperref}
\urlstyle{same}

% Bib settings
\usepackage[
    backend=biber,
    % style=apa,
    style=chem-acs,
    doi=true,
    url=true,
    % url=false,
    articletitle=true,
    maxbibnames=99
    %natbib=true
    ]{biblatex}
\addbibresource{bibliography.bib}

% \usepackage[
%     backend=biber,
%     % style=apa,
%     style=chem-acs,
%     doi=true,
%     url=true,
%     articletitle=true,
%     chaptertitle=true,
%     maxbibnames=99
%     %natbib=true
% ]{biblatex}
\newcommand{\citean}[1]{\citeauthor{#1}\autocite{#1}}

% % % ToC Settings
% \usepackage{tocloft}
% % Dots for sections
% \renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
% % No dots for other sections
% \renewcommand{\cftsubsecdotsep}{\cftnodots}
% \renewcommand{\cftsubsubsecdotsep}{\cftnodots}
% 
% % Spacing for ToC section titles
% \setlength{\cftsubsecnumwidth}{0pt}
% \setlength{\cftsubsubsecnumwidth}{0pt}

% % IEEE settings
% \usepackage{fmtcount}
% \usepackage[titles]{tocloft}
% 
% % TOC entries to use Roman numerals and bold small caps
% \renewcommand{\cftsecfont}{\scshape\bfseries} % Small caps, bold
% \renewcommand{\cftsecpagefont}{\bfseries}      % Page numbers bold
% 
% % Modifying section numbering
% \renewcommand{\thesection}{\Roman{section}}
% \renewcommand{\thesubsection}{\thesection.\Alph{subsection}}
% \renewcommand{\thesubsubsection}{\arabic{subsubsection})} % sub-sections, (1) 2)
% 
% % Adjust spacing
% % \setlength{\cftsecindent}{0pt}
% % \setlength{\cftsecnumwidth}{2em}


% % Custom commands for new section titles to TOC
% \newcommand{\subsection}[1]{%
%     \subsection*{\thesubsection {#1}} %
%     \addcontentsline{toc}{subsection}{\protect\numberline{}{\thesubsection #1}} %
%     \addtocounter{subsection}{1}
% }

% \newcommand{\subsubsection}[1]{%
%     \subsubsection*{\thesubsubsection {#1}} %
%     \addcontentsline{toc}{subsubsection}{\protect\numberline{}{\thesubsubsection #1}} %
%     \addtocounter{subsubsection}{1}
% }

% Section numbering
% \newcommand{\secref}[1]{\S\ref{#1}}
% \newcommand{\secrefdot}[1]{\ref{#1}}
\newcommand{\secref}[1]{\hyperref[#1]{\S\ref{#1}}}
% \newcommand{\secrefdot}[1]{\hyperref[#1]{\ref{#1}.}}
\newcommand{\secrefboth}[1]{\hyperref[#1]{\ref{#1}.} & \nameref*{#1}}

\newcommand{\ditto}{\multicolumn{1}{c}{\texttt{"}}}

% Define custom commands for metrics and labels
% \newcommand{\acc}{\textnormal{accuracy}}
% \newcommand{\prec}{\textnormal{precision}}
\newcommand{\truepos}{\textnormal{TP}}
\newcommand{\trueneg}{\textnormal{TN}}
\newcommand{\falseneg}{\textnormal{FN}}
\newcommand{\falsepos}{\textnormal{FP}}


\begin{document}
\thispagestyle{empty}
\begin{titlepage}
    \begin{center}
        % \vspace*{250px}
        \vspace*{2cm}
        \Huge
        % Machine Learning Algorithm Development for Quality Assurance\\of Lutz Catchment Runoff Data
        Machine Learning for Quality Assurance\\of Lutz Catchment Runoff Data
        
        \bigskip

        \LARGE
        Final Report
        
        \bigskip

        Gillian McGinnis
        
        \bigskip
        % List remaining team members here, one name per line, first name, last name, alphabetically by last name
        
        ADVISORS:\\
        % POTENTIAL ADVISORS:\\ %\emph{LIST POTENTIAL ADVISORS FOR GROUP HERE}
        \Large
        % Dr. Sriram Iyengar (\emph{The University of Arizona College of Medicine, Phoenix}),\\
        % Steven Paton (\emph{Smithsonian Tropical Research Institute})
        Sriram Iyengar, PhD (\emph{The University of Arizona College of Medicine, Phoenix}),\\
        Steven Paton, MSc (\emph{Smithsonian Tropical Research Institute})
        % Dr. Steven Paton (\emph{Smithsonian Tropical Research Institute})
        
        \bigskip

        \LARGE
        December 10, 2025
        % Date: December 10, 2025

        % \bigskip
        \bigskip
        %     \end{center}
% \end{titlepage}

% \begin{titlepage}
%     \title{Machine Learning Algorithm Development for Quality Assurance\\of Lutz~catchment Runoff Data}
%     \author{Gillian McGinnis, Project Manager}
%     % \date{}

%     \maketitle
% \end{titlepage}


    \end{center}
\end{titlepage}
\thispagestyle{empty}




\tableofcontents

\thispagestyle{empty}
\pagebreak
\listoffigures

\listoftables

% \section*{List of Acronyms}
% \begingroup
%     \setlength{\leftskip}{2em}
%     \printacronyms[template=tabular, heading=none]
% \endgroup

\printacronyms[name={List of Acronyms}, template=tabular, display=all]
% \printacronyms[name={List of Abbreviations}]

\thispagestyle{empty}
\pagebreak
\pagenumbering{arabic}

% Abstract, Introduction, Methods, Results, Discussion/Conclusions
% \section*{Abstract}
% \addcontentsline{toc}{section}{Abstract}
\begin{abstract}
    % The primary goal of this project is to create models that can automate quality assurance of temporal runoff data from the Lutz~catchment of Barro~Colorado~Island~(\ac{bci}) on the Republic~of~Panama.
    The long-term goal of this project is to create models that can automate quality assurance of temporal runoff data from the Lutz~catchment of \acl{bci} on the Republic~of~Panama.
    % The algorithms will conduct quality assurance of temporal runoff data from the Lutz~Weir of Barro~Colorado~Island, Panama.
    This will remove the need for manual data corrections, which not only use the valuable time and energy of researchers, but also have the potential to be imprecise or inconsistent.
    % This will streamline the analytical process for researchers, freeing up more time and energy for more pressing analyses.
    Rainfall and soil moisture measurement data will be integrated to produce a more well-informed model.
    % Other variables---such as rainfall and soil moisture measurements---can be integrated to produce a more well-informed model.
    % 
    The optimized \ac{xgb} model for classifying the presence of weir obstructions achieved a recall of 0.838 and precision of 0.206 after threshold optimization, indicating high ability to flag actual blockage events with the tradeoff of flagging many non-problematic data points.
    % 
    Future work will continue to improve on the obstruction model, as well as create detection models for other failure modes and conduct quality assurance on the runoff values accordingly.

\end{abstract}

% \pagebreak

\section{Introduction}\label{sec_es}

Runoff data has been continually collected from the Lutz~weir (Figure~\ref{pic_weir}) since 1972, making it one of the longest, continually monitored micro-catchment datasets for the neotropics.\autocite{mapPC}
The weir is located in the Lutz~catchment, a 9.73~ha area located in 100-year-old, secondary low land tropical rainforest on \acf{bci}, a 15~km\textsuperscript{2} island located in Lake Gatun in the Republic~of~Panama (Figure~\ref{map_bcilutz}).\autocite{mapPC}
% \ac{bci} is located in the middle of the largest lake in the Republic of Panama.
% The Lutz~catchment encompasses 9.73~ham and is located in 100-year-old, secondary low land tropical rainforest on \ac{bci}, a 15~km\textsuperscript{2} island located in the middle of Lake Gatun, the largest lake in the Republic of Panama and a key section of the Panama Canal.
\ac{bci} is operated by the \ac{stri}, a branch of the Smithsonian~Institution.\autocite{stri2025}
% \ac{bci} is one of the most intensely studied tropical rainforests in the world.
% Runoff at the Lutz~catchment---as well as other meteorological and hydrological data---have been continually monitored since 1972, making it one longest, continually monitored micro-catchment datasets for the neotropics.\autocite{mapPC}
% Data from this catchment provide critical information concerning the hydrological balance of \ac{bci}.

% \subsection{Background}

% Runoff data has been electronically collected from the weir since 1989 in frequencies ranging from three~minutes to three~hours, resulting in millions of data points with more added every day.
% Runoff data has been electronically collected from the weir since 1989 in five~minute intervals, resulting in millions of data points with more added every day.

% Runoff data was continually collected in hand-written records (which have since been digitized) in intervals ranging from three~minutes to three~hours, and electronically measured every five~minutes since mid-1989.\autocite{hydroPC}
% Runoff data prior to mid-1989 was collected through hand-written records (which have since been digitized) in intervals ranging from three~minutes to three~hours.
% Electronic sensors have reported values every five~minutes since mid-1989.\autocite{hydroPC}
At first, hand-written records (which have since been digitized) recorded runoff in intervals ranging from three~minutes to three~hours, but since mid-1989, runoff values have been electronically measured every five~minutes.\autocite{hydroPC}
% Runoff data has been continually collected since 1972, at first through hand-written records (which have since been digitized) in intervals ranging from three~minutes to three~hours, and electronically measured every five~minutes since mid-1989.\autocite{hydroPC}
The time series dataset contains millions of data points, with more data continuously added as the sensor continues to record new measurements.
In addition, rainfall measurements have been recorded at the Clearing~station 170~m north of the weir.
Soil moisture measurements have been taken weekly from 10 sampling sites within the catchment at two~depths each (Figure~\ref{map_soil}).\autocite{hydroPC}
% These data sets are available on the Smithsonian~Research~Data~Repository with \texttt{CC~BY~4.0}~licenses.\autocite{srdrRepo}
% All data sets are readily available in the form of \texttt{.csv} files, which are regularly posted to the Smithsonian~Research~Data~Repository with \texttt{CC BY 4.0}~licenses.\autocite{srdrRepo}
% Steven~Paton---director of the Physical Monitoring Program at the \ac{stri}---will be the primary contact regarding background information of the data, knowledge of exceptional data points, and any hydrological questions that come about.


% The weir has been electronically collecting runoff data since 1989 in frequencies ranging from three~minutes to three~hours, resulting in millions of data points with more added every day.
Quality issues in the runoff data come about due to environmental factors (such as partial blockages of the outflow channel), sensor failures, and calibration issues, to name a few.
% Currently, quality assurance is conducted manually in Visual~FoxPro interactive interface.
% Currently, quality assurance is conducted manually using a customized program in Visual~FoxPro~(\ac{vfp}) to allow for visual inspection and correction of the data.
Currently, quality assurance is conducted manually using a custom written program in \ac{vfp}, a popular data management program from the 2000s.
This program permits the visual inspection and correction of the data, a critical feature for manual quality assurance of micro-catchment data.
% Currently, Steven~Paton---director of the Physical Monitoring Program at the \ac{stri}---conducts quality assurance manually using a customized program in Visual~FoxPro~(\ac{vfp}) to allow for visual inspection and correction of the data.
% Steven~Paton---director of the Physical Monitoring Program at the \ac{stri}---will be the primary contact regarding background information of the data, knowledge of exceptional data points, and any hydrological questions that come about.
% Not only does this approach run the risk of imprecise or inconsistent corrections, but the deprecated status of the language and interface leaves it vulnerable.
However, not only does this approach run the risk of imprecise or inconsistent corrections, but the deprecated status of \ac{vfp} leaves it vulnerable to future inaccessibility as it may eventually no longer be supported.
Past attempts at automated quality assurance have failed due to the failure modes' resilience against traditional stochastic methods.
Because quality assurance requires some subjective decisions by the individual analyzing the data using the \ac{vfp} system, the results are potentially biased to some, unknown degree.\autocite{mapPC}

% % % LONGER
% The weir has been electronically collecting runoff data since July 19, 1989, with frequency measurements ranging from three~minutes to three~hours.
% It has been collecting runoff data since January 1, 1972, with frequency of measurements ranging from three~minutes to three~hours.
% Until July 19, 1989, water level was recorded on paper charts, which have since been digitized.
% Frequency of measurements range from three~minutes to three~hours.
% In addition to rainfall measurements from Clearing station 170~m north of the weir, the runoff data contains millions of records over time.
% The set contains millions of data points, with more added as the sensor continues to record measurements.
% Millions of data points are currently present in the set, with more added as the sensor continues to record measurements.
% % 
% Quality issues in the data come about due to the need for re-calibration, debris blockages resulting in monomodal runoff value increases without rainfall, complications from weir blockages paired with rainfall, false positives, impossible sub-zero values, evapotranspiration, considerations for annual intentional pond drains, and signal noise.
% % 
% Currently, quality assurance is conducted manually via the Visual~FoxPro interactive visual interface.
% Not only does this approach run the risk of imprecise or inconsistent corrections, but the deprecated status of the language and interface leaves it vulnerable.
% % 
% % Past attempts at quality assurance using stochastic methods have failed, as past teams have attempted to automate it with stochastic methods but concluded it to be resilient against automation.
% Past attempts at automated quality assurance have failed due to the failure modes' resilience against traditional stochastic methods.
% %

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=.6\textwidth]{figures/larsen2021figs_setup.jpg}
    \caption[Photograph of the Lutz~weir]{A photograph of the Lutz~weir, including the electronic sensor measuring runoff.\autocite{larsen2021figs}}
    \label{pic_weir}
\end{figure}

% \pagebreak

\begin{figure}[hbt!]
    \centering
    \fbox{\includegraphics[width=.6\textwidth]{figures/larsen2021figs_bci.jpg}}
    \caption[Map showing the~Lutz~catchment on \acl{bci} in the~Republic~of~Panama]{The location of the~Lutz~catchment on \acf{bci} in the~Republic~of~Panama.\autocite{larsen2021figs}}
    \label{map_bcilutz}
\end{figure}

% \smallbreak}

\begin{figure}[!hbt]
    \centering
    \fbox{\includegraphics[width=.6\textwidth]{figures/map_bci_soil.jpg}}
    \caption[Fine-scale topographic map of the Lutz~catchment sampling sites]{A fine-scale topographic map of the Lutz~catchment showing the locations of the weir, tower, and 10 soil moisture sampling sites.\autocite{hydroPC}}
    \label{map_soil}
\end{figure}

\pagebreak

% Relevant data sets are available on the Smithsonian~Research~Data~Repository with \texttt{CC~BY~4.0}~licenses.\autocite{srdrRepo}
% All data sets are readily available in the form of \texttt{.csv} files, which are regularly posted to the Smithsonian~Research~Data~Repository online repository with \texttt{CC BY 4.0} licensing.\autocite{srdrRepo}
% Relevant data will first need to be uniformly coded and united into a cohesive set in order to conduct relevant analyses.
% 
% Steven~Paton---director of the Physical Monitoring Program at the Smithsonian~Tropical~Research~Institute---will be the primary contact regarding background information of the data, knowledge of exceptional data point ranges (such as extenuating circumstances, e.g., a tree fall damaging equipment), and any hydrological questions that come about.
% Steven~Paton---director of the Physical Monitoring Program at the \ac{stri}---will be the primary contact regarding background information of the data, knowledge of exceptional data points, and any hydrological questions that come about.
% Project work will be conducted remotely, with regular advisor check-ins via teleconference.

% The goals by the end of the project are to have a system for multiclass classification to flag periods of time with failure mode(s), and models to correct flagged data points.
% The original goals for the project were to have a system for multiclass classification to flag periods of time with failure mode(s), and models to correct flagged data points.
% It was also anticipated that correction models would be developed for at least two failure modes, however time constraints and 
% As more research was conducted, it was determined that alternative approaches would be more effective and accurate.

% By the end of the project, it is anticipated that a model to flag and classify ranges of time containing a failure mode will be possible.
% Ideally, the model will also be able to flag periods with multiple/simultaneous failure modes.
% Additionally, models will be created to attempt quality assurance corrections on flagged time periods.
% Although not all failure modes may be able to be successfully addressed (for instance, there is only one major incident of signal noise), at least two should be addressed.
% Measured success of the models can determine which failure modes are the most difficult to accurately repair.
% Measured success of the models can determine which failure modes are the most difficult to accurately repair.
% Measured success of the models can provide information into the feasibility of applying machine learning (ML) models in the context of micro-catchment runoff data quality assurance, as well as identify the most significant difficulties which can be addressed in future refinements of the models.
%  as well as identify the most significant difficulties to be applied for future refinements of the end product model(s).
%  explore the feasibility of applying current AI models in the context of quality assurance of micro-catchment runoff data, as well as to identify the most significant difficulties to this approach so that they may be worked on in future refinements of the end product model(s).
% To maintain a feasibility \& realistic goal for the semester, it is anticipated that accurate correction models can be made for at least two failure modes.
% It is anticipated that at least two failure modes
% At least two failure modes are expected to be addressed.
% It is exp


% \pagebreak

% % \vspace*{\fill}
% % \begin{figure}[!p]
% \begin{figure}[!hbt]
%     \centering
%     \fbox{\includegraphics[width=.95\textwidth]{figures/map_bci_ca.jpg}}
%     % \caption{The locations of the weir and tower in relation to the 10 soil moisture sampling sites in the Lutz~catchment on Barro~Colorado~Island, Panama.\autocite{stri2025}}
%     \caption{The location of \ac{bci}, Republic~of~Panama, showing the locations of the Lutz~catchment, weir, and tower.\autocite{mapPC}}
%     % \caption{The location of Barro~Colorado~Island in Panama CITE.}
%     \label{fig_map_island}
% \end{figure}
% % \vspace*{\fill}


% \subsection{Background}

\subsection{Failure Modes}

% There are five major failure modes flagged in the data set: obstruction, spike, calibration, weir cleaning, and signal noise.
Periods of time in the data are flagged as containing ``failure modes'', as explained in Table~\ref{tab_fmtypes}.
Some failures occur in extremely short intervals of time (even as few as one reading), while others can spoil data spanning multiple weeks.
One long-term goal of this project is to create systems to determine where there are readings containing a particular failure mode.
% This also requires the system to determine the time range in which a failure mode is occurring.
% A challenge for this feature is that some failures occur in extremely short intervals of time (even as few as one reading), while others can spoil data spanning multiple weeks.
Examples of each failure mode can be found in Appendix~\ref{apx_fm}.

% \begin{table}[htbp]
%     % \small
%     \centering
%     \rowcolors{2}{white}{gray!20}
%     \renewcommand{\arraystretch}{1.23}
%     \begin{tabular}{l p{0.395\textwidth} p{0.395\textwidth}}
%         \toprule
%         Failure mode & Description & Complicating factors \\
%         \midrule
%         Obstruction & Debris blocks the weir's `V', resulting in gradual increases in water level, and later abrupt decreases following blockage removal. & Rainfall events before, during, or shortly after a blockage can interrupt the base flow and decay curves. \\
%         Spike & Short and abrupt changes in level typically caused by equipment issues. & Extremely short-term issue that could be skipped over by random sampling. \\
%         Calibration & Standard checks that require baseline correction or slight data pivot. & Recovery after blockage clearing can render calibration points ineffective.\\
%         Sub-zero & The stream runs dry, or the pond is being drained for cleaning. & Rain during a pond draining can render new data unrecoverable.\\
%         Signal noise & Equipment failure results in impossible variability of reported values. & Impossible to manually fix \& resists standard de-noising.\\
%         \bottomrule
%     \end{tabular}
%     \caption{Overview of weir failure modes\autocite{hydroPC}}
%     \label{tab_fmtypes}
% \end{table}

\begin{table}[htbp]
    \small
    \centering
    \rowcolors{2}{white}{gray!20}
    \renewcommand{\arraystretch}{1.23}
    \begin{tabular}{l p{0.335\textwidth} p{0.335\textwidth} l}
        \toprule
        Failure mode & Description & Complicating factors & Example\\
        \midrule
        Obstruction & Debris blocks the weir's `V', resulting in gradual increases in water level, and later abrupt decreases following blockage removal. & Rainfall events before, during, or shortly after a blockage can interrupt the base flow and decay curves. & Figure~\ref{fig_ex_block}\\
        Spike & Short and abrupt changes in level typically caused by equipment issues. & Extremely short-term issue that could be skipped over by random sampling.  & Figure~\ref{fig_ex_spike}\\
        Calibration & Standard checks that require baseline correction or slight data pivot. & Recovery after blockage clearing can render calibration points ineffective. & Figure~\ref{fig_ex_cal}\\
        Sub-zero & The stream runs dry, or the pond is being drained for cleaning. & Rain during a pond draining can render new data unrecoverable. & Figure~\ref{fig_ex_subz}\\
        Signal noise & Equipment failure results in impossible variability of reported values. & Impossible to manually fix \& resists standard de-noising. & Figure~\ref{fig_ex_noise}\\
        \bottomrule
    \end{tabular}
    \caption[Overview of weir failure modes]{Overview of weir failure modes.\autocite{hydroPC}}
    \label{tab_fmtypes}
\end{table}

% Examples of each failure mode can be found in APPENDIX CITE.
% Not all failure modes require all available variables in order to be successfully flagged, as shown in Table~\ref{tab_fmvars}.
% Obstructions are the most difficult failure mode to detect and correct, and thus were selected as the first failure mode to construct a model for.
% Additionally, the calibration variable can have mixed success, since an obstruction or weir~cleaning can render it useless.

% \begin{table}[htbp]
%     % \small
%     \centering
%     \rowcolors{2}{white}{gray!20}
%     \renewcommand{\arraystretch}{1.23}
%     \begin{tabular}{l | c c c c}
%         \toprule
%         Failure mode & Raw runoff & Rainfall & Calibration & Soil moisture \\
%         \midrule
%         Obstruction & \checkmark & \checkmark & \checkmark & \checkmark \\
%         Spike & \checkmark & & & \\
%         Calibration & \checkmark & & \checkmark & \\
%         Sub-zero & \checkmark & & & \\
%         Signal noise & \checkmark & & & \\
%         \bottomrule
%     \end{tabular}
%     \caption{Variables that can influence each failure mode\autocite{hydroPC}}
%     \label{tab_fmvars}
% \end{table}


\subsection{Model Selection}

Machine learning has been used in hydrology for applications such as predictive runoff modelling\autocite{mohammadi2021} and modelling agricultural drought.\autocite{houmma2022}
Rainfall runoff data collected by citizen scientists in Ethiopia has been visually (via graphical inspections) and quantitatively (via statistical methods) analyzed for quality compared to nearby professional references.\autocite{mengistiea2024}
% In the field of remote sensing and geospatial analysis, recurrent~neural~networks---especially~long short\nobreakdash-term~memory~(\ac{lstm})---are successful for analyzing multi-temporal data, however scalability is limited and they can be prone to overfitting.\autocite{dritsas2025}
% 
In environmental machine learning, improper missing data management techniques can also worsen data leakage.\autocite{zhu2023}

Initially, a multi-class classification model was considered to flag points containing failure modes.
However, not all failure modes require all available variables in order to be successfully flagged, as shown in Table~\ref{tab_fmvars}.
% Multiple different binary models can be constructed, as each failure mode has differing indicators and variable influences (e.g., a “spike” only depends on the raw value itself, “calibration” depends on the raw value versus the calibration point, and “obstruction” can be influenced from the raw value, rainfall, and soil moisture).
Thus, individual models can be constructed based on failure mode and input variables in order to minimize the complexity of each.

\begin{table}[htbp]
    % \small
    \centering
    % \rowcolors{3}{white}{gray!20}
    \rowcolors{3}{gray!20}{white}
    \renewcommand{\arraystretch}{1.23}
    \begin{tabular}{l | c c c c}
        \toprule
         & \multicolumn{4}{c}{Variable} \\
        Failure mode & Raw runoff & Rainfall & Calibration & Soil moisture \\
        \midrule
        Obstruction & \checkmark & \checkmark & \checkmark & \checkmark \\
        Spike & \checkmark & & & \\
        Calibration & \checkmark & & \checkmark & \\
        % Sub-zero & \checkmark & & & \\
        Sub-zero & \checkmark & \checkmark & & \\
        Signal noise & \checkmark & & & \\
        \bottomrule
    \end{tabular}
    % \caption{Variables that relate to each failure mode\autocite{hydroPC}}
    \caption[Variables as they relate to each failure mode]{Variables that relate to or otherwise complicate each failure mode.}
    \label{tab_fmvars}
\end{table}

\pagebreak

Obstructions are the most difficult failure mode to detect and correct, and thus were selected as the first failure mode to construct a model for.
External variables of rainfall and soil moisture content are necessary for a well-informed model.
Additionally, the calibration variable can have mixed success, since an obstruction or weir~cleaning can render it useless.
In the context of modelling expected droughts, \citean{houmma2022} highlights how incorporation of local and microclimate variables are becoming increasingly necessary due to climate change and ``exceptional~situations'' which otherwise make extreme peaks of variables \& interdependencies no longer able to be statistically modelable.

% Despite time series data not being independently \& identically distributed (\textit{iid}), statistical time-series problems can still use binary classification.\autocite{ryabko2012}
Despite time series data not being \ac{iid}, statistical time-series problems can still use binary classification.\autocite{ryabko2012}
For example, in hydrology \citean{hudnurkar2022} applied binary classification to rainfall time series, categorizing ``rain'' or ``not rain''.

Unsupervised anomaly detection (such as isolation forest) would be useful to flag any unobserved or new failure modes, however given the existing number of identifiable failure modes, it is unnecessary for the primary goal.
Support vector machines were also ruled out because they are better for small-to-medium preprocessed regular data, and do not work well for time-series sequences.\autocite{li2018}
% Unsupervised anomaly detection (such as isolation forest) would be useful to flag any unobserved or new failure modes, however given the scope of the project and existing number of identifiable failure modes, it is unlikely to be necessary for this analysis.
% 
% Because failure modes are not temporally dependent (e.g., there is not a consistent sensor failure at regular time intervals), other time series models should not be used.

In the field of remote sensing and geospatial analysis, recurrent~neural~networks---especially \ac{lstm}---are successful for analyzing multi-temporal data, however scalability is limited and they can be prone to overfitting.\autocite{dritsas2025}
% Although long short-term memory (\ac{lstm}) (a type of recurrent neural network) are good for identifying and learning temporal dependency patterns \& sequences, it requires clean \& regular data, and scalability is limited.\autocite{dritsas2025}
Non-continuous nature of other variables may also prevent meaningful pattern detection, meaning that it may struggle with the differing granularity of input variables for detecting obstructions.
\ac{lstm} models are also sensitive to noisy data, and may result in less accurate predictions.\autocite{baek2023}
Furthermore, \ac{lstm} are difficult to interpret, and would thus have resulted in a black box issue as to why a particular failure mode flag was triggered since feature importance is unclear.\autocite{jakubowski2021}

% Ultimately, \ac{xgb} was selected as the first model to try with the obstruction model.
Boosting uses many weak learning models to sequentially boost its performance such that the final model is a strong learner.\autocite{murphy2022}
% 
Gradient tree boosting uses simple \& shallow decision trees as weak learners, finding good terminal nodes using standard regression tree learning on the residuals then resolving for the weights of each leaf by minimizing the loss function.\autocite{murphy2022}
% 
It is optimized for large datasets and is made even faster with parallel processing.\autocite{chen2016}
\acf{xgb} adds a regularizer, utilizes a second-order loss approximation, can sample features internally, and scales better.\autocites{murphy2022}{chen2016}
% Extreme gradient boosting (\ac{xgb}) extends beyond gradient boosted trees by adding a regularizer, utilizing second-order loss approximation, internal feature sampling, and scales better.\autocites{murphy2022}{chen2016}
\ac{xgb} has been used for time series anomaly classification in quality assurance in the manufacturing sector.\autocite{mellah2022}
% It also handles class imbalance better (as is the case with the failure mode flags).
Importantly, it does not require perfectly clean or consistent data---thus, it can handle missing values and noise.\autocite{chen2016}
However, it requires careful hyperparameter tuning and explicit feature inputs since it does not ``learn'' temporal trends.
Ultimately, \ac{xgb} was selected as the first model to attempt for flagging instances of the obstruction failure mode.

% chen2016, 
% murphy2022
% xgbdoc2025

% “How does it go from sample(s) to a model?”
% ➢ Gradient tree boosting involves iteratively adding weak learners (shallow & simple
% decision trees), using gradient descent to minimize the prediction error.
% ➢ New trees will try to improve on the shortcomings of the previous ones (i.e., minimize
% loss).
% ➢ The final model is an ensemble, which sums up the predictions from all the individual
% trees.
% “What is \ac{xgb} actually doing?”
% ➢ Compared to standard gradient tree boosting, \ac{xgb} adds a regularizer, utilizes a
% second-order loss approximation, can sample features internally, and scales better.
% “Is \ac{xgb} using random bootstrap? If so, how does it extract the bootstrap?”
% ➢ Gradient boosted trees do not use bootstrapping.
% ➢ Gradient boosted trees can use (random, without replacement) subsampling.
% ➢ In \ac{xgb} the subsampling ratio can be specified, as can the ratios for subsampling by
% tree, level, or node.
% ○ All default to 1 unless specified (i.e., no subsampling; uses the entirety of the
% input data).
% ○ This sampling defaults to uniform random, but a gradient-based sampling
% method can be used (meaning that instances that are not performing as well are
% more likely to be selected for training), but requires a GPU.
% ○ Column subsampling performance varies by problem, and may actually worsen
% performance for classification tasks.

% “Specific to our data?”
% ➢ *See “Our data” under “Examples” section


% Pros:
% Optimized for large datasets and is faster w/ parallel processing
% Has better accuracy (esp. on large datasets) and predictive performance
% Handles class imbalance better
% SHAP can be implemented to improve model interpretation (see LTSM below)
% Does not require perfectly “clean”/consistent data (i.e., can handle missing values, noise, etc.)

% Cons:
% Requires careful hyperparameter tuning.
% Requires explicit feature inputs (i.e., does not learn temporal trends—would need the specific values given [e.g., “rolling mean”, “change in runoff since 2 hr ago”, etc.])
% Is susceptible to overfitting.
% No built-in temporal memory (however, is often used for time-series forecasting/prediction)


% Obstructions are the most difficult failure mode to detect and correct, and thus were selected as the first failure mode to construct a model for.
% Additionally, the calibration variable can have mixed success, since an obstruction or weir~cleaning can render it useless.

% \begin{table}[htbp]
%     % \small
%     \centering
%     \rowcolors{2}{white}{gray!20}
%     \renewcommand{\arraystretch}{1.23}
%     \begin{tabular}{l | c c c c}
%         \toprule
%         Failure mode & Raw runoff & Rainfall & Calibration & Soil moisture \\
%         \midrule
%         Obstruction & \checkmark & \checkmark & \checkmark & \checkmark \\
%         Spike & \checkmark & & & \\
%         Calibration & \checkmark & & \checkmark & \\
%         Sub-zero & \checkmark & & & \\
%         Signal noise & \checkmark & & & \\
%         \bottomrule
%     \end{tabular}
%     \caption{Variables that can influence each failure mode\autocite{hydroPC}}
%     \label{tab_fmvars}
% \end{table}

\subsection{Data Splitting}

% chronological order is important to preserve, and there would be severe data leakage if test samples were taken sporadically throughout the set.
% Things to keep in mind:
% Cannot randomly or selectively split—periods of time must remain continuous wherever possible.
% Time series is not iid (independent & identically distributed); random splits cause data leakage
% Number of instances of each failure mode in each set must be representative enough
% Certain failure modes are rare or have no occurrences for long periods of time.

Unlike other applications of train/test splits in machine learning, time-series data in this context should not be randomly split, and must instead remain continuous whenever possible.
% This is because time-series data is not \textit{iid}, so random splits would cause data leakage.
This is because time-series data is not \ac{iid}, so random splits would cause data leakage.
% 
One consideration when making a train/test split is that real-world variables causing inconsistencies in the data or different concentrations of error instances make a simple split difficult define without careful pre-processing.

Time series analyses also use ``window'' splits, which involve training on a window of time, validating on an immediately proceeding window, and repeating as necessary.
This approach is beneficial for tuning hyperparameters, as it can indicate model stability over time and reflects real-world implementation.
% \cite[Figure~1]{chu1995}

\begin{figure}[hbt!]
    \centering
    \fbox{\includegraphics[width=.65\textwidth]{figures/chu1995.png}}
    % \fbox{\includegraphics[height=.95\textheight]{figures/chu1995.png}}
    % \caption{Different time series segmentation techniques (\citeauthor{chu1995}, \citeyear{chu1995}, Figure~1.\autocite{chu1995}).}
    \caption[Different time series segmentation techniques]{A comparison of different time series segmentation techniques.\autocite{chu1995}}
    \label{fig_chu1995}
\end{figure}

% \autocite{chu1995}



\pagebreak

\section{Methods}

% All data sets are readily available in the form of \texttt{.csv} files, which are regularly posted to the Smithsonian~Research~Data~Repository with \texttt{CC BY 4.0}~licenses.\autocite{srdrRepo}
Analyses were conducted in Python and compiled in Jupyter~Notebooks.
Annotated code, data files, and project reports can be found in the project's GitHub repository.\footnote{Project repository: \url{https://github.com/gmcginnis/info-698-capstone}.}

% Prior to analysis, a significant amount of background research was conducted to determine an appropriate model and methodology for the time-series binary classification.

% 
% For the obstruction failure mode flagging model, the following approach was used:
% 
% \begin{singlespace*}
% \setlist{nolistsep}

% \begin{enumerate}[noitemsep]
%     \item Import data
%     \item Prepare data
%     \item Engineer features
%     \item Conduct train/test split % accidentally reversed in poster
%     \item Remove highly-correlated features % accidentally reversed in poster
%     \item Tune \ac{xgb} hyperparameters
%     \item Fit to get out-of-fold (\ac{oof}) predictions
%     \item Tune post-hoc smoothing parameters
%     \item Fit tuned \ac{xgb} model to entire training set
%     \item Predict on the test set
%     \item Analyze results (with and without post-hoc parameters)
% \end{enumerate}

% \end{singlespace*}
% Methods for Blockage flagging
% Language: 	Python (Jupyter Notebooks)
% Dataset:		n = 3,472,682 annotated points
% Process:
% Introduce raw data (imported from CSV)
% Prepare data (clean & wrangle)
% Engineer features (time, lags, & rolling stats)
% Remove high-correlation features (>0.97)
% Conduct Train/Test split (80:20)
% Tune \ac{xgb} hyperparameters
% 	(w/ PR-AUC—good for imbalanced classes)
% Fit to get out-of-fold predictions (\ac{oof})
% Tune post-hoc smoothing parameters w/ $F_1$
% 	(median-windowing & classification threshold)
% Fit tuned \ac{xgb} model to entire training set
% Predict on the test set (the held-out 20%)
% Analyze results (w/ & w/o post-hoc params.)

\subsection{Data Preparation}
% \subsection{\thesection.1 Import data}
% \subsection*{1) Import data}
% \subsubsection{Data import}
\subsubsection{Importing}

All data sets are readily available in the form of \texttt{.csv} files, which are regularly posted to the Smithsonian~Research~Data~Repository with \texttt{CC BY 4.0}~licenses.\autocite{srdrRepo}
% The data is in four files, relating to runoff, rain, shallow-depth soil moisture
In their raw form, the data were stored in five individual data sets: runoff measurements (including raw and adjusted measurements), rain measurements, weir calibration values, shallow-depth soil moisture measurements, and deep-depth soil moisture measurements.

% \subsection*{2) Prepare data}
% \subsection{Data preparation}
\subsubsection{Wrangling}

In order to conduct model analyses, data first had to be uniformly coded and united into cohesive sets.
% 
% 
% 
% \subsubsection{Date Selections}
Because electronic monitoring did not begin until mid-1989, values that relied on digitization of manually-collected charts were first removed from the runoff datasets.
Occasional sensor failures also resulted in modern records relying on these manual fallbacks, so those values were also removed.
The largest of these failures was a window beginning in January~2013 when the sensor failed and there was no backup until a different one was installed in August~2014.
Because these values relied on the manually-collected chart resource, they were also removed prior to modelling.
% The sensor failed in early 2013, and there was no backup. Values were recording using the chart resource, and gap filled accordingly. Electronic recording resumed with a different sensor in late 2014.
Some entries were lacking in a data source flag, so those which were between chart-reliant values were also removed.

Soil data was not collected from late~February through late~December~2020 due to the COVID-19 pandemic.
Because the obstruction failure mode detection depends on these values (due to differences in how they appear in drier seasons versus wetter seasons), this window of time was also removed from the data prior to model creation.

Data points which were both marked as ``missing'' and had reported raw runoff values of $-999.0$ were also removed
Other gaps of missing values occur and should be addressed.\footnote{This also differs from instances of gap fills.}
In the manually-adjusted results, the levels of such points had been set to $0.0$, but because there were so few instances of them in the filtered data set, they were removed for obstruction analysis ($n=12$).

% \subsubsection{Soil Moisture Abnormalities}
% \subsubsection*{Soil moisture abnormalities}
\paragraph{Soil moisture abnormalities}

There were some discrepancies in records between the shallow and deep soil moisture data sets.
Although the ``deep'' set is for depths of 30-40~cm, some entires from 0-10~cm were included.
Most of these values matched those in the ``shallow'' (1-10~cm) data based on timestamp and sample site number, however there were a few instances where the reported values differed.
It was ultimately decided that those mismatched values ($n=11$) could safely be eliminated from the ``deep'' set.
Although this issue did not remove a significant amount of entries when compared to the resulting totals ($n=11,489$), it had not been found previously and thus made a substantive contribution to the \ac{stri} set.

Additionally, a few ``shallow'' samples with non-standard location numbers were removed ($n=3$).\footnote{Although it was possible that a minor mis-coding caused this issue, it was not possible at the time of discovery to verify the solution because the primary data contact was unavailable for the majority of the project duration due to the federal government shutdown.}
% Although it was possible that a minor mis-coding caused this issue, it was not possible at the time of discovery to verify the solution.\footnote{This was because the primary data contact was unavailable for the majority of the project duration due to the federal government shutdown.}
% 
Duplicated entries were also removed, as were those flagged as being ``bad'' or ``doubtful'' values.

% \subsubsection{Failure Mode Flags}
% \subsubsection*{Failure mode flags}
\paragraph{Failure mode flags}

The failure modes were originally encoded in singular string entries.
Some data points have multiple failure modes, and some entries contained duplicated flags.
For analysis, a series of boolean variables were created to represent these.
These consisted of: obstruction, spike, gap~fill (often related to sub-zero failure mode), weir~cleaning (also related to sub-zero), spike, and calibration.


%    \item Import data
%     \item Prepare data

% \subsection*{3) Engineer features}
% \subsection{Feature Engineering}
\subsubsection{Feature engineering}

Because \ac{xgb} models use one entry of input features at a time, data had to be extended such that each entry also contains information of data before it.
These representations came in the form of rolling statistics (e.g., mean rainfall over the past three hours) and lag features (a shift to a past data point, e.g., runoff 10 minutes prior).
In the context of obstruction detection, rolling statistics included the sum and standard deviation of runoff values in windows spanning 10~minutes~(min), 15~min, 20~min, 25~min, 30~min, 1~hour~(hr), 3~hr, 6~hr, 12~hr, and 24~hr prior to the data point, and rainfall values in windows spanning 10~min, 30~min, 1~hr, 3~hr, and 6~hr prior.
Every other runoff value from 5~min through 3~hr prior were added as lag features, as were the lags for every aforementioned rain rolling sum features from 15~min, 30~min, 1~hr, 2~hr, and 3~hr prior.

Temporal distances from events were also calculated and saved as features.
This included minutes since a rainfall occurred, minutes since a calibration point was taken, and days since a soil moisture sample was taken.
Change from the immediately preceding runoff measurement and rain measurement were also added as features, as was a feature which calculated the cumulative value for any rain ``events'' (e.g., the cumulative sum for a continuous rain lasting 30~minutes).
A decay feature was also added for this cumulative rain feature, which duplicated the cumulative rain but added values forward weighted by an exponential decay value.
The decay rate was set to -0.1, but would benefit from expert insight for a more precise value to better represent actual flow decay behavior.

Features to represent seasonality and time-of-day considerations were also added.
These took the form of day of the year, month of the year, hour of the day, and minute of the the hour, each transformed with sine and cosine to allow the model to be based on the cyclical patterns of time rather than abrupt distances.\footnote{For example, the numeric value for the day of the year of December~31 (365) is `far' from that of January~01 (001), but in reality they are very near}.
% Additional features were

%     \item Conduct train/test split
% \subsection*{4) Conduct train/test split}
% \subsection{Train/Test Split}
\subsubsection{Train/test split}

% Unlike other applications of train/test splits in machine learning, time-series data in this context should \textit{not} be randomly split.
% This is because chronological order is important to preserve, and there would be severe data leakage if test samples were taken sporadically throughout the set.
An initial 80/20 train/test split was conducted on the data.
% Thus, an 80/20 train/test split was conducted on the data. %
% 
% For the obstruction failure mode, this resulted in a training set spanning from 19~July~1989~11:55 through 08~March~2018~21:50 ($n=2,778,145$) and a test set from 08~March~2018~21:55 through 01~August~2025~13:00 ($n=694,537$).
The sets were also separated into feature variables ($X$) and the target variable ($y$, in this case the obstruction failure mode flag).
% An 80/20 train/test split was conducted on the data.
% Time-series data should not be randomly split like typical \textit{k}-fold cross-validation.

A visual overview of this initial 80/20 split and methodology for splits in subsequent steps is provided in Figure~\ref{fig_methods_split}.

\begin{figure}[hbt!]
    \centering
    \fbox{\includegraphics[width=.5\textwidth]{figures/methods_split.png}}
    \caption[Splitting techniques used in the analysis]{Visual representation of splitting techniques used in the analysis.}
    \label{fig_methods_split}
\end{figure}

%     \item Remove highly-correlated features
% \subsection*{5) Remove highly-correlated features}
% \subsection{Feature Selection}
\subsubsection{Feature selection}

To reduce high-dimensionality, avoid overfitting, and improve processing time \& internal memory, highly-correlated features were removed.
This was conducted by using an existing function to compute the pairwise Pearson correlation between all features in the $X$ training set, and removing one of those in pairs in which correlation was calculated as greater than 0.97.
% In the context of the obstruction failure mode model, there were 31 features removed.%
% \footnote{These included: a forward-fill of the cumulative rain events; the cumulative rain event with extended decay; the runoff rolling sums of 10~min, 15~min, 30~min, \& 1~hr; the differences in runoff value \& rain from the previous entry; the runoff values from 10~min, 20~min, 30~min, \& 40~min prior; the 15~min \& 30~min lagged values of 3~hr \& 6~hr rolling sums of rain; days since soil shallow samples 2 thru 10 \& days since deep samples 1 thru 10 (thus, leaving ``days since shallow sample 1'' in the set).}

% \footnote{\verb|ffill_eventsum_ra_rain, decay-0.1_eventsum_ra_rain, raw_ro_rollsum_2, raw_ro_rollsum_3, raw_ro_rollsum_6, raw_ro_rollsum_12, raw_ro_change, ra_rain_change, raw_ro_lag2, raw_ro_lag4, raw_ro_lag6, raw_ro_lag8, ra_rain_rollsum_36_lag3, ra_rain_rollsum_36_lag6, ra_rain_rollsum_72_lag3, ra_rain_rollsum_72_lag6, daysince_2.0_shallow, daysince_4.0_shallow, daysince_5.0_shallow, daysince_9.0_shallow, daysince_10.0_shallow, daysince_1.0_deep, daysince_2.0_deep, daysince_3.0_deep, daysince_4.0_deep, daysince_5.0_deep, daysince_6.0_deep, daysince_7.0_deep, daysince_8.0_deep, daysince_9.0_deep, daysince_10.0_deep|}

% \begin{verbatim}
%     ffill_eventsum_ra_rain, decay-0.1_eventsum_ra_rain, raw_ro_rollsum_2, raw_ro_rollsum_3, raw_ro_rollsum_6, raw_ro_rollsum_12, raw_ro_change, ra_rain_change, raw_ro_lag2, raw_ro_lag4, raw_ro_lag6, raw_ro_lag8, ra_rain_rollsum_36_lag3, ra_rain_rollsum_36_lag6, ra_rain_rollsum_72_lag3, ra_rain_rollsum_72_lag6, daysince_2.0_shallow, daysince_4.0_shallow, daysince_5.0_shallow, daysince_9.0_shallow, daysince_10.0_shallow, daysince_1.0_deep, daysince_2.0_deep, daysince_3.0_deep, daysince_4.0_deep, daysince_5.0_deep, daysince_6.0_deep, daysince_7.0_deep, daysince_8.0_deep, daysince_9.0_deep, daysince_10.0_deep
% \end{verbatim}

% df.corr(): When called on a Pandas DataFrame, this method computes the pairwise correlation between all columns in the DataFrame. By default, it uses the Pearson correlation coefficient, which measures the linear relationship between continuous variables.

\subsection{Modelling}
%     \item Tune \ac{xgb} hyperparameters
% \subsection*{6) Tune \ac{xgb} hyperparameters}
% \subsection{\ac{xgb} Hyperparameter Tuning}
\subsubsection{Hyperparameter tuning}

% To tune the \ac{xgb} hyperparameters, the training data was further split into five expanding
% \ac{xgb} requires careful hyperparameter tuning.
To tune \ac{xgb} hyperparameters, an expanding window cross-validation (Figure~\ref{fig_chu1995}.1b) approach was used.
This is similar to how a model would act once deployed, as it would only gain more data over time.
Five splits on the training data were conducted, with the internal ``training'' set expanding with each fold (Figure~\ref{fig_methods_split}).
Because the data is mostly high-resolution, a larger window width is permissible.\autocite{ranjan2021}

% For 50 iterations, for each of the five expanding-window splits,
In each of 50 iterations, a unique and randomly-selected combination of hyperparameters was selected from predefined distributions.
The set was then evaluated across the five expanding-window splits, wherein the model was trained on the subset and used an internal validation set for early stopping to determine the optimal number of boosting ``rounds'' necessary before performance degrades or has diminishing returns.
The performance metric for all rounds was the \ac{aucpr}.\footnote{In this context, \ac{aucpr} is the better metric compared to \acf{rocauc} since it is better-suited for imbalanced classes and places more emphasis on positive cases, and is superior to $F_1$ since it considers the full range of classification thresholds.}
The specific set of hyperparameters and boosting rounds found to yield the highest \ac{aucpr} score across all cross-validation folds were then selected as the options for the model.

% The resulting best hyperparameters are provided in Table~\ref{tab_xgbparams}.

% \begin{table}[htbp]
%     % \small
%     \centering
%     \rowcolors{2}{white}{gray!20}
%     \renewcommand{\arraystretch}{1.23}
%     \begin{tabular}{l p{0.35\textwidth} r}
%         \toprule
%         Parameter & Purpose & Tuned value \\
%         \midrule
%         \textit{n} estimators & Number of trees & 122 \\
%         Learning rate & Impact of new trees & 0.102 \\
%         Max depth & Maximum depth of individual trees & 3 \\
%         Subsample & Random subset of training rows when building each tree & 0.646 \\
%         Column subsample by tree & Random subset of features when building each tree & 0.709 \\
%         Scale positive weight & Handles class imbalance & 11 \\
%         Gamma & Minimum loss reduction to split & 0.128 \\
%         Alpha & L1 regularization value & 0.936 \\
%         \bottomrule
%     \end{tabular}
%     \caption{Tuned hyperparameters for the \ac{xgb} classification model of obstruction detection.}
%     \label{tab_xgbparams}
% \end{table}



% ------------------------------------------------------------
% Fold 1:
% 	Train:	1989-07-19 11:55:00 thru 1993-12-27 17:05:00
% 	Val:	1993-12-27 17:10:00 thru 1998-09-15 04:50:00
% ------------------------------------------------------------
% Fold 2:
% 	Train:	1989-07-19 11:55:00 thru 1998-09-15 04:50:00
% 	Val:	1998-09-15 04:55:00 thru 2003-02-23 05:55:00
% ------------------------------------------------------------
% Fold 3:
% 	Train:	1989-07-19 11:55:00 thru 2003-02-23 05:55:00
% 	Val:	2003-02-23 06:00:00 thru 2007-08-12 15:15:00
% ------------------------------------------------------------
% Fold 4:
% 	Train:	1989-07-19 11:55:00 thru 2007-08-12 15:15:00
% 	Val:	2007-08-12 15:20:00 thru 2012-01-26 23:30:00
% ------------------------------------------------------------
% Fold 5:
% 	Train:	1989-07-19 11:55:00 thru 2012-01-26 23:30:00
% 	Val:	2012-01-26 23:35:00 thru 2018-03-08 21:50:00
% ------------------------------------------------------------


% As per \ac{xgb}ing documentation and tutorials, early stopping with random search for hyperparameter tuning had to be iterated upon manually, as RandomizedSearchCV does not support using a separate validation set within each CV fold.
% Source: https://xgboosting.com/xgboost-early-stopping-with-random-search/

% The area under the precision-recall curve (AUC-PR) can be used to evaluate the performance, since it considers a range of classification thresholds. This is better than an ROC AUC metric since there is greater class imbalance (i.e., True is more rare).
% Source: https://xgboosting.com/evaluate-xgboost-performance-with-precision-recall-curve/
% \ac{xgb} information here.


% \subsection*{7) Fit to get out-of-fold (\ac{oof}) predictions}
% \subsection{Out-of-Fold Predictions}
\subsubsection{Out-of-fold predictions}
%     \item Fit to get out-of-fold (\ac{oof}) predictions

% Out-of-fold (\ac{oof}) predictions are used be used to tune smoothing and thresholding parameters. To do this, the model with tuned hyperparameters will predict each sliding window set from before. This reflects the real-world performance of the model as it can fit to more data.

For further post-hoc tuning, \ac{oof} predictions were collected using the same expanding-window splits as in hyperparameter tuning, fitting to each fold's ``training'' set and storing the predicted probability values on the window's ``test'' set (Figure~\ref{fig_methods_split}).

% Once tuned, post-hoc parameters---specifically, median-smoothing windows and the classification threshold---were tuned using \ac{oof} predictions.
% These predictions use the same expanding-window splits as in hyperparameter tuning, fitting to each fold's ``training'' set and storing the predicted probability values on the window's ``test'' set (Figure~\ref{fig_methods_split}).
% Smoothing & Thresholding

% \subsection*{8) Tune post-hoc smoothing parameters}
% \subsection{Tuning Post-Hoc Parameters}
\subsection{Tuning post-hoc parameters}
%     \item Tune post-hoc smoothing parameters

Smoothing can improve predictions by preventing standalone points that differ from their neighbors.\footnote{e.g., having a sequence of ``True'' interrupted by one ``False'', or vice-versa, both of which are unlikely in the context of obstructions.}
% Windowing can help smooth predictions by preventing standalone points that differ from their neighbors (e.g., having a sequence of True interrupted by one False, or vice-versa, both of which are unlikely in this context due to how weir blockages occur).
Median windowing is often used for image analysis, but is effective at removing outliers or interruptions in the data while maintaining edges.

By default, a threshold of $0.5$ is selected in binary classification for categorizing a point as ``True'' or ``False''.
However, in practice a model more or less sensitive to ``True'' may make final results more useful.
These measures were found by finding the combination of median window size and threshold that maximizes the $F_1$ score.\footnote{$F_1$ balances considerations for both precision and accuracy.}

Applying smoothing via median windows of sizes from 1 (no smoothing) through~35 were tested with 100~thresholds ranging from 0.01~through~0.99.
% Initially, the best parameters selected were a median windowing size of 29 and a classification threshold of 0.307 ($F_1=0.463$).

% \subsubsection*{Removing marginal gains}
\paragraph{Removing marginal gains}
% Remembering Occam's Razor... marginal gains in $F_1$ can result in over-fitting (such as applying large smoothing windows for an $F_1$ gain of, for instance, 0.0003).
% The changes in $F_1$ differed so little from one median window to another, another
% Prioritizing minor changes in $F_1$ can result in over-fitting or creating an overly-complex model process (such as applying large smoothing windows for an $F_1$ gain of only 0.0003 in the \ac{oof} set).
Prioritizing minor changes in $F_1$ can result in over-fitting or creating an overly-complex model process.\footnote{For example, applying a large smoothing window for an $F_1$ gain of only 0.0003 in the \ac{oof} set may have diminishing returns.}
% Thus, prior to finalizing post-hoc parameters, window/threshold combinations with an $F_1$ 99\% similar to the ``best'' $F_1$ were selected, and the final combination with the smallest window was selected.
% 
% Thus, the window/threshold combination with the smallest median window and an $F_1$ 99\% similar to the ``best'' $F_1$ was selected; the resulting values were no windowing and a threshold of 0.317.
Thus, the window/threshold combination with the smallest median window and an $F_1$ 99\% similar to the ``best'' $F_1$ was selected.

% The results of threshold and $F_1$ score for all median windows are shown in Figure~\ref{fig_oof_tune}.\footnote{All 30 windows reported extremely similar $F_1$ score curves as the classification threshold was adjusted, making it difficult to visually distinguish them individually in the graphic.}

% \begin{figure}[hbt!]
%     \centering
%     \includegraphics[width=.6\textwidth]{outputs/figures/oof_tune.png}
%     % \includegraphics{outputs/figures/oof_tune.png}
%     \caption{$F_1$ vs threshold size for all windows on \ac{oof} set.}
%     \label{fig_oof_tune}
% \end{figure}


% \subsection*{9) Fit tuned \ac{xgb} model to entire training set}
% \subsection{Fitting Tuned Model}
\subsection{Testing Model}
\subsubsection{Fitting tuned model}

Prior to analyzing performance, the tuned \ac{xgb} model with optimal parameters was fitted to the entire training set (i.e., the first 80\% of the data) (Figure~\ref{fig_methods_split}).
%     \item Fit tuned \ac{xgb} model to entire training set

% \subsection*{10) Predict on the test set}
% \subsection{Test Set Prediction}
\subsubsection{Test set prediction}

Finally, the fitted model was used to predict the values on the test set (i.e., the held-out 20\% of the data).
% The resulting precision-recall curve (showing performance across different thresholds) is shown in Figure~\ref{fig_final_prcurve}.
The outputs were stored as predicted probabilities in order to test the different threshold performances.

% \begin{figure}[hbt!]
%     \centering
%     \includegraphics[width=.6\textwidth]{outputs/figures/final_prcurve.png}
%     % \includegraphics{outputs/figures/oof_tune.png}
%     \caption{Precision-recall curve of the test set with average precision (AP) values.}
%     \label{fig_final_prcurve}
% \end{figure}
%     \item Predict on the test set


%     \item Analyze results (with and without post-hoc parameters)
% \subsection*{11) Analyze results (with and without post-hoc parameters)}
% \subsection{Analyze Results}
\subsubsection{Performance analysis}

To quantify performance improvement using the tuned threshold, results were calculated using the default, initial-best (with median smoothing), and final thresholds.

In binary classification, the predicted values are compared to the actual values of $y$.
True~positives~(TP) are instances where the model-predicted value and the actual value are both ``True''.
True~negatives~(TN) are when the prediction and actual value are both ``False''.
False~positives~(FP) are instances where the model incorrectly predicts ``True'', while the actual value is ``False''.\footnote{This is analogous to a false alarm.}
False~negatives~(FN) are when the model predicts incorrectly ``False'', as the actual value is ``True''.

\pagebreak
% \noindent
Four metrics utilizing these values were used to evaluate model performance:

\noindent
% Accuracy (Equation~\ref{eq_accuracy}) quantifies the proportion of correct predictions---however, this metric can be misleading on data with imbalanced classes and thus should be considered a less important performance measurement.\footnote{This metric can be misleading on data with imbalanced classes. For example, if 90\% of a data set's true values are ``False'', a model that always predicts ``False'' is still 90\% accurate.}
Accuracy\footnote{This metric can be misleading on data with imbalanced classes. For example, if 90\% of a data set's true values are ``False'', a model that always predicts ``False'' is still 90\% accurate.} quantifies the proportion of correct predictions:
% Out of all the predictions made, how many were correct?
% Metric can be misleading on data w/ imbalanced classes
\begin{equation}
    \textnormal{accuracy} = \frac{\truepos + \trueneg}{\truepos + \trueneg + \falseneg + \falsepos}
    \label{eq_accuracy}
\end{equation}

\noindent
% Precision (Equation~\ref{eq_precision}) measures the proportion of ``True'' predictions that were correct.
Precision measures the proportion of ``True'' predictions that were correct:
% Out of the positive predictions made, how many were correct?
\begin{equation}
    \textnormal{precision} = \frac{\truepos}{\truepos + \falsepos}
    \label{eq_precision}
\end{equation}

\noindent
% Recall (Equation~\ref{eq_recall}) measures the fraction of actual ``True'' values that the model correctly identified.
Recall measures the fraction of actual ``True'' values that the model correctly identified:
\begin{equation}
    \textnormal{recall} = \frac{\truepos}{\truepos + \falseneg}
    \label{eq_recall}
\end{equation}

\noindent
% $F_1$ (Equation~\ref{eq_fone}) is a measure that combines precision \& recall.
$F_1$ is a measure that combines precision \& recall:
\begin{equation}
    F_1 = 2\times\frac{\textnormal{precision}\times\textnormal{recall}}{\textnormal{precision} + \textnormal{recall}}
    \label{eq_fone}
\end{equation}


% The results are reported in Table~\ref{tab_metrics}.

% \begin{table}[htb!]
%     % \small
%     \centering
%     \rowcolors{2}{white}{gray!20}
%     \renewcommand{\arraystretch}{1.23}
%     \begin{tabular}{r r | r r r r}
%         \toprule
%         Smoothing window & Threshold & Accuracy & Precision & Recall & $F_1$ \\
%         \midrule
%         - & 0.5 & 0.667 & 0.253 & 0.686 & 0.370 \\
%         29 & 0.5 & 0.669 & 0.255 & 0.686 & 0.371 \\
%         29 & 0.307 & 0.509 & 0.204 & 0.845 & 0.329 \\
%         - & 0.317 & 0.518 & 0.206 & 0.838 & 0.331 \\
%         \bottomrule
%     \end{tabular}
%     \caption{Performance metrics of the final model at different smoothing and thresholds.}
%     \label{tab_metrics}
% \end{table}


\pagebreak

\section{Results}

A time series of runoff and soil moisture samples of the complete data set used in the analysis is shown in Figure~\ref{fig_ts_month_rosoil}.

\begin{figure}[hbt!]
    \centering
    % \fbox{\includegraphics[width=.95\textwidth]{outputs/figures/ts_month_rosoil.png}}
    \includegraphics[width=\textwidth]{outputs/figures/ts_month_rosoil.png}
    \caption[Time series of monthly average runoff and soil moisture values]{Time series showing monthly averages of raw (green) \& adjusted (red) runoff and soil moisture values (pink \& purple for shallow \& deep, respectively).}
    \label{fig_ts_month_rosoil}
\end{figure}

% Annotated code, data files, and project reports can be found in the project's GitHub repository.\footnote{Project repository: \url{https://github.com/gmcginnis/info-698-capstone}.}

% Performance metrics information here.

% \subsection{Highly-correlated features}

The original dataset had 22~features.\footnote{These included raw runoff, rain, and 20~soil~moisture measurements (as there were 10~total sites at two depths each).}
In total, 117~additional~features were added via feature engineering.
Applying the correlation threshold~of~0.97 resulted in the removal of 31~features from the feature dataset.%
% In the context of the obstruction failure mode model, there were 31 features removed.%
\footnote{These included: a forward-fill of the cumulative rain events; the cumulative rain event with extended decay; the runoff rolling sums of 10~min, 15~min, 30~min, \& 1~hr; the differences in runoff value \& rain from the previous entry; the runoff values from 10~min, 20~min, 30~min, \& 40~min prior; the 15~min \& 30~min lagged values of 3~hr \& 6~hr rolling sums of rain; days since shallow soil samples 2~thru~10 \& days since deep~samples 1~thru~10 (thus, leaving ``days since shallow sample 1'' in the set).}
This resulted in a total of 108 input features ($X$).
% 
% \subsection{Split}
% 
For the obstruction failure mode, the initial 80/20 split resulted in a training set spanning from 19~July~1989~11:55 through 08~March~2018~21:50 ($n=2,778,145$) and a test set from 08~March~2018~21:55 through 01~August~2025~13:00 ($n=694,537$).

% \subsection{\ac{xgb} hyperparameters}

The tuned \ac{xgb} hyperparameters are provided in Table~\ref{tab_xgbparams}.

\begin{table}[htbp]
    % \small
    \centering
    \rowcolors{2}{white}{gray!20}
    \renewcommand{\arraystretch}{1.23}
    \begin{tabular}{l p{0.35\textwidth} r}
        \toprule
        Parameter & Purpose & Tuned value \\
        \midrule
        \textit{n} estimators & Number of trees & 122 \\
        Learning rate & Impact of new trees & 0.102 \\
        Max depth & Maximum depth of individual trees & 3 \\
        Subsample & Random subset of training rows when building each tree & 0.646 \\
        Column subsample by tree & Random subset of features when building each tree & 0.709 \\
        Scale positive weight & Handles class imbalance & 11 \\
        Gamma & Minimum loss reduction to split & 0.128 \\
        Alpha & L1 regularization value & 0.936 \\
        \bottomrule
    \end{tabular}
    \caption[Tuned hyperparameters for the obstruction detection classification model]{Tuned hyperparameters for the \ac{xgb} classification model of obstruction detection.}
    \label{tab_xgbparams}
\end{table}

% \subsection{Post-hoc parameters}
\pagebreak
After applying the best hyperparameters and fitting to get \ac{oof} predictions, the best post-hoc parameters initially selected were a smoothing windowing size of 29 and a classification threshold of 0.307 ($F_1=0.463$).
After controlling for marginal gains (i.e., 99\% similarity in $F_1$), the resulting values were no windowing and with a threshold of 0.317.
The results of threshold and $F_1$ score for all smoothing windows are shown in Figure~\ref{fig_oof_tune}.\footnote{All 30 windows reported extremely similar $F_1$ score curves as the classification threshold was adjusted, making it difficult to visually distinguish them individually in the graphic.}

\begin{figure}[hbtp!]
    \centering
    % \includegraphics[width=.6\textwidth]{outputs/figures/oof_tune.png}
    \includegraphics[width=.85\textwidth]{outputs/figures/oof_tune.png}
    % \includegraphics{outputs/figures/oof_tune.png}
    \caption[Performance of different classificaiton thresholds on \acl{oof} data]{$F_1$ vs classification threshold for all windows on \acl{oof} set.}
    \label{fig_oof_tune}
\end{figure}

\pagebreak

% \subsection{Fitted model}

The resulting precision-recall curve of the model after being fitted on the full training set and predicting probabilities of the held-out test set is shown in Figure~\ref{fig_final_prcurve}.
After being fitted on the full training set and predicting the held-out test set, the resulting metrics of model performance with different post-hoc parameters are reported in Table~\ref{tab_metrics}.
% The resulting precision-recall curve of the predicted probabilities of the fitted model on the test set is shown in Figure~\ref{fig_final_prcurve}.
% The resulting precision-recall curve after being fitted on the full training set and predicting probabilities of the held-out test set is shown in Figure~\ref{fig_final_prcurve}, and the resulting metrics of the performance with different post-hoc parameters are reported in Table~\ref{tab_metrics}.
% A time series showing monthly averages of model accuracy over the complete test set is shown in Figure~\ref{fig_ts_accuracy}.
% An example of model performance in detecting blockages is shown in Figure~\ref{fig_performance_ex}.

% \subsection{Test set metrics}

% The resulting metrics of the test set with different post-hoc parameters are reported in Table~\ref{tab_metrics}.

\begin{figure}[hbtp!]
    \centering
    \includegraphics[width=.85\textwidth]{outputs/figures/final_prcurve.png}
    % \includegraphics{outputs/figures/oof_tune.png}
    \caption[Precision-recall curve of the test set]{Precision-recall curve of the test set with \acf{ap} values.}
    \label{fig_final_prcurve}
\end{figure}

\begin{table}[htb!]
    % \small
    \centering
    % \rowcolors{2}{white}{gray!20}
    \renewcommand{\arraystretch}{1.23}
    \begin{tabular}{r r | r r r r}
        \toprule
        Smoothing window & Threshold & Accuracy & Precision & Recall & $F_1$ \\
        \midrule
        \midrule
        - & 0.500 & 0.667 & 0.253 & 0.686 & 0.370 \\
        \midrule
        29 & 0.500 & 0.669 & 0.255 & 0.686 & 0.371 \\
        29 & 0.307 & 0.509 & 0.204 & 0.845 & 0.329 \\
        \midrule
        - & 0.317 & 0.518 & 0.206 & 0.838 & 0.331 \\
        \bottomrule
    \end{tabular}
    \caption[Performance metrics of the final model]{Performance metrics of the final model at different smoothing and thresholds.}
    \label{tab_metrics}
\end{table}

% \begin{figure}[hbtp!]
%     \centering
%     \includegraphics[width=.6\textwidth]{outputs/figures/final_prcurve.png}
%     % \includegraphics{outputs/figures/oof_tune.png}
%     \caption[Precision-recall curve of the test set]{Precision-recall curve of the test set with \acf{ap} values.}
%     \label{fig_final_prcurve}
% \end{figure}

\pagebreak
A time series showing monthly averages of model accuracy over the complete test set is shown in Figure~\ref{fig_ts_accuracy}.
An example of model performance in detecting blockages is shown in Figure~\ref{fig_performance_ex}.

% A time series showing monthly averages of model accuracy over the complete test set is shown in Figure~\ref{fig_ts_accuracy}.
% An example of model performance in detecting blockages is shown in Figure~\ref{fig_performance_ex}.

\begin{figure}[hbtp!]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/ts_accuracy.png}
    % \includegraphics{outputs/figures/ts_accuracy.png}
    \caption[Monthly model accuracy and concentration of blockages]{Monthly model accuracy (teal) and percentage of data containing blockages (light green).}
    \label{fig_ts_accuracy}
\end{figure}


% An example of model performance in detecting blockages is shown in Figure~\ref{fig_performance_ex}.

\begin{figure}[hbtp!]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/preformance_ex.png}
    % \includegraphics{outputs/figures/preformance_ex.png}
    \caption[Example of final model performance on a small subset of test data]{Example of final model performance on a small subset of the test data, where red lines are actual instances of an obstruction.}
    \label{fig_performance_ex}
\end{figure}

\pagebreak

% \section{Discussion}
\section{Discussion and Future Work}

A reflection on some of the challenges faced over the course of the project can be found in Appendix~\ref{apx_challenges}.

\subsection{Interpretation of Results}

Based on Figure~\ref{fig_oof_tune}, it can be concluded that threshold tuning significantly improved model performance, however window smoothing appeared ineffective.
The test results in Table~\ref{tab_metrics} show that threshold tuning greatly improved recall but at the detriment of other metrics: high recall indicates that the tuned-threshold model can successfully identify most true instances of obstructions, however low precision indicates that many data points were erroneously flagged as being obstructions.
% This was further proven by the test results in Table~\ref{tab_metrics}, since all metrics improved .
% 
The threshold selected from the absolute best smoothing window (i.e., before marginal gains filtering) had a slightly higher recall than the final selected threshold.

% \subsection{Limitations}

% The engineered features for the \ac{xgb} model are all look-behind to avoid data leakage and reflect performance of a ``live'' model.
% This differs from the manually-identified flags, which often use look-ahead metrics.\footnote{For example, a steep dropoff in runoff values followed by a calibratino point can indicate the end of a blockage.}
% % 
% Despite \ac{xgb} being able to yield successful results despite noisy data,\autocite{chen2016} it is possible that other failure modes significantly impede model performance.

From Figure~\ref{fig_ts_accuracy}, model accuracy seems to improve when there are fewer actual instances of obstructions.
The jagged accuracy line indicates that the current model will perform inconsistently on time periods with differing concentrations of obstruction events.
% However, the matching of peaks in mid-2018 and late 2024 to early 2025 seem to indicate that performance improved with greater instances of actual blco
The performance example in Figure~\ref{fig_performance_ex} shows that the model successfully highlighted much of the obstruction events, however struggled to maintain a smooth window to indicate the full range of the failure mode.
% Although smoothing with median windowing did not quantifiably improve performance, it is possible that other smothing 
% The highlighted window in the afternoon of July 2 does follow the pattern of other obstruction instances (due to not having a rain event to cause the increase), however not all non-rain increases are indicative of a blockage.\footnote{Such as periods of of evapotranspiration.}

% Regarding the tuned \ac{xgb} hyperparameters (Table~\ref{tab_xgbparams}), it is possible that the column subsampling could be negatively impacting the performance.
It is possible that column subsampling (Table~\ref{tab_xgbparams}) could be negatively impacting model performance.
% An analysis by \citean{chen2016} found that column subsampling in a classification experiment gave slightly worse performance compared to using all features, potnetially ``due to the fact that there are few important features in [the] dataset'' and that it would instead benefit to greedily select from all features.
An analysis by \citean{chen2016} found that column subsampling in a classification experiment gave slightly worse performance compared to using all features, potnetially due to the lack of ``important'' features in the dataset, and that the model may benefit from greedily selecting from all features instead.
% Given that features were already reduced using pairwise correlation filtering, 
More advanced feature selection prior to modelling may remove the need for column subsampling, thus potentially improving the model by simplifying it and reducing dimensionality further.
% \begin{quote}
%     “In this [classification] experiment, we also [found that] column subsamples gives slightly worse performance than using all the features. This could due to the fact that there are few important features in this dataset and we can benefit from greedily select from all the features.” (Chen, 792)
% \end{quote}

% \subsection{Limitations}

\subsection{Future Work}


% \pagebreak

% \section{Future Work}

Fitting large-parameter models usually require very large training datasets to obtain satisfactory results.
However, model performance usually do not scale linearly with the size of the training set; rather, after initial rapid improvements with increasing train set size, they reach often a point of diminishing returns.\autocite{phaseiiPC}
The point where diminishing returns has been reached will be a combination of the model accuracy required, the nature of the model and the data being used, and the cost of refitting the model with new data.
% The performance value of retraining the model as new data becomes available should be measured.
Thus, it is proposed that the existing model be retrained with smaller subsets of the original training data, allowing performance of the resulting fits to be compared to a model based on the full training set in order to quantify how quickly the model improves with increasing training set size.\autocite{phaseiiPC}
This will inform on how frequently the model might need to be updated (if at all) with additional data as it comes in in order to appreciably improve the results.\autocite{phaseiiPC}

Improvements to feature engineering for the \ac{xgb} model based on expert input would likely benefit the model performance.
The model only relied on look-behind features to avoid data leakage and reflect performance of a ``live'' model.
% The engineered features for the \ac{xgb} model are all look-behind to avoid data leakage and reflect performance of a ``live'' model.
This differs from the manually-identified flags, which often use look-ahead metrics.\footnote{For example, a steep dropoff in runoff values followed by a calibratino point can indicate the end of a blockage.}
It is possible that performance would improve if instead the classification model was able to utilize such ``look-ahead'' features, although implementation of this would require careful consideration since it is atypical from usual time series analysis, and would prevent the model from being able to predict on live data as it arrives in the database.
% 
Despite \ac{xgb} being able to yield successful results despite noisy data,\autocite{chen2016} it is possible that other failure modes impede model performance.


Future work will improve on the model for obstruction detection, and create other models for the other failure modes.
It is likely that different classification algorithms may have to be considered for other failure mode detection models, so further research will be necessary.

% Improvements to feature engineering for the \ac{xgb} model based on expert input would likely benefit the model performance.
% The model only relied on look-behind features to avoid data leakage and reflect performance of a ``live'' model.
% % The engineered features for the \ac{xgb} model are all look-behind to avoid data leakage and reflect performance of a ``live'' model.
% This differs from the manually-identified flags, which often use look-ahead metrics.\footnote{For example, a steep dropoff in runoff values followed by a calibratino point can indicate the end of a blockage.}
% It is possible that performance would improve if instead the classification model was able to utilize such ``look-ahead'' features, although implementation of this would require careful consideration since it is atypical from usual time series analysis, and would prevent the model from being able to predict on live data as it arrives in the database.
% % 
% Despite \ac{xgb} being able to yield successful results despite noisy data,\autocite{chen2016} it is possible that other failure modes significantly impede model performance.

Additionally, other models are expected to be constructed in order to correct the raw runoff values during windows of failure modes to the appropriate adjusted values.
If failure mode flagging models continue to have poor performance in regards to precision, it may be necessary to create a system to apply such correction models only to manually-confirmed windows of time, since modifying accurate data harms data quality \& integrity.
However, failure mode flagging models with good recall may still be successsfully utilized as a ``first-pass'' alert system for initial quality assurance checks.

% Director of the Physical Monitoring Program at \ac{stri} Steven Paton also adds:
% \begin{quotation}
%     Fitting multi-parameter models, particularly those involving large numbers of parameters, usually require very large training datasets to obtain satisfactory results.
%     However, model accuracy (however that may be scored) usually do not scale linearly with the size of the training set.
%     Rather, after initial rapid improvements with increasing train set size, they reach often a point of diminishing returns.
%     The point where diminishing returns has been reached will be a combination of the model accuracy required, the nature of the model and the data being used, and the cost of refitting the model with new data.

%     In the case of the current project in which we are attempting to automate the search for data errors in a very complex hydrological data set, we wish to know the value (in terms of increased accuracy and precision) of retraining the model as new data become available.
%     We propose that the model be trained with subsets of the original training data (eg. 95\%, 90\%, 80\%, 66\% and 50\%).
%     The resulting models will be evaluated against the model based on 100\% of the training set in order to explore how quickly the model improves with increasing training set size.
%     This will inform us on how frequently the model might be updated (if at all) with additional data in order to appreciably improve the results.\autocite{phaseiiPC}
% \end{quotation}


% A reflection on some of the challenges faced over the course of the project can be found in Appendix~\ref{apx_challenges}.

\pagebreak
\pagebreak
\pagebreak

% sections
% Description of problem
% why it's important
% data explination
% raw data
% sources of failure (easy to hard)
% challenges faced (incl cleanup)
% progress of each failure mode
% explain what done so far
% future work
% conclusions
% limitations why the prob is complicated & why you go tt to here

\pagebreak
\pagebreak
\pagebreak

\section{Literature Review/Market Research}\label{sec_lit}
% This section describes who your research project is for, what they want and what other work has been done on your topic area.. This section is designed to illustrate you have an understanding of the existing research and work previously completed.


\subsection*{Overall research landscape}
% Overall research landscape: Do some basic research about existing research, cite papers and clarify how your project will contribute new knowledge to the world and or utilize novel approaches to existing data sets. 


% 
% In hydrology, machine learning (ML) has been used for predictive runoff modelling,\autocite{mohammadi2021} modelling agricultural drought,\autocite{houmma2022}, MORE EXAMPLES.
ML has been used in hydrology for applications such as predictive runoff modelling\autocite{mohammadi2021} and modelling agricultural drought.\autocite{houmma2022}
% In hydrology, machine learning (ML) has been used for predictive runoff modelling\autocite{mohammadi2021} and modelling agricultural drought.\autocite{houmma2022}
% Rainfall-runoff data collected by citizen scientists in Ethiopia has been visually (via graphical inspections) and quantitatively (via statistical methods) analyzed for quality compared to nearby professional references.\autocite{mengistiea2024}
% 
% \citean{houmma2022} highlights how incorporation of local and microclimate variables are becoming increasingly necessary for modeling expected droughts due to climate change and ``exceptional situations'' making extreme peaks of variables \& interdependencies no longer able to be statistically modelable.
% 
Rainfall runoff data collected by citizen scientists in Ethiopia has been visually (via graphical inspections) and quantitatively (via statistical methods) analyzed for quality compared to nearby professional references.\autocite{mengistiea2024}
In the field of remote sensing and geospatial analysis, recurrent~neural~networks---especially~long short\nobreakdash-term~memory~(\ac{lstm})---are successful for analyzing multi-temporal data, however scalability is limited and they can be prone to overfitting.\autocite{dritsas2025}
% 
In environmental ML, improper missing data management techniques can also worsen data leakage.\autocite{zhu2023}
% Data leakage issues are another consideration in environmental ML, and can stem from improper missing data management techniques.\autocite{zhu2023}

% 
% Rainfall-runoff data collected by citizen scientists in Ethiopia has been visually (via graphical inspections) and quantitatively (via statistical methods) analyzed for quality compared to nearby professional references.\autocite{mengistiea2024}
% According to Paton, ML algorithms have yet to be applied to singular catchment runoff data quality~assurance.

The Lutz~catchment (Figure~\ref{fig_map}) is a 9.73~ha area located in the secondary low land tropical rainforest on \ac{bci}, a 15~km\textsuperscript{2} island located in Lake Gatun in the Republic~of~Panama.\autocite{mapPC}
% \ac{bci} is located in the middle of the largest lake in the Republic of Panama.
% The Lutz~catchment encompasses 9.73~ham and is located in 100-year-old, secondary low land tropical rainforest on Barro Colorado Island (\ac{bci}). \ac{bci} is a 15~km2 island located in the middle of Lake Gatun, the largest lake in the Republic of Panama and a key section of the Panama Canal.
\ac{bci} is operated by \ac{stri}, a branch of the Smithsonian~Institution.\autocite{stri2025}
% \ac{bci} is one of the most intensely studied tropical rainforests in the world.
Runoff at the Lutz~catchment---as well as other meteorological and hydrological data---have been continually monitored since 1972, making it one longest, continually monitored micro-catchment datasets for the neotropics.\autocite{mapPC}
% Data from this catchment provide critical information concerning the hydrological balance of \ac{bci}.

% According to Paton, there have been few---if any---attempts to apply ML algorithms to singular catchment runoff data quality~assurance.
% Previously, stochastic methods were attempted for automation of the Lutz~catchment data quality assurance, however the team concluded it was unsuccessful in being more accurate than Paton's manual corrections.
% % However, what makes the Lutz~catchment data unique is th
% % Although water flow is a complex phenomenon (especially with other environmental considerations such as soil moisture content and temperature), it is anticipated that the quantity of raw and manually-corrected data and inclusion of additional variables will enable an algorithmic approach to correct the data to a reasonably-accurate degree even without the complexities of hydrological movement.
% Although water flow is a complex phenomenon, it is anticipated that the quantity of raw and manually-corrected data as well as inclusion of additional variables will enable an algorithmic approach to correct the data to a reasonably-accurate degree.

External variables of rainfall and soil moisture content are necessary for a well-informed model.
In the context of modelling expected droughts, \citean{houmma2022} highlights how incorporation of local and microclimate variables are becoming increasingly necessary due to climate change and ``exceptional~situations'' which otherwise make extreme peaks of variables \& interdependencies no longer able to be statistically modelable.

% \bigbreak

% \begin{figure}[!hbt]
%     \centering
%     \fbox{\includegraphics[width=.6\textwidth]{figures/map_bci_soil.jpg}}
%     % \caption{The locations of the weir and tower in relation to the 10 soil moisture sampling sites in the Lutz~catchment on Barro~Colorado~Island, Panama.\autocite{stri2025}}
%     % \caption{A fine-scale topographic map of the Lutz~catchment on Barro~Colorado~Island, Panama, showing the locations of the weir, tower, and 10 soil moisture sampling sites.\autocite{hydroPC}}
%     \caption{A fine-scale topographic map of the Lutz~catchment showing the locations of the weir, tower, and 10 soil moisture sampling sites.\autocite{hydroPC}}
%     % \caption{The location of Barro~Colorado~Island in Panama CITE.}
%     \label{fig_map}
% \end{figure}
% The inclusion of additional variables is also expected to improve the models, as \citean{houmma2022} highlights how incorporation of local and microclimate variables are becoming increasingly necessary for modeling expected droughts due to climate change and ``exceptional situations'' making extreme peaks of variables \& interdependencies no longer able to be statistically modelable.
% 
% Rainfall runoff data collected by citizen scientists in Ethiopia has been visually (via graphical inspections) and quantitatively (via statistical methods) analyzed for quality compared to nearby professional references.\autocite{mengistiea2024}
% Previously, stochastic methods were attempted for automation of the Lutz~catchment data quality assurance, however the team concluded it was unsuccessful in being more accurate than Paton's manual corrections.

% In the context of bioinformatics, \citean{kane2022} uses NN workflows for multiclass classification while avoiding data leakage, compensating for missing values, and reducing dimensionality.
% NN workflows are used in bioinformatics for multiclass classification
% The paper focuses on a proposed NN workflow in the context of bioinformatics for multiclass classification while avoiding data leakage, compensating for missing values, and reducing dimensionality.
% To compensate for missing data, kNN-imputation and Iterative imputation were used (numeric models could not be used due to the presence of non-numeric variables). 

% Literature review to show and explain common misunderstandings of ML in environmental research publications.
% Examples from environmental literature of different missing data management (MDM) techniques, including removal, imputation, and replacement (p. 17674). Data leakage issues that can stem from improper MDM are also addressed (p. 17678)

% \pagebreak

\subsection*{User insights}
% User insights: It is important to listen to potential users to identify their needs and pain points. Utilize the empathy interview to speak with potential users of your research output and list the key insights  and pain points you have discovered during the process and detail how your product will address them. 

% The target user for these models will be hydrology researchers analyzing temporal weir and environmental data in parallel.
% However, given the specificity in location of the weir of interest, it is possible that other factors (such as temperature) may have greater or less influence on runoff data at other weirs around the world and could render final correction models unsuitable for other weirs.
Paton currently uses Visual~FoxPro to manipulate the data.
% Visual~FoxPro is currently the only software used by Paton to manipulate the data.
While the interface has been sufficient for completing the tasks at hand, the software is vulnerable due to its depreciation.
Furthermore, current quality assurance methods rely on subjective decisions by the individual analyzing the data, meaning that results are potentially biased to some unknown degree.\autocite{mapPC}
% Quality assurance requires some subjective decisions by the person analyzing the data using the \ac{vfp} system (Paton so far been the only one involved in this activity) meaning that the results are potentially biased to some, unknown degree.
Additionally, it is not currently set up to automatically detect problematic portions of the data.
Because data monitoring is ongoing, new data must be regularly checked for occurrence of any type of failure mode.

% Currently, there is no plan to ``market'' these tools---rather, emphases on readability, accessibility, and reproducibility will be maintained.
% % 

According to Paton, there have been few---if any---attempts to apply ML algorithms to singular catchment runoff data quality~assurance.
Previously, stochastic methods were attempted for automation of the Lutz~catchment data quality assurance, however the team concluded it was unsuccessful in being more accurate than Paton's manual corrections.
% However, what makes the Lutz~catchment data unique is th
% Although water flow is a complex phenomenon (especially with other environmental considerations such as soil moisture content and temperature), it is anticipated that the quantity of raw and manually-corrected data and inclusion of additional variables will enable an algorithmic approach to correct the data to a reasonably-accurate degree even without the complexities of hydrological movement.
Although water flow is a complex phenomenon, it is anticipated that the quantity of raw and manually-corrected data as well as inclusion of additional variables will enable an algorithmic approach to correct the data to a reasonably-accurate degree.

\pagebreak

% \begin{figure}[!hbt]
%     \centering
%     \fbox{\includegraphics[width=.95\textwidth]{figures/map_bci_soil.jpg}}
%     % \caption{The locations of the weir and tower in relation to the 10 soil moisture sampling sites in the Lutz~catchment on Barro~Colorado~Island, Panama.\autocite{stri2025}}
%     % \caption{A fine-scale topographic map of the Lutz~catchment on Barro~Colorado~Island, Panama, showing the locations of the weir, tower, and 10 soil moisture sampling sites.\autocite{hydroPC}}
%     \caption{A fine-scale topographic map of the Lutz~catchment showing the locations of the weir, tower, and 10 soil moisture sampling sites.\autocite{hydroPC}}
%     % \caption{The location of Barro~Colorado~Island in Panama CITE.}
%     \label{fig_map}
% \end{figure}

\pagebreak

\section{Research Project Deliverables}\label{sec_deliv}
% This section describes the scope and features of the _analysis_ created by the _project_. It is what the project will PRODUCE. You should write a draft version of these but this is what your mentor will help you with especially. This is often the breakdown of what will and will not be considered a passing project so make this detailed!

% The key objective of the next section is to address the following itels: 
% \renewcommand{\thesubsection}{\bf{}}
% \setcounter{subsection}{1}
\subsection{Final Presentation Format}
% Final Presentation Format
% How will your final project be submitted? Will it culminate in a paper with introduction, methods, and results section and at least 5 peer reviewed citations, that is <10 pages long and has at least two informative graphics? Will it culminate in an RShiny application that is also available on GitHub?

The final project will be submitted in the form of a GitHub repository containing code that is relevant, readable, and thoroughly annotated to maximize accessibility and reproducibility.
In addition to code files, a written report with background information, methods/approach, and final model performance statistics \& visualizations will also be created.
% A written report with model performance statistics \& visualizations.

% The system will determine where there are readings containing a potential failure mode. This also requires the system to determine the time range in which a failure mode is occurring.
% A challenge for this feature is that some failures occur in extremely short intervals of time (even as few as one reading), while others can spoil data spanning multiple weeks.

\subsection{What Analysis Is Being Run?}
% Once the data is in, what analyses exactly should be run? E.G. what type of neural network is being run, what type of correlation analysis is being run, etc.

\subsubsection*{Classification of Failure Mode}

Periods of time in the data can be flagged as containing ``failure modes'', as explained in Table~\ref{tab_fm}.
The system will determine where there are readings containing a particular failure mode. This also requires the system to determine the time range in which a failure mode is occurring.
A challenge for this feature is that some failures occur in extremely short intervals of time (even as few as one reading), while others can spoil data spanning multiple weeks.

% Seasonal autoregressive integrated moving average (seasonal ARIMA [SARIMA]) can be used to analyze time series data that have seasonal patterns by combining seasonal autoregressive (SAR), seasonal differencing (SI), and seasonal moving average (SMA) terms.

% Periods of time in the data can be flagged as containing particular ``failure modes'', as explained in Table~\ref{tab_fm}.
To flag data points, supervised classification can utilize rolling statistics and the external environmental variables (which provide some seasonal insights).
Gradient boosted trees or \ac{lstm} can be used, but more literature review and exploratory data analysis will need to be conducted prior to finalizing which model(s) to utilize.
% Upon a period being flagged as containing a failure mode, it will need to be classified.
% A brief overview of different failure modes is provided in Table~\ref{tab_fm}.

% \bigskip
% \bigskip

% % \vspace*{\fill}
% \begin{table}[htbp]
%     % \small
%     \centering
%     \rowcolors{2}{white}{gray!20}
%     \renewcommand{\arraystretch}{1.23}
%     \begin{tabular}{l p{0.395\textwidth} p{0.395\textwidth}}
%         \toprule
%         Failure Mode & Description & Complicating Factors \\
%         \midrule
%         Calibration & Standard checks that require baseline correction or slight data pivot. & Recovery after blockage clearing can render calibration points ineffective.\\
%         Spike & Short and abrupt changes in level typically caused by equipment issues. & Extremely short-term issue that could be skipped over by random sampling. \\
%         Sub-zero & The stream runs dry, or the pond is being drained for cleaning. & Rain during a pond draining can render new data unrecoverable.\\
%         Signal noise & Equipment failure results in impossible variability of reported values. & Impossible to manually fix.\\
%         % \midrule
%         % Blockage & Debris blocks the weir's `V', resulting in a gentle increase in water level. & Rainfall events before, during, or shortly after a blockage can interrupt the base flow and decay curves. \\
%         Blockage & Debris blocks the weir's `V', resulting in gradual increases in water level, and later abrupt decreases following blockage removal. & Rainfall events before, during, or shortly after a blockage can interrupt the base flow and decay curves. \\
%         % Blockage & Debris blocks the weir's `V', resulting in a gentle increase in water level. \\
%         % Blockage after rainfall & A blockage interrupts the normal water level decay curve following a rainfall event. \\
%         % Blockage during rainfall & A blockage occurs during a rainfall event, interrupting the base flow and decay curves.\\
%         \bottomrule
%     \end{tabular}
%     \caption{Overview of Weir Failure Modes\autocite{hydroPC}}
%     \label{tab_fm}
% \end{table}
% % \vspace*{\fill}

% \pagebreak

\subsubsection*{Quality Assurance on Periods of Failure}

% Feature~\hyperref[subsec_class]{2}
% Based on the classification result(s), additional models will attempt to adjust the raw outputs to the appropriate values had an error not occurred.
Based on the classification result(s), additional models will attempt to adjust the raw outputs to the appropriate values had one of the above-mentioned failure modes not occurred.
This will involve creation of multiple separate models, as the method in which the data is corrected differs based on the classification of failure type---for example, an interrupted drainage caused by a blockage in the weir can be fixed by the expected decay curve, while a spike is simply ``flattened'' to the level of adjacent data points using a simple linear regression model.

\subsection{What Accuracy Is Expected?}
% Sometimes models don't perform super well. If you're running a model, do you expect 100% accuracy? 65% accuracy across multiple outputs? An R2 of .6? What happens if the model isn't very accurate?

Accuracy for certain failure mode classifications are expected to be high---for instance, ``sub-zero'' values are simple to flag. Others, such as ``spikes'', are also expected to be easily found.
Accuracy for multiple failure modes may be more difficult, especially in the dry season when there are non-erroneous fluctuations in flow caused by evapotranspiration.
Multiple, large rainfall events also pose particular challenges, as well as overlapping failure modes (e.g., simultaneous blockage and calibration problems).

Similarly, correction models for simpler failure modes are expected to be highly accurate and precise, since they are often simple linear gap-fills.
Corrections for the failure mode of a blockage in combination with a rain event may have reasonable accuracy but lower precision.

\subsection{What if the Analysis doesn't work?}
% Sometimes analyses don't work. Is a null result acceptable, or do you need to do work to circumvent that?

If the error type categorization has poor performance or is unable to consistently perform, and the needs of the correction models become more pressing as the semester continues, the data set does include the manually-flagged error types.
The results of the ML-assisted corrections will be compared against the results of the manual~corrections to data not included in the training sets.
Large deviations from the expected values will be considered ``failures''.\autocite{mapPC}
If the correction models do not work, it will still provide valuable insights into which failure modes are most difficult to automate or most ``resistant'' to certain algorithms.

% \subsection{What if the Data Isn't Available?}
% This is only for projects that require data scraping – what will happen if your data is not scrapable? How will you salvage your project?
% \bigbreak
% \bigbreak

\pagebreak

\appendix
\section*{Appendices}
\addcontentsline{toc}{section}{Appendices}

% \setcounter{subsection}{1}
% \renewcommand{\thesubsection}{\Alph{subsection}.\hspace{1em}}
% \subsection{Failure Mode Examples}
\setcounter{section}{1}
\renewcommand{\thesubsection}{\thesection}
\subsection{Failure Mode Examples}\label{apx_fm}
% \renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\thefigure}{\thesubsection\arabic{figure}}
\setcounter{figure}{0}
% \setcounter{subsubsection}{1}
% \renewcommand{\thesubsubsection}{\arabic{subsubsection} )\hspace{1em}}
% \subsubsection{Obstruction}

\begin{figure}[hbtp!]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/ex_block.png}
    % \includegraphics{outputs/figures/preformance_ex.png}
    \caption[Example of obstruction failure mode]{Example of consecutive distinct obstruction events.}
    \label{fig_ex_block}
\end{figure}

% \subsubsection{Spike}

\begin{figure}[hbtp!]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/ex_spike.png}
    % \includegraphics{outputs/figures/preformance_ex.png}
    \caption[Example of spike failure mode]{Example of two distinct spike events.}
    \label{fig_ex_spike}
\end{figure}

\begin{figure}[hbtp!]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/ex_cal.png}
    % \includegraphics{outputs/figures/preformance_ex.png}
    \caption[Example of calibration failure mode]{Example of an long-spanning calibration adjustment.}
    \label{fig_ex_cal}
\end{figure}

\begin{figure}[hbtp!]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/ex_subz.png}
    % \includegraphics{outputs/figures/preformance_ex.png}
    \caption[Example of sub-zero failure mode]{Example of sub-zero runoff measurements.}
    \label{fig_ex_subz}
\end{figure}

\begin{figure}[hbtp!]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/ex_noise.png}
    % \includegraphics{outputs/figures/preformance_ex.png}
    \caption[Example of noise failure mode]{Example of noisy runoff measurements, which have yet to have accurate adjusted values assigned.}
    \label{fig_ex_noise}
\end{figure}

% \begin{itemize}
%     \item The Faculty Advisor will provide knowledge and expertise to help the group stretch their skills. 
%     \item The Faculty Advisor will participate in a weekly or bi-weekly call/meeting with the Project Team to review the project status, upcoming deliverables, priorities, issues, and progress to the agreed Project Plan.
%     \item The Faculty Advisor will provide document review, feedback and approval, rejection, approval with contingencies with adequate time for the Project Team to meet the course due dates. 
%     \item The Faculty Advisor will provide feedback to requested support requirements from the Project Team. This includes feedback and guidance on design implementations decisions, design files, test plans, test procedures and test results.
%     \item The Faculty Advisor shall provide technical advice and guidance to the Project Team answering inquiries approximately 1 hour per week. 
%     \item Modifications to the Project Plan by the Project Team will be resolved and documented within 1 week of the request.
%     \item Grade the finalized project using a skill-based rubric
%     \item Attend iShowcase in May.
% \end{itemize}

\pagebreak
\setcounter{section}{2}
\subsection{Challenges}
\label{apx_challenges}

A number of unforeseen challenges were faced during the analysis, due in-part to the novelty and size of the project itself.
% However, each was able to provide a unique learning opportunity and skill sharpening.
It was known that past stochastic approaches had failed; thus, extensive background research was necessary to determine the appropriate machine learning model types.

Data cleaning and wrangling also took significantly longer than expected.
% This was partly due to the need for clarifications of custom 
Although all data was relatively uniform in its available state, filtering for the specific entries of interest and removing problematic entries without unnecessary data deletion was a challenge at time.
% Additionally, the differences in data frequency for obstruction detection
% Fortunately, this will reduce the preparation required for future analysis of other failure modes, and also provided the opportunity for some more advanced wrangling techniques.
Fortunately, this will reduce the preparation required for future analysis of other failure modes.

Communication with the expert data contact at \ac{stri} was not possible the majority of October and November due to the federal government shutdown.
This did, however, provide the opportunity for more thoughtful data selection decisions since relying solely on external clarifications was not immediately available.
% This did result in some data selection ``executive decisions'' since clarifications were not possible.

Computational power also proved a challenge, as there were more than 4.17~million data points (each with their own associated datetime-stamp) collectively in the imported sets.
Even after removing irrelevant data points, the complexity was expanded further with feature engineering, as this resulted in most data points having more than one hundred individual values associated with it.
Ultimately, a cloud-based high-RAM GPU proved the most efficient approach for the final model runs, although this required other code adjustments to further improve memory and prioritize paralelle processing.

Although only one failure mode was able to be addressed in the scope of the project, it was specifically chosen because it was the most difficult, and because such machine~learning analysis had yet to be applied in this way to hydrological time series data on such a large scale.

% It was knwon that past stochastic approaches had failed; thus, extensive background research was necessary to determine the appropriate machine learning model types.

% Unfamiliarity with time series analysis and the appropriate machine~learning models was an additional challenge.
% However, this provided the opportunity to research and explore further.
% Additionally, the federal government shutdown spanning the majority of October and November resulted in a large span 

% Challenges
% New project & approach
% Past stochastic approaches had failed; extensive background research was necessary to determine appropriate machine learning model types
% Gaps & missing data
% Sensor failures, missing values, and inconsistent labels, comments, & flags
% Differences in data frequency
% Runoff & rain:
%   every 5 minutes for 36 years from 1 site
% Soil moisture:
%   once a week from 10 sites ×2 depths each
% Computational power
% More than 4.15 million total entries
% Access & communication
% Primary data contact was unavailable the majority of Oct & Nov due to the federal government shutdown

% Unforeseen difficulties experienced during these analys

\pagebreak
% \nocite{*}

\printbibliography
\label{sec_bib}

\end{document}